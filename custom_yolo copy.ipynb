{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# from torch.nn.init import constant_, xavier_uniform_\n",
    "\n",
    "# from ultralytics.nn.modules.block import C3k2, SPPF, C2PSA\n",
    "# from ultralytics.nn.modules.conv import Conv\n",
    "# from ultralytics.nn.modules.head import Detect\n",
    "\n",
    "x = torch.randn(8, 1, 630, 630)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Module 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#YOLO's nn module\n",
    "class SPPF(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=5):\n",
    "        \"\"\"\n",
    "        Initializes the SPPF layer with given input/output channels and kernel size.\n",
    "\n",
    "        This module is equivalent to SPP(k=(5, 9, 13)).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = c1 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
    "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through Ghost Convolution block.\"\"\"\n",
    "        y = [self.cv1(x)]\n",
    "        y.extend(self.m(y[-1]) for _ in range(3))\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "    \n",
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class DWConv(Conv):\n",
    "    \"\"\"Depth-wise convolution.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):  # ch_in, ch_out, kernel, stride, dilation, activation\n",
    "        \"\"\"Initialize Depth-wise convolution with given parameters.\"\"\"\n",
    "        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n",
    "\n",
    "\n",
    "class DWConvTranspose2d(nn.ConvTranspose2d):\n",
    "    \"\"\"Depth-wise transpose convolution.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out\n",
    "        \"\"\"Initialize DWConvTranspose2d class with given parameters.\"\"\"\n",
    "        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))\n",
    "\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    \"\"\"Convolution transpose 2d layer.\"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=2, s=2, p=0, bn=True, act=True):\n",
    "        \"\"\"Initialize ConvTranspose2d layer with batch normalization and activation function.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(c1, c2, k, s, p, bias=not bn)\n",
    "        self.bn = nn.BatchNorm2d(c2) if bn else nn.Identity()\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies transposed convolutions, batch normalization and activation to input.\"\"\"\n",
    "        return self.act(self.bn(self.conv_transpose(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Applies activation and convolution transpose operation to input.\"\"\"\n",
    "        return self.act(self.conv_transpose(x))\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
    "        \"\"\"Initializes a standard bottleneck module with optional shortcut connection and configurable parameters.\"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
    "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the YOLO FPN to input data.\"\"\"\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "    \n",
    "class Proto(nn.Module):\n",
    "    \"\"\"YOLOv8 mask Proto module for segmentation models.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c_=256, c2=32):\n",
    "        \"\"\"\n",
    "        Initializes the YOLOv8 mask Proto module with specified number of protos and masks.\n",
    "\n",
    "        Input arguments are ch_in, number of protos, number of masks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cv1 = Conv(c1, c_, k=3)\n",
    "        self.upsample = nn.ConvTranspose2d(c_, c_, 2, 2, 0, bias=True)  # nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.cv2 = Conv(c_, c_, k=3)\n",
    "        self.cv3 = Conv(c_, c2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass through layers using an upsampled input image.\"\"\"\n",
    "        return self.cv3(self.cv2(self.upsample(self.cv1(x))))\n",
    "\n",
    "    \n",
    "class C2f(nn.Module):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
    "        \"\"\"Initializes a CSP bottleneck with 2 convolutions and n Bottleneck blocks for faster processing.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through C2f layer.\"\"\"\n",
    "        y = list(self.cv1(x).chunk(2, 1))\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "    def forward_split(self, x):\n",
    "        \"\"\"Forward pass using split() instead of chunk().\"\"\"\n",
    "        y = self.cv1(x).split((self.c, self.c), 1)\n",
    "        y = [y[0], y[1]]\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "class C3(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with 3 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n",
    "        \"\"\"Initialize the CSP Bottleneck with given channels, number, shortcut, groups, and expansion values.\"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c1, c_, 1, 1)\n",
    "        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\n",
    "        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
    "    \n",
    "class C3k2(C2f):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):\n",
    "        \"\"\"Initializes the C3k2 module, a faster CSP Bottleneck with 2 convolutions and optional C3k blocks.\"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        self.m = nn.ModuleList(\n",
    "            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)\n",
    "        )\n",
    "\n",
    "class C3k(C3):\n",
    "    \"\"\"C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):\n",
    "        \"\"\"Initializes the C3k module with specified channels, number of layers, and configurations.\"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        # self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention module that performs self-attention on the input tensor.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input tensor dimension.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        attn_ratio (float): The ratio of the attention key dimension to the head dimension.\n",
    "\n",
    "    Attributes:\n",
    "        num_heads (int): The number of attention heads.\n",
    "        head_dim (int): The dimension of each attention head.\n",
    "        key_dim (int): The dimension of the attention key.\n",
    "        scale (float): The scaling factor for the attention scores.\n",
    "        qkv (Conv): Convolutional layer for computing the query, key, and value.\n",
    "        proj (Conv): Convolutional layer for projecting the attended values.\n",
    "        pe (Conv): Convolutional layer for positional encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, attn_ratio=0.5):\n",
    "        \"\"\"Initializes multi-head attention module with query, key, and value convolutions and positional encoding.\"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.key_dim = int(self.head_dim * attn_ratio)\n",
    "        self.scale = self.key_dim**-0.5\n",
    "        nh_kd = self.key_dim * num_heads\n",
    "        h = dim + nh_kd * 2\n",
    "        self.qkv = Conv(dim, h, 1, act=False)\n",
    "        self.proj = Conv(dim, dim, 1, act=False)\n",
    "        self.pe = Conv(dim, dim, 3, 1, g=dim, act=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Attention module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): The output tensor after self-attention.\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        N = H * W\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.view(B, self.num_heads, self.key_dim * 2 + self.head_dim, N).split(\n",
    "            [self.key_dim, self.key_dim, self.head_dim], dim=2\n",
    "        )\n",
    "\n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (v @ attn.transpose(-2, -1)).view(B, C, H, W) + self.pe(v.reshape(B, C, H, W))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class PSABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    PSABlock class implementing a Position-Sensitive Attention block for neural networks.\n",
    "\n",
    "    This class encapsulates the functionality for applying multi-head attention and feed-forward neural network layers\n",
    "    with optional shortcut connections.\n",
    "\n",
    "    Attributes:\n",
    "        attn (Attention): Multi-head attention module.\n",
    "        ffn (nn.Sequential): Feed-forward neural network module.\n",
    "        add (bool): Flag indicating whether to add shortcut connections.\n",
    "\n",
    "    Methods:\n",
    "        forward: Performs a forward pass through the PSABlock, applying attention and feed-forward layers.\n",
    "\n",
    "    Examples:\n",
    "        Create a PSABlock and perform a forward pass\n",
    "        >>> psablock = PSABlock(c=128, attn_ratio=0.5, num_heads=4, shortcut=True)\n",
    "        >>> input_tensor = torch.randn(1, 128, 32, 32)\n",
    "        >>> output_tensor = psablock(input_tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c, attn_ratio=0.5, num_heads=4, shortcut=True) -> None:\n",
    "        \"\"\"Initializes the PSABlock with attention and feed-forward layers for enhanced feature extraction.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = Attention(c, attn_ratio=attn_ratio, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(Conv(c, c * 2, 1), Conv(c * 2, c, 1, act=False))\n",
    "        self.add = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Executes a forward pass through PSABlock, applying attention and feed-forward layers to the input tensor.\"\"\"\n",
    "        x = x + self.attn(x) if self.add else self.attn(x)\n",
    "        x = x + self.ffn(x) if self.add else self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class C2PSA(nn.Module):\n",
    "    \"\"\"\n",
    "    C2PSA module with attention mechanism for enhanced feature extraction and processing.\n",
    "\n",
    "    This module implements a convolutional block with attention mechanisms to enhance feature extraction and processing\n",
    "    capabilities. It includes a series of PSABlock modules for self-attention and feed-forward operations.\n",
    "\n",
    "    Attributes:\n",
    "        c (int): Number of hidden channels.\n",
    "        cv1 (Conv): 1x1 convolution layer to reduce the number of input channels to 2*c.\n",
    "        cv2 (Conv): 1x1 convolution layer to reduce the number of output channels to c.\n",
    "        m (nn.Sequential): Sequential container of PSABlock modules for attention and feed-forward operations.\n",
    "\n",
    "    Methods:\n",
    "        forward: Performs a forward pass through the C2PSA module, applying attention and feed-forward operations.\n",
    "\n",
    "    Notes:\n",
    "        This module essentially is the same as PSA module, but refactored to allow stacking more PSABlock modules.\n",
    "\n",
    "    Examples:\n",
    "        >>> c2psa = C2PSA(c1=256, c2=256, n=3, e=0.5)\n",
    "        >>> input_tensor = torch.randn(1, 256, 64, 64)\n",
    "        >>> output_tensor = c2psa(input_tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, e=0.5):\n",
    "        \"\"\"Initializes the C2PSA module with specified input/output channels, number of layers, and expansion ratio.\"\"\"\n",
    "        super().__init__()\n",
    "        assert c1 == c2\n",
    "        self.c = int(c1 * e)\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv(2 * self.c, c1, 1)\n",
    "\n",
    "        self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=self.c // 64) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Processes the input tensor 'x' through a series of PSA blocks and returns the transformed tensor.\"\"\"\n",
    "        a, b = self.cv1(x).split((self.c, self.c), dim=1)\n",
    "        b = self.m(b)\n",
    "        return self.cv2(torch.cat((a, b), 1))\n",
    "    \n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = distance.chunk(2, dim)\n",
    "    x1y1 = anchor_points - lt\n",
    "    x2y2 = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2\n",
    "        wh = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net 구조기반 YOLO Module로 Segmentation 모델 짜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "# Backbone 정의\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from ultralytics.nn.modules import *\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, nc=1):\n",
    "        super().__init__()\n",
    "        # 인코더 (C3K2 blocks 추가)\n",
    "        self.enc1 = nn.Sequential( \n",
    "            C2f(1, 32, 1), # 1, 630, 630 -> 64, 630, 630\n",
    "            C3k2(32, 32, shortcut=False) # 64, 630, 630 -> 64, 630, 630\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2) # 64, 630, 630 -> 64, 315, 315\n",
    "        self.enc2 = nn.Sequential( \n",
    "            C2f(32, 64, 1), # 64, 315, 315 -> 128, 315, 315\n",
    "            C3k2(64, 64, shortcut=False) # 128, 315, 315 -> 128, 315, 315\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2) # 128, 315, 315 -> 128, 157, 157\n",
    "        self.enc3 = nn.Sequential( \n",
    "            C2f(64, 128, 1), # 128, 157, 157 -> 256, 157, 157\n",
    "            C3k2(128, 128, shortcut=False) # 256, 157, 157 -> 256, 157, 157\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2) # 256, 157, 157 -> 256, 78, 78\n",
    "        self.enc4 = nn.Sequential(\n",
    "            C2f(128, 256, 1), # 256, 78, 78 -> 512, 78, 78\n",
    "            C3k2(256, 256, shortcut=False) # 512, 78, 78 -> 512, 78, 78\n",
    "        )\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 브릿지 (SPPF + C2PSA)\n",
    "        self.bridge = nn.Sequential(\n",
    "            C2f(256, 512, 1), # 512, 39, 39 -> 1024, 39, 39\n",
    "            SPPF(512, 512),  # Spatial Pyramid Pooling - Fast # 1024, 39, 39 -> 1024, 39, 39\n",
    "            C2PSA(512, 512)  # CSP with Parallel Spatial Attention # 1024, 39, 39 -> 1024, 39, 39\n",
    "        )\n",
    "        \n",
    "        # 디코더 (C3K2 blocks 추가)\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, 2, stride=2) # 1024, 39, 39 -> 512, 78, 78\n",
    "        self.dec4 = nn.Sequential(\n",
    "            C2f(512, 256, 1), # 1024, 78, 78 -> 512, 78, 78\n",
    "            C3k2(256, 256, shortcut=False) # 512, 78, 78 -> 512, 78, 78\n",
    "        )\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, 2, stride=2) # 512, 78, 78 -> 256, 157, 157\n",
    "        self.dec3 = nn.Sequential(\n",
    "            C2f(256, 128, 1), # 512, 157, 157 -> 256, 157, 157\n",
    "            C3k2(128, 128, shortcut=False) # 256, 157, 157 -> 256, 157, 157\n",
    "        )\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, 2, stride=2) # 256, 157, 157 -> 128, 315, 315\n",
    "        self.dec2 = nn.Sequential(\n",
    "            C2f(128, 64, 1), # 256, 315, 315 -> 128, 315, 315\n",
    "            C3k2(64, 64, shortcut=False) # 128, 315, 315 -> 128, 315, 315\n",
    "        )\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, 2, stride=2) # 128, 315, 315 -> 64, 630, 630\n",
    "        self.dec1 = nn.Sequential(\n",
    "            C2f(64, 32, 1), # 128, 630, 630 -> 64, 630, 630\n",
    "            C3k2(32, 32, shortcut=False) # 64, 630, 630 -> 64, 630, 630\n",
    "        )\n",
    "        \n",
    "        # self.out = nn.Conv2d(64, nc, 1)\n",
    "\n",
    "    def center_crop(self, skip, x):\n",
    "        \"\"\"스킵 커넥션과 업샘플링된 특징 맵의 크기를 맞춤\"\"\"\n",
    "        if x.shape[2:] != skip.shape[2:]: # 크기가 다를 경우\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True) # 업샘플링\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 인코더\n",
    "        # x : 1, 630, 630\n",
    "        e1 = self.enc1(x) # 64, 630, 630\n",
    "        e2 = self.enc2(self.pool1(e1)) # 128, 315, 315\n",
    "        e3 = self.enc3(self.pool2(e2)) # 256, 157, 157\n",
    "        e4 = self.enc4(self.pool3(e3)) # 512, 78, 78\n",
    "        \n",
    "        # 브릿지\n",
    "        b = self.bridge(self.pool4(e4)) # 1024, 39, 39\n",
    "        \n",
    "        # 디코더 (크기 맞추기 추가)\n",
    "        x = self.upconv4(b) # 1024, 39, 39 -> 512, 78, 78\n",
    "        x = self.center_crop(e4, x) # 512, 78, 78\n",
    "        d4 = self.dec4(torch.cat([x, e4], 1)) # 512, 78, 78\n",
    "        \n",
    "        x = self.upconv3(d4) # 512, 78, 78 -> 256, 157, 157\n",
    "        x = self.center_crop(e3, x) # 256, 157, 157\n",
    "        d3 = self.dec3(torch.cat([x, e3], 1)) # 256, 157, 157\n",
    "        \n",
    "        x = self.upconv2(d3) # 256, 157, 157 -> 128, 315, 315\n",
    "        x = self.center_crop(e2, x) # 128, 315, 315\n",
    "        d2 = self.dec2(torch.cat([x, e2], 1)) # 128, 315, 315\n",
    "        \n",
    "        x = self.upconv1(d2) # 128, 315, 315 -> 64, 630, 630\n",
    "        x = self.center_crop(e1, x) # 64, 630, 630\n",
    "        d1 = self.dec1(torch.cat([x, e1], 1)) # 64, 630, 630\n",
    "        \n",
    "        return d1 # 64, 630, 630\n",
    "\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels=32, out_channels=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 다중 스케일 특징 추출\n",
    "        self.aspp = nn.ModuleList([ # 64, 630, 630 -> 64, 630, 630\n",
    "            # 1x1 convolution\n",
    "            nn.Sequential( # 64, 630, 630 -> 64, 630, 630\n",
    "                nn.Conv2d(in_channels, 32, 1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            # 3x3 dilated convolutions\n",
    "            # Output Size = [Input Size + 2*padding - dilation*(kernel_size - 1) - 1 / stride] + 1\n",
    "\n",
    "            nn.Sequential( # 64, 630, 630 -> 64, 630, 630\n",
    "                nn.Conv2d(in_channels, 32, 3, padding=6, dilation=6),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential( # 64, 630, 630 -> 64, 630, 630\n",
    "                nn.Conv2d(in_channels, 32, 3, padding=12, dilation=12),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            # Global context\n",
    "            nn.Sequential( # 64, 630, 630 -> 64, 630, 630\n",
    "                nn.AdaptiveAvgPool2d((630,630)),\n",
    "                nn.Conv2d(in_channels, 32, 1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # 특징 융합\n",
    "        self.fusion = nn.Sequential( # 64, 630, 630 -> 64, 630, 630\n",
    "            C3k2(32 * 4, 64, shortcut=False), # 64 * 4, 630, 630 -> 128, 630, 630\n",
    "            C2f(64, 32, 1), # 128, 630, 630 -> 64, 630, 630\n",
    "            SPPF(32, 32) # 64, 630, 630 -> 64, 630, 630\n",
    "        )\n",
    "        \n",
    "        # 최종 예측\n",
    "        self.final = nn.Sequential( # 64, 630, 630 -> 64, 630, 630\n",
    "            nn.Conv2d(32, 16, 3, padding=1), # 64, 630, 630 -> 32, 630, 630\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(16, out_channels, 1) # 32, 630, 630 -> 1, 630, 630\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ASPP 특징 추출\n",
    "        aspp_features = []\n",
    "        for aspp_module in self.aspp:\n",
    "            if isinstance(aspp_module[-1], nn.ReLU):\n",
    "                aspp_features.append(aspp_module(x))\n",
    "            else:\n",
    "                # Global context 처리\n",
    "                global_feature = aspp_module(x)\n",
    "                aspp_features.append(F.interpolate(\n",
    "                    global_feature, \n",
    "                    size=x.shape[2:], \n",
    "                    mode='bilinear', \n",
    "                    align_corners=True\n",
    "                ))\n",
    "        \n",
    "        # 특징 연결 및 융합\n",
    "        fused = torch.cat(aspp_features, dim=1) # 64, 630, 630\n",
    "        refined = self.fusion(fused) # 64, 630, 630\n",
    "        \n",
    "        # 최종 예측\n",
    "        return self.final(refined) # 1, 630, 630\n",
    "    \n",
    "class CoordPredictionHead(nn.Module):\n",
    "    def __init__(self, in_channels=64):\n",
    "        super().__init__()\n",
    "        self.coord_head = nn.Sequential(\n",
    "            SPPF(in_channels, 32),       # 특징 피라미드 풀링\n",
    "            C2PSA(32, 16),              # 공간 attention\n",
    "            nn.Conv2d(16, 2, 1)         # x,y 좌표 예측\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.coord_head(x)\n",
    "\n",
    "# 전체 모델 정의\n",
    "class ETModel(nn.Module):\n",
    "    def __init__(self, backbone, detect_head):\n",
    "        \"\"\"\n",
    "        Full model combining backbone and detection head.\n",
    "        \"\"\"\n",
    "        super(ETModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.detect_head = detect_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the full model.\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)  # Extract features\n",
    "        output = self.detect_head(features)  # Detection/segmentation output\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 모델 초기화\u001b[39;00m\n\u001b[0;32m      6\u001b[0m backbone \u001b[38;5;241m=\u001b[39m Backbone()\n\u001b[1;32m----> 7\u001b[0m detect_head \u001b[38;5;241m=\u001b[39m \u001b[43mCoordPredictionHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m ETModel(backbone, detect_head)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# # 학습용 데이터 예시 for segmentation\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# x = torch.randn(2, 1, 630, 630)  # Batch size 8, 1-channel ET images\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# y = torch.randint(0, 6, (2, 630, 630))  # Multi-class segmentation labels\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 학습용 데이터 예시 for detection\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 184\u001b[0m, in \u001b[0;36mCoordPredictionHead.__init__\u001b[1;34m(self, in_channels)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m    183\u001b[0m         SPPF(in_channels, \u001b[38;5;241m32\u001b[39m),       \u001b[38;5;66;03m# 특징 피라미드 풀링\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m         \u001b[43mC2PSA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m,              \u001b[38;5;66;03m# 공간 attention\u001b[39;00m\n\u001b[0;32m    185\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)         \u001b[38;5;66;03m# x,y 좌표 예측\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[2], line 303\u001b[0m, in \u001b[0;36mC2PSA.__init__\u001b[1;34m(self, c1, c2, n, e)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes the C2PSA module with specified input/output channels, number of layers, and expansion ratio.\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m c1 \u001b[38;5;241m==\u001b[39m c2\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(c1 \u001b[38;5;241m*\u001b[39m e)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1 \u001b[38;5;241m=\u001b[39m Conv(c1, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 손실 함수 및 평가 지표 정의\n",
    "loss_function = TverskyLoss(include_background=True, to_onehot_y=True, softmax=True)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)\n",
    "\n",
    "# 모델 초기화\n",
    "backbone = Backbone()\n",
    "detect_head = CoordPredictionHead()\n",
    "model = ETModel(backbone, detect_head)\n",
    "\n",
    "# # 학습용 데이터 예시 for segmentation\n",
    "# x = torch.randn(2, 1, 630, 630)  # Batch size 8, 1-channel ET images\n",
    "# y = torch.randint(0, 6, (2, 630, 630))  # Multi-class segmentation labels\n",
    "\n",
    "# 학습용 데이터 예시 for detection\n",
    "x = torch.randn(2, 1, 630, 630)  # 입력 이미지\n",
    "y = torch.randn(2, 2, 630, 630)  # x,y 좌표 레이블\n",
    "\n",
    "# Forward Pass\n",
    "# outputs = model(x)\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "model = model.to('cuda')\n",
    "x = x.to('cuda')\n",
    "y = y.to('cuda')\n",
    "summary(model, input_size=(2, 1, 630, 630))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(ckpt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_yolo_unet.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# YOLO로 로드\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_yolo_unet.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m result \u001b[38;5;241m=\u001b[39m yolo_model\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     50\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms_640_dropout025_\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     60\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\ultralytics\\engine\\model.py:145\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\ultralytics\\engine\\model.py:285\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    282\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:912\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    910\u001b[0m ckpt, weight \u001b[38;5;241m=\u001b[39m torch_safe_load(weight)  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m    911\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Model compatibility updates\u001b[39;00m\n\u001b[0;32m    915\u001b[0m model\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m DEFAULT_CFG_KEYS}  \u001b[38;5;66;03m# attach args to model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from ultralytics import YOLO\n",
    "# from ultralytics.nn.tasks import BaseModel\n",
    "\n",
    "# class CustomYOLOUNet(BaseModel):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.backbone = Backbone()\n",
    "#         self.head = SegmentationHead()\n",
    "#         self.model = ETModel(self.backbone, self.head)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "        \n",
    "#     def _predict_once(self, batch):\n",
    "#         # YOLO 예측 메서드 구현\n",
    "#         return self.forward(batch['img'])\n",
    "        \n",
    "#     def get_state_dict(self):\n",
    "#         return {\n",
    "#             'model': self.model.state_dict(),\n",
    "#             'backbone': self.backbone.state_dict(),\n",
    "#             'head': self.head.state_dict()\n",
    "#         }\n",
    "\n",
    "# # 모델 초기화 및 저장\n",
    "# model = CustomYOLOUNet()\n",
    "# ckpt = {\n",
    "#     'model': model.get_state_dict(),  # state_dict 형태로 저장\n",
    "#     'epoch': -1,\n",
    "#     'version': None,\n",
    "#     'optimizer': None,\n",
    "#     'model_args': {},\n",
    "#     'train_args': {},\n",
    "#     'task': 'segment',\n",
    "#     'box': None,\n",
    "#     'names': {0: 'apo-ferritin',\n",
    "#             1: 'beta-amylase',\n",
    "#             2: 'beta-galactosidase',\n",
    "#             3: 'ribosome',\n",
    "#             4: 'thyroglobulin',\n",
    "#             5: 'virus-like-particle'}\n",
    "# }\n",
    "\n",
    "# torch.save(ckpt, 'custom_yolo_unet.pt')\n",
    "\n",
    "# # YOLO로 로드\n",
    "# yolo_model = YOLO('custom_yolo_unet.pt')\n",
    "# result = yolo_model.train(\n",
    "#     name=\"s_640_dropout025_\",\n",
    "#     data=\"./data.yaml\",\n",
    "#     epochs=300,\n",
    "#     patience=10,\n",
    "#     batch=8,\n",
    "#     imgsz=630,\n",
    "#     optimizer='AdamW',\n",
    "#     lr0=0.001,\n",
    "#     dropout=0.25,\n",
    "#     exist_ok=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 손실 계산\n",
    "# y_onehot = F.one_hot(y, num_classes=6).permute(0, 3, 1, 2).float()  # One-hot encode the target\n",
    "# # 손실 계산\n",
    "# loss = loss_function(outputs, y.unsqueeze(1))  # 타겟에 채널 차원 추가 (B, 1, H, W)\n",
    "\n",
    "# # 평가 지표 계산\n",
    "# dice_metric(outputs, y_onehot)\n",
    "# dice_score = dice_metric.aggregate().item()\n",
    "# dice_metric.reset()\n",
    "\n",
    "# print(\"Loss:\", loss.item())\n",
    "# print(\"Dice Metric:\", dice_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Optimizer (학습 루프에서 활용)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # 학습 루프 (간단한 예)\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     outputs = model(x)\n",
    "#     loss = loss_function(outputs, y_onehot)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         dice_metric(outputs, y_onehot)\n",
    "#         dice_score = dice_metric.aggregate().item()\n",
    "#         dice_metric.reset()\n",
    "#         print(f\"Epoch {epoch + 1}, Dice Metric: {dice_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
