{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from torch.nn.init import constant_, xavier_uniform_\n",
    "\n",
    "from ultralytics.nn.modules.block import C3k2, SPPF, C2PSA\n",
    "from ultralytics.nn.modules.conv import Conv\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "\n",
    "x = torch.randn(1, 1, 640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#YOLO's nn module\n",
    "class SPPF(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=5):\n",
    "        \"\"\"\n",
    "        Initializes the SPPF layer with given input/output channels and kernel size.\n",
    "\n",
    "        This module is equivalent to SPP(k=(5, 9, 13)).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = c1 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
    "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through Ghost Convolution block.\"\"\"\n",
    "        y = [self.cv1(x)]\n",
    "        y.extend(self.m(y[-1]) for _ in range(3))\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "    \n",
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class DWConv(Conv):\n",
    "    \"\"\"Depth-wise convolution.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):  # ch_in, ch_out, kernel, stride, dilation, activation\n",
    "        \"\"\"Initialize Depth-wise convolution with given parameters.\"\"\"\n",
    "        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n",
    "\n",
    "\n",
    "class DWConvTranspose2d(nn.ConvTranspose2d):\n",
    "    \"\"\"Depth-wise transpose convolution.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out\n",
    "        \"\"\"Initialize DWConvTranspose2d class with given parameters.\"\"\"\n",
    "        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))\n",
    "\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    \"\"\"Convolution transpose 2d layer.\"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=2, s=2, p=0, bn=True, act=True):\n",
    "        \"\"\"Initialize ConvTranspose2d layer with batch normalization and activation function.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(c1, c2, k, s, p, bias=not bn)\n",
    "        self.bn = nn.BatchNorm2d(c2) if bn else nn.Identity()\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies transposed convolutions, batch normalization and activation to input.\"\"\"\n",
    "        return self.act(self.bn(self.conv_transpose(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Applies activation and convolution transpose operation to input.\"\"\"\n",
    "        return self.act(self.conv_transpose(x))\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
    "        \"\"\"Initializes a standard bottleneck module with optional shortcut connection and configurable parameters.\"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
    "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the YOLO FPN to input data.\"\"\"\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "    \n",
    "class Proto(nn.Module):\n",
    "    \"\"\"YOLOv8 mask Proto module for segmentation models.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c_=256, c2=32):\n",
    "        \"\"\"\n",
    "        Initializes the YOLOv8 mask Proto module with specified number of protos and masks.\n",
    "\n",
    "        Input arguments are ch_in, number of protos, number of masks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cv1 = Conv(c1, c_, k=3)\n",
    "        self.upsample = nn.ConvTranspose2d(c_, c_, 2, 2, 0, bias=True)  # nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.cv2 = Conv(c_, c_, k=3)\n",
    "        self.cv3 = Conv(c_, c2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass through layers using an upsampled input image.\"\"\"\n",
    "        return self.cv3(self.cv2(self.upsample(self.cv1(x))))\n",
    "\n",
    "    \n",
    "class C2f(nn.Module):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
    "        \"\"\"Initializes a CSP bottleneck with 2 convolutions and n Bottleneck blocks for faster processing.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through C2f layer.\"\"\"\n",
    "        y = list(self.cv1(x).chunk(2, 1))\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "    def forward_split(self, x):\n",
    "        \"\"\"Forward pass using split() instead of chunk().\"\"\"\n",
    "        y = self.cv1(x).split((self.c, self.c), 1)\n",
    "        y = [y[0], y[1]]\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "class C3(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with 3 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n",
    "        \"\"\"Initialize the CSP Bottleneck with given channels, number, shortcut, groups, and expansion values.\"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c1, c_, 1, 1)\n",
    "        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\n",
    "        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
    "    \n",
    "class C3k2(C2f):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):\n",
    "        \"\"\"Initializes the C3k2 module, a faster CSP Bottleneck with 2 convolutions and optional C3k blocks.\"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        self.m = nn.ModuleList(\n",
    "            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)\n",
    "        )\n",
    "\n",
    "class C3k(C3):\n",
    "    \"\"\"C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):\n",
    "        \"\"\"Initializes the C3k module with specified channels, number of layers, and configurations.\"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        # self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention module that performs self-attention on the input tensor.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input tensor dimension.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        attn_ratio (float): The ratio of the attention key dimension to the head dimension.\n",
    "\n",
    "    Attributes:\n",
    "        num_heads (int): The number of attention heads.\n",
    "        head_dim (int): The dimension of each attention head.\n",
    "        key_dim (int): The dimension of the attention key.\n",
    "        scale (float): The scaling factor for the attention scores.\n",
    "        qkv (Conv): Convolutional layer for computing the query, key, and value.\n",
    "        proj (Conv): Convolutional layer for projecting the attended values.\n",
    "        pe (Conv): Convolutional layer for positional encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, attn_ratio=0.5):\n",
    "        \"\"\"Initializes multi-head attention module with query, key, and value convolutions and positional encoding.\"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.key_dim = int(self.head_dim * attn_ratio)\n",
    "        self.scale = self.key_dim**-0.5\n",
    "        nh_kd = self.key_dim * num_heads\n",
    "        h = dim + nh_kd * 2\n",
    "        self.qkv = Conv(dim, h, 1, act=False)\n",
    "        self.proj = Conv(dim, dim, 1, act=False)\n",
    "        self.pe = Conv(dim, dim, 3, 1, g=dim, act=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Attention module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): The output tensor after self-attention.\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        N = H * W\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.view(B, self.num_heads, self.key_dim * 2 + self.head_dim, N).split(\n",
    "            [self.key_dim, self.key_dim, self.head_dim], dim=2\n",
    "        )\n",
    "\n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (v @ attn.transpose(-2, -1)).view(B, C, H, W) + self.pe(v.reshape(B, C, H, W))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class PSABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    PSABlock class implementing a Position-Sensitive Attention block for neural networks.\n",
    "\n",
    "    This class encapsulates the functionality for applying multi-head attention and feed-forward neural network layers\n",
    "    with optional shortcut connections.\n",
    "\n",
    "    Attributes:\n",
    "        attn (Attention): Multi-head attention module.\n",
    "        ffn (nn.Sequential): Feed-forward neural network module.\n",
    "        add (bool): Flag indicating whether to add shortcut connections.\n",
    "\n",
    "    Methods:\n",
    "        forward: Performs a forward pass through the PSABlock, applying attention and feed-forward layers.\n",
    "\n",
    "    Examples:\n",
    "        Create a PSABlock and perform a forward pass\n",
    "        >>> psablock = PSABlock(c=128, attn_ratio=0.5, num_heads=4, shortcut=True)\n",
    "        >>> input_tensor = torch.randn(1, 128, 32, 32)\n",
    "        >>> output_tensor = psablock(input_tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c, attn_ratio=0.5, num_heads=4, shortcut=True) -> None:\n",
    "        \"\"\"Initializes the PSABlock with attention and feed-forward layers for enhanced feature extraction.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = Attention(c, attn_ratio=attn_ratio, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(Conv(c, c * 2, 1), Conv(c * 2, c, 1, act=False))\n",
    "        self.add = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Executes a forward pass through PSABlock, applying attention and feed-forward layers to the input tensor.\"\"\"\n",
    "        x = x + self.attn(x) if self.add else self.attn(x)\n",
    "        x = x + self.ffn(x) if self.add else self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class C2PSA(nn.Module):\n",
    "    \"\"\"\n",
    "    C2PSA module with attention mechanism for enhanced feature extraction and processing.\n",
    "\n",
    "    This module implements a convolutional block with attention mechanisms to enhance feature extraction and processing\n",
    "    capabilities. It includes a series of PSABlock modules for self-attention and feed-forward operations.\n",
    "\n",
    "    Attributes:\n",
    "        c (int): Number of hidden channels.\n",
    "        cv1 (Conv): 1x1 convolution layer to reduce the number of input channels to 2*c.\n",
    "        cv2 (Conv): 1x1 convolution layer to reduce the number of output channels to c.\n",
    "        m (nn.Sequential): Sequential container of PSABlock modules for attention and feed-forward operations.\n",
    "\n",
    "    Methods:\n",
    "        forward: Performs a forward pass through the C2PSA module, applying attention and feed-forward operations.\n",
    "\n",
    "    Notes:\n",
    "        This module essentially is the same as PSA module, but refactored to allow stacking more PSABlock modules.\n",
    "\n",
    "    Examples:\n",
    "        >>> c2psa = C2PSA(c1=256, c2=256, n=3, e=0.5)\n",
    "        >>> input_tensor = torch.randn(1, 256, 64, 64)\n",
    "        >>> output_tensor = c2psa(input_tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, e=0.5):\n",
    "        \"\"\"Initializes the C2PSA module with specified input/output channels, number of layers, and expansion ratio.\"\"\"\n",
    "        super().__init__()\n",
    "        assert c1 == c2\n",
    "        self.c = int(c1 * e)\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv(2 * self.c, c1, 1)\n",
    "\n",
    "        self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=self.c // 64) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Processes the input tensor 'x' through a series of PSA blocks and returns the transformed tensor.\"\"\"\n",
    "        a, b = self.cv1(x).split((self.c, self.c), dim=1)\n",
    "        b = self.m(b)\n",
    "        return self.cv2(torch.cat((a, b), 1))\n",
    "    \n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = distance.chunk(2, dim)\n",
    "    x1y1 = anchor_points - lt\n",
    "    x2y2 = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2\n",
    "        wh = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    \"\"\"YOLO Detect head for detection models.\"\"\"\n",
    "\n",
    "    dynamic = False  # force grid reconstruction\n",
    "    export = False  # export mode\n",
    "    end2end = False  # end2end\n",
    "    max_det = 300  # max_det\n",
    "    shape = None\n",
    "    anchors = torch.empty(0)  # init\n",
    "    strides = torch.empty(0)  # init\n",
    "    legacy = False  # backward compatibility for v3/v5/v8/v9 models\n",
    "\n",
    "    def __init__(self, nc=80, ch=()):\n",
    "        \"\"\"Initializes the YOLO detection layer with specified number of classes and channels.\"\"\"\n",
    "        super().__init__()\n",
    "        self.nc = nc  # number of classes\n",
    "        self.nl = len(ch)  # number of detection layers\n",
    "        self.reg_max = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)\n",
    "        self.no = nc + self.reg_max * 4  # number of outputs per anchor\n",
    "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
    "        c2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))  # channels\n",
    "        self.cv2 = nn.ModuleList(\n",
    "            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch\n",
    "        )\n",
    "        self.cv3 = (\n",
    "            nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch)\n",
    "            if self.legacy\n",
    "            else nn.ModuleList(\n",
    "                nn.Sequential(\n",
    "                    nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),\n",
    "                    nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),\n",
    "                    nn.Conv2d(c3, self.nc, 1),\n",
    "                )\n",
    "                for x in ch\n",
    "            )\n",
    "        )\n",
    "        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n",
    "\n",
    "        if self.end2end:\n",
    "            self.one2one_cv2 = copy.deepcopy(self.cv2)\n",
    "            self.one2one_cv3 = copy.deepcopy(self.cv3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\n",
    "        if self.end2end:\n",
    "            return self.forward_end2end(x)\n",
    "\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n",
    "        if self.training:  # Training path\n",
    "            return x\n",
    "        y = self._inference(x)\n",
    "        return y if self.export else (y, x)\n",
    "\n",
    "    def forward_end2end(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass of the v10Detect module.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (dict, tensor): If not in training mode, returns a dictionary containing the outputs of both one2many and one2one detections.\n",
    "                           If in training mode, returns a dictionary containing the outputs of one2many and one2one detections separately.\n",
    "        \"\"\"\n",
    "        x_detach = [xi.detach() for xi in x]\n",
    "        one2one = [\n",
    "            torch.cat((self.one2one_cv2[i](x_detach[i]), self.one2one_cv3[i](x_detach[i])), 1) for i in range(self.nl)\n",
    "        ]\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n",
    "        if self.training:  # Training path\n",
    "            return {\"one2many\": x, \"one2one\": one2one}\n",
    "\n",
    "        y = self._inference(one2one)\n",
    "        y = self.postprocess(y.permute(0, 2, 1), self.max_det, self.nc)\n",
    "        return y if self.export else (y, {\"one2many\": x, \"one2one\": one2one})\n",
    "\n",
    "    def _inference(self, x):\n",
    "        \"\"\"Decode predicted bounding boxes and class probabilities based on multiple-level feature maps.\"\"\"\n",
    "        # Inference path\n",
    "        shape = x[0].shape  # BCHW\n",
    "        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)\n",
    "        if self.dynamic or self.shape != shape:\n",
    "            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
    "            self.shape = shape\n",
    "\n",
    "        if self.export and self.format in {\"saved_model\", \"pb\", \"tflite\", \"edgetpu\", \"tfjs\"}:  # avoid TF FlexSplitV ops\n",
    "            box = x_cat[:, : self.reg_max * 4]\n",
    "            cls = x_cat[:, self.reg_max * 4 :]\n",
    "        else:\n",
    "            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)\n",
    "\n",
    "        if self.export and self.format in {\"tflite\", \"edgetpu\"}:\n",
    "            # Precompute normalization factor to increase numerical stability\n",
    "            # See https://github.com/ultralytics/ultralytics/issues/7371\n",
    "            grid_h = shape[2]\n",
    "            grid_w = shape[3]\n",
    "            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)\n",
    "            norm = self.strides / (self.stride[0] * grid_size)\n",
    "            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])\n",
    "        else:\n",
    "            dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides\n",
    "\n",
    "        return torch.cat((dbox, cls.sigmoid()), 1)\n",
    "\n",
    "    def bias_init(self):\n",
    "        \"\"\"Initialize Detect() biases, WARNING: requires stride availability.\"\"\"\n",
    "        m = self  # self.model[-1]  # Detect() module\n",
    "        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1\n",
    "        # ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency\n",
    "        for a, b, s in zip(m.cv2, m.cv3, m.stride):  # from\n",
    "            a[-1].bias.data[:] = 1.0  # box\n",
    "            b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)\n",
    "        if self.end2end:\n",
    "            for a, b, s in zip(m.one2one_cv2, m.one2one_cv3, m.stride):  # from\n",
    "                a[-1].bias.data[:] = 1.0  # box\n",
    "                b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)\n",
    "\n",
    "    def decode_bboxes(self, bboxes, anchors):\n",
    "        \"\"\"Decode bounding boxes.\"\"\"\n",
    "        return dist2bbox(bboxes, anchors, xywh=not self.end2end, dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def postprocess(preds: torch.Tensor, max_det: int, nc: int = 80):\n",
    "        \"\"\"\n",
    "        Post-processes YOLO model predictions.\n",
    "\n",
    "        Args:\n",
    "            preds (torch.Tensor): Raw predictions with shape (batch_size, num_anchors, 4 + nc) with last dimension\n",
    "                format [x, y, w, h, class_probs].\n",
    "            max_det (int): Maximum detections per image.\n",
    "            nc (int, optional): Number of classes. Default: 80.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Processed predictions with shape (batch_size, min(max_det, num_anchors), 6) and last\n",
    "                dimension format [x, y, w, h, max_class_prob, class_index].\n",
    "        \"\"\"\n",
    "        batch_size, anchors, _ = preds.shape  # i.e. shape(16,8400,84)\n",
    "        boxes, scores = preds.split([4, nc], dim=-1)\n",
    "        index = scores.amax(dim=-1).topk(min(max_det, anchors))[1].unsqueeze(-1)\n",
    "        boxes = boxes.gather(dim=1, index=index.repeat(1, 1, 4))\n",
    "        scores = scores.gather(dim=1, index=index.repeat(1, 1, nc))\n",
    "        scores, index = scores.flatten(1).topk(min(max_det, anchors))\n",
    "        i = torch.arange(batch_size)[..., None]  # batch indices\n",
    "        return torch.cat([boxes[i, index // nc], scores[..., None], (index % nc)[..., None].float()], dim=-1)\n",
    "\n",
    "\n",
    "class Segment(Detect):\n",
    "    \"\"\"YOLO Segment head for segmentation models.\"\"\"\n",
    "\n",
    "    def __init__(self, nc=80, nm=32, npr=256, ch=()):\n",
    "        \"\"\"Initialize the YOLO model attributes such as the number of masks, prototypes, and the convolution layers.\"\"\"\n",
    "        super().__init__(nc, ch)\n",
    "        self.nm = nm  # number of masks\n",
    "        self.npr = npr  # number of protos\n",
    "        self.proto = Proto(ch[0], self.npr, self.nm)  # protos\n",
    "\n",
    "        c4 = max(ch[0] // 4, self.nm)\n",
    "        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients.\"\"\"\n",
    "        p = self.proto(x[0])  # mask protos\n",
    "        bs = p.shape[0]  # batch size\n",
    "\n",
    "        mc = torch.cat([self.cv4[i](x[i]).view(bs, self.nm, -1) for i in range(self.nl)], 2)  # mask coefficients\n",
    "        x = Detect.forward(self, x)\n",
    "        if self.training:\n",
    "            return x, mc, p\n",
    "        return (torch.cat([x, mc], 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "# Backbone 정의\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Simple feature extractor backbone.\n",
    "        \"\"\"\n",
    "        self.layer1 = C2f(1, 64, n=1)  # [Batch, 64, H/2, W/2]\n",
    "        self.layer2 = C2f(64, 128, n=2)  # [Batch, 128, H/4, W/4]\n",
    "        self.layer3 = C2f(128, 256, n=4)  # [Batch, 256, H/8, W/8]\n",
    "        self.layer4 = C2f(256, 512, n=6)  # [Batch, 512, H/16, W/16]\n",
    "        self.layer5 = C2f(512, 1024, n=4)  # [Batch, 1024, H/32, W/32]\n",
    "        \n",
    "        self.c2psa = C2PSA(1024, 1024, n=3, e=0.5)  # [Batch, 1024, H/32, W/32]\n",
    "        \n",
    "        self.segment_head = Segment(nc=6, nm=32, npr=256, ch=[1024])  # [Batch, 6, H/32, W/32]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for feature extraction.\n",
    "        \"\"\"\n",
    "        out1 = self.layer1(x)  # [Batch, 16, H/2, W/2]\n",
    "        out2 = self.layer2(out1)  # [Batch, 32, H/4, W/4]\n",
    "        out3 = self.layer3(out2)  # [Batch, 64, H/8, W/8]\n",
    "        return [out1, out2, out3]  # Multi-scale outputs\n",
    "\n",
    "# YOLO 스타일 Detect 헤드 정의\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, nc=6, ch=()):\n",
    "        \"\"\"\n",
    "        Detection head for multi-class segmentation.\n",
    "        Args:\n",
    "            nc (int): Number of classes.\n",
    "            ch (tuple): Input channel sizes from backbone layers.\n",
    "        \"\"\"\n",
    "        super(Detect, self).__init__()\n",
    "        self.nc = nc  # Number of classes\n",
    "        self.nl = len(ch)  # Number of input layers\n",
    "        self.cv = nn.ModuleList(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(x, x // 2, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(x // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(x // 2, self.nc, kernel_size=1)  # Output logits\n",
    "            )\n",
    "            for x in ch\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the detection head.\n",
    "        \"\"\"\n",
    "        # Process each feature map\n",
    "        outputs = [self.cv[i](xi) for i, xi in enumerate(x)]\n",
    "        # Upsample all outputs to the largest feature map size\n",
    "        outputs_upsampled = [\n",
    "            F.interpolate(out, size=outputs[0].shape[2:], mode='bilinear', align_corners=False)\n",
    "            for out in outputs\n",
    "        ]\n",
    "        # Combine outputs by averaging\n",
    "        combined_output = torch.stack(outputs_upsampled, dim=0).mean(dim=0)\n",
    "        return combined_output  # Shape: (Batch, Classes, H, W)\n",
    "\n",
    "# 전체 모델 정의\n",
    "class ETModel(nn.Module):\n",
    "    def __init__(self, backbone, detect_head):\n",
    "        \"\"\"\n",
    "        Full model combining backbone and detection head.\n",
    "        \"\"\"\n",
    "        super(ETModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.detect_head = detect_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the full model.\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)  # Extract features\n",
    "        output = self.detect_head(features)  # Detection/segmentation output\n",
    "        return output\n",
    "\n",
    "# 손실 함수 및 평가 지표 정의\n",
    "loss_function = TverskyLoss(include_background=True, to_onehot_y=True, softmax=True)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)\n",
    "\n",
    "# 모델 초기화\n",
    "backbone = Backbone()\n",
    "detect_head = Detect(nc=6, ch=[16, 32, 64])  # Channel sizes from backbone layers\n",
    "model = ETModel(backbone, detect_head)\n",
    "\n",
    "# 학습용 데이터 예시\n",
    "x = torch.randn(8, 1, 630, 630)  # Batch size 8, 1-channel ET images\n",
    "y = torch.randint(0, 6, (8, 315, 315))  # Multi-class segmentation labels\n",
    "\n",
    "# Forward Pass\n",
    "outputs = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8361127376556396\n",
      "Dice Metric: 0.2858162522315979\n"
     ]
    }
   ],
   "source": [
    "# 손실 계산\n",
    "y_onehot = F.one_hot(y, num_classes=6).permute(0, 3, 1, 2).float()  # One-hot encode the target\n",
    "# 손실 계산\n",
    "loss = loss_function(outputs, y.unsqueeze(1))  # 타겟에 채널 차원 추가 (B, 1, H, W)\n",
    "\n",
    "# 평가 지표 계산\n",
    "dice_metric(outputs, y_onehot)\n",
    "dice_score = dice_metric.aggregate().item()\n",
    "dice_metric.reset()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Dice Metric:\", dice_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer (학습 루프에서 활용)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 학습 루프 (간단한 예)\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(x)\n",
    "    loss = loss_function(outputs, y_onehot)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dice_metric(outputs, y_onehot)\n",
    "        dice_score = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "        print(f\"Epoch {epoch + 1}, Dice Metric: {dice_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
