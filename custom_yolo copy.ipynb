{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from torch.nn.init import constant_, xavier_uniform_\n",
    "\n",
    "from ultralytics.nn.modules.block import C3k2, SPPF, C2PSA\n",
    "from ultralytics.nn.modules.conv import Conv\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "\n",
    "x = torch.randn(1, 1, 640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#YOLO's nn module\n",
    "class SPPF(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=5):\n",
    "        \"\"\"\n",
    "        Initializes the SPPF layer with given input/output channels and kernel size.\n",
    "\n",
    "        This module is equivalent to SPP(k=(5, 9, 13)).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = c1 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
    "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through Ghost Convolution block.\"\"\"\n",
    "        y = [self.cv1(x)]\n",
    "        y.extend(self.m(y[-1]) for _ in range(3))\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "    \n",
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "class DWConv(Conv):\n",
    "    \"\"\"Depth-wise convolution.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):  # ch_in, ch_out, kernel, stride, dilation, activation\n",
    "        \"\"\"Initialize Depth-wise convolution with given parameters.\"\"\"\n",
    "        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n",
    "\n",
    "\n",
    "class DWConvTranspose2d(nn.ConvTranspose2d):\n",
    "    \"\"\"Depth-wise transpose convolution.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out\n",
    "        \"\"\"Initialize DWConvTranspose2d class with given parameters.\"\"\"\n",
    "        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))\n",
    "\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    \"\"\"Convolution transpose 2d layer.\"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=2, s=2, p=0, bn=True, act=True):\n",
    "        \"\"\"Initialize ConvTranspose2d layer with batch normalization and activation function.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(c1, c2, k, s, p, bias=not bn)\n",
    "        self.bn = nn.BatchNorm2d(c2) if bn else nn.Identity()\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies transposed convolutions, batch normalization and activation to input.\"\"\"\n",
    "        return self.act(self.bn(self.conv_transpose(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Applies activation and convolution transpose operation to input.\"\"\"\n",
    "        return self.act(self.conv_transpose(x))\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
    "        \"\"\"Initializes a standard bottleneck module with optional shortcut connection and configurable parameters.\"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
    "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the YOLO FPN to input data.\"\"\"\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "    \n",
    "class Proto(nn.Module):\n",
    "    \"\"\"YOLOv8 mask Proto module for segmentation models.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c_=256, c2=32):\n",
    "        \"\"\"\n",
    "        Initializes the YOLOv8 mask Proto module with specified number of protos and masks.\n",
    "\n",
    "        Input arguments are ch_in, number of protos, number of masks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cv1 = Conv(c1, c_, k=3)\n",
    "        self.upsample = nn.ConvTranspose2d(c_, c_, 2, 2, 0, bias=True)  # nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.cv2 = Conv(c_, c_, k=3)\n",
    "        self.cv3 = Conv(c_, c2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass through layers using an upsampled input image.\"\"\"\n",
    "        return self.cv3(self.cv2(self.upsample(self.cv1(x))))\n",
    "\n",
    "    \n",
    "class C2f(nn.Module):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
    "        \"\"\"Initializes a CSP bottleneck with 2 convolutions and n Bottleneck blocks for faster processing.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through C2f layer.\"\"\"\n",
    "        y = list(self.cv1(x).chunk(2, 1))\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "    def forward_split(self, x):\n",
    "        \"\"\"Forward pass using split() instead of chunk().\"\"\"\n",
    "        y = self.cv1(x).split((self.c, self.c), 1)\n",
    "        y = [y[0], y[1]]\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "class C3(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with 3 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n",
    "        \"\"\"Initialize the CSP Bottleneck with given channels, number, shortcut, groups, and expansion values.\"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c1, c_, 1, 1)\n",
    "        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\n",
    "        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
    "    \n",
    "class C3k2(C2f):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):\n",
    "        \"\"\"Initializes the C3k2 module, a faster CSP Bottleneck with 2 convolutions and optional C3k blocks.\"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        self.m = nn.ModuleList(\n",
    "            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)\n",
    "        )\n",
    "\n",
    "class C3k(C3):\n",
    "    \"\"\"C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):\n",
    "        \"\"\"Initializes the C3k module with specified channels, number of layers, and configurations.\"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        # self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention module that performs self-attention on the input tensor.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input tensor dimension.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        attn_ratio (float): The ratio of the attention key dimension to the head dimension.\n",
    "\n",
    "    Attributes:\n",
    "        num_heads (int): The number of attention heads.\n",
    "        head_dim (int): The dimension of each attention head.\n",
    "        key_dim (int): The dimension of the attention key.\n",
    "        scale (float): The scaling factor for the attention scores.\n",
    "        qkv (Conv): Convolutional layer for computing the query, key, and value.\n",
    "        proj (Conv): Convolutional layer for projecting the attended values.\n",
    "        pe (Conv): Convolutional layer for positional encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, attn_ratio=0.5):\n",
    "        \"\"\"Initializes multi-head attention module with query, key, and value convolutions and positional encoding.\"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.key_dim = int(self.head_dim * attn_ratio)\n",
    "        self.scale = self.key_dim**-0.5\n",
    "        nh_kd = self.key_dim * num_heads\n",
    "        h = dim + nh_kd * 2\n",
    "        self.qkv = Conv(dim, h, 1, act=False)\n",
    "        self.proj = Conv(dim, dim, 1, act=False)\n",
    "        self.pe = Conv(dim, dim, 3, 1, g=dim, act=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Attention module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): The output tensor after self-attention.\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        N = H * W\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.view(B, self.num_heads, self.key_dim * 2 + self.head_dim, N).split(\n",
    "            [self.key_dim, self.key_dim, self.head_dim], dim=2\n",
    "        )\n",
    "\n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (v @ attn.transpose(-2, -1)).view(B, C, H, W) + self.pe(v.reshape(B, C, H, W))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class PSABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    PSABlock class implementing a Position-Sensitive Attention block for neural networks.\n",
    "\n",
    "    This class encapsulates the functionality for applying multi-head attention and feed-forward neural network layers\n",
    "    with optional shortcut connections.\n",
    "\n",
    "    Attributes:\n",
    "        attn (Attention): Multi-head attention module.\n",
    "        ffn (nn.Sequential): Feed-forward neural network module.\n",
    "        add (bool): Flag indicating whether to add shortcut connections.\n",
    "\n",
    "    Methods:\n",
    "        forward: Performs a forward pass through the PSABlock, applying attention and feed-forward layers.\n",
    "\n",
    "    Examples:\n",
    "        Create a PSABlock and perform a forward pass\n",
    "        >>> psablock = PSABlock(c=128, attn_ratio=0.5, num_heads=4, shortcut=True)\n",
    "        >>> input_tensor = torch.randn(1, 128, 32, 32)\n",
    "        >>> output_tensor = psablock(input_tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c, attn_ratio=0.5, num_heads=4, shortcut=True) -> None:\n",
    "        \"\"\"Initializes the PSABlock with attention and feed-forward layers for enhanced feature extraction.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = Attention(c, attn_ratio=attn_ratio, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(Conv(c, c * 2, 1), Conv(c * 2, c, 1, act=False))\n",
    "        self.add = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Executes a forward pass through PSABlock, applying attention and feed-forward layers to the input tensor.\"\"\"\n",
    "        x = x + self.attn(x) if self.add else self.attn(x)\n",
    "        x = x + self.ffn(x) if self.add else self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class C2PSA(nn.Module):\n",
    "    \"\"\"\n",
    "    C2PSA module with attention mechanism for enhanced feature extraction and processing.\n",
    "\n",
    "    This module implements a convolutional block with attention mechanisms to enhance feature extraction and processing\n",
    "    capabilities. It includes a series of PSABlock modules for self-attention and feed-forward operations.\n",
    "\n",
    "    Attributes:\n",
    "        c (int): Number of hidden channels.\n",
    "        cv1 (Conv): 1x1 convolution layer to reduce the number of input channels to 2*c.\n",
    "        cv2 (Conv): 1x1 convolution layer to reduce the number of output channels to c.\n",
    "        m (nn.Sequential): Sequential container of PSABlock modules for attention and feed-forward operations.\n",
    "\n",
    "    Methods:\n",
    "        forward: Performs a forward pass through the C2PSA module, applying attention and feed-forward operations.\n",
    "\n",
    "    Notes:\n",
    "        This module essentially is the same as PSA module, but refactored to allow stacking more PSABlock modules.\n",
    "\n",
    "    Examples:\n",
    "        >>> c2psa = C2PSA(c1=256, c2=256, n=3, e=0.5)\n",
    "        >>> input_tensor = torch.randn(1, 256, 64, 64)\n",
    "        >>> output_tensor = c2psa(input_tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, e=0.5):\n",
    "        \"\"\"Initializes the C2PSA module with specified input/output channels, number of layers, and expansion ratio.\"\"\"\n",
    "        super().__init__()\n",
    "        assert c1 == c2\n",
    "        self.c = int(c1 * e)\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv(2 * self.c, c1, 1)\n",
    "\n",
    "        self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=self.c // 64) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Processes the input tensor 'x' through a series of PSA blocks and returns the transformed tensor.\"\"\"\n",
    "        a, b = self.cv1(x).split((self.c, self.c), dim=1)\n",
    "        b = self.m(b)\n",
    "        return self.cv2(torch.cat((a, b), 1))\n",
    "    \n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = distance.chunk(2, dim)\n",
    "    x1y1 = anchor_points - lt\n",
    "    x2y2 = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2\n",
    "        wh = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 156 but got size 157 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 131\u001b[0m\n\u001b[0;32m    129\u001b[0m model \u001b[38;5;241m=\u001b[39m SegmentationModel()\n\u001b[0;32m    130\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m630\u001b[39m, \u001b[38;5;241m630\u001b[39m)\n\u001b[1;32m--> 131\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 125\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    124\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(x)\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 111\u001b[0m, in \u001b[0;36mSegmentationHead.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    108\u001b[0m e1, e2, e3, e4, bridge \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m    110\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec4(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup4(bridge), e4], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 111\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec3(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup3\u001b[49m\u001b[43m(\u001b[49m\u001b[43md4\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    112\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec2(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(d3), e2], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    113\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec1(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(d2), e1], \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 156 but got size 157 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "# Backbone 정의\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics.nn.modules import *\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, nc=1):\n",
    "        super().__init__()\n",
    "        # 인코더 (C3K2 blocks 추가)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            C2f(1, 64, 1),\n",
    "            C3k2(64, 64, shortcut=False)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = nn.Sequential(\n",
    "            C2f(64, 128, 1),\n",
    "            C3k2(128, 128, shortcut=False)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = nn.Sequential(\n",
    "            C2f(128, 256, 1),\n",
    "            C3k2(256, 256, shortcut=False)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = nn.Sequential(\n",
    "            C2f(256, 512, 1),\n",
    "            C3k2(512, 512, shortcut=False)\n",
    "        )\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 브릿지 (SPPF + C2PSA)\n",
    "        self.bridge = nn.Sequential(\n",
    "            C2f(512, 1024, 1),\n",
    "            SPPF(1024, 1024),  # Spatial Pyramid Pooling - Fast\n",
    "            C2PSA(1024, 1024)  # CSP with Parallel Spatial Attention\n",
    "        )\n",
    "        \n",
    "        # 디코더 (C3K2 blocks 추가)\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec4 = nn.Sequential(\n",
    "            C2f(1024, 512, 1),\n",
    "            C3k2(512, 512, shortcut=False)\n",
    "        )\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            C2f(512, 256, 1),\n",
    "            C3k2(256, 256, shortcut=False)\n",
    "        )\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            C2f(256, 128, 1),\n",
    "            C3k2(128, 128, shortcut=False)\n",
    "        )\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            C2f(128, 64, 1),\n",
    "            C3k2(64, 64, shortcut=False)\n",
    "        )\n",
    "        \n",
    "        # self.out = nn.Conv2d(64, nc, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 인코더\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        e4 = self.enc4(self.pool3(e3))\n",
    "        \n",
    "        # 브릿지\n",
    "        b = self.bridge(self.pool4(e4))\n",
    "        \n",
    "        # 디코더\n",
    "        d4 = self.dec4(torch.cat([self.upconv4(b), e4], 1))\n",
    "        d3 = self.dec3(torch.cat([self.upconv3(d4), e3], 1))\n",
    "        d2 = self.dec2(torch.cat([self.upconv2(d3), e2], 1))\n",
    "        d1 = self.dec1(torch.cat([self.upconv1(d2), e1], 1))\n",
    "        \n",
    "        return d1\n",
    "\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 다중 스케일 특징 추출\n",
    "        self.aspp = nn.ModuleList([\n",
    "            # 1x1 convolution\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, 64, 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            # 3x3 dilated convolutions\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, 64, 3, padding=6, dilation=6),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, 64, 3, padding=12, dilation=12),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            # Global context\n",
    "            nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(in_channels, 64, 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # 특징 융합\n",
    "        self.fusion = nn.Sequential(\n",
    "            C3k2(64 * 4, 128, shortcut=False),\n",
    "            C2f(128, 64, 1),\n",
    "            SPPF(64, 64)\n",
    "        )\n",
    "        \n",
    "        # 최종 예측\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(32, out_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ASPP 특징 추출\n",
    "        aspp_features = []\n",
    "        for aspp_module in self.aspp:\n",
    "            if isinstance(aspp_module[-1], nn.ReLU):\n",
    "                aspp_features.append(aspp_module(x))\n",
    "            else:\n",
    "                # Global context 처리\n",
    "                global_feature = aspp_module(x)\n",
    "                aspp_features.append(F.interpolate(\n",
    "                    global_feature, \n",
    "                    size=x.shape[2:], \n",
    "                    mode='bilinear', \n",
    "                    align_corners=True\n",
    "                ))\n",
    "        \n",
    "        # 특징 연결 및 융합\n",
    "        fused = torch.cat(aspp_features, dim=1)\n",
    "        refined = self.fusion(fused)\n",
    "        \n",
    "        # 최종 예측\n",
    "        return self.final(refined)\n",
    "\n",
    "# 전체 모델 정의\n",
    "class ETModel(nn.Module):\n",
    "    def __init__(self, backbone, detect_head):\n",
    "        \"\"\"\n",
    "        Full model combining backbone and detection head.\n",
    "        \"\"\"\n",
    "        super(ETModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.detect_head = detect_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the full model.\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)  # Extract features\n",
    "        output = self.detect_head(features)  # Detection/segmentation output\n",
    "        return output\n",
    "\n",
    "# 손실 함수 및 평가 지표 정의\n",
    "loss_function = TverskyLoss(include_background=True, to_onehot_y=True, softmax=True)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)\n",
    "\n",
    "# 모델 초기화\n",
    "backbone = Backbone()\n",
    "detect_head = SegmentationHead()\n",
    "model = ETModel(backbone, detect_head)\n",
    "\n",
    "# 학습용 데이터 예시\n",
    "x = torch.randn(8, 1, 630, 630)  # Batch size 8, 1-channel ET images\n",
    "y = torch.randint(0, 6, (8, 315, 315))  # Multi-class segmentation labels\n",
    "\n",
    "# Forward Pass\n",
    "outputs = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8361127376556396\n",
      "Dice Metric: 0.2858162522315979\n"
     ]
    }
   ],
   "source": [
    "# 손실 계산\n",
    "y_onehot = F.one_hot(y, num_classes=6).permute(0, 3, 1, 2).float()  # One-hot encode the target\n",
    "# 손실 계산\n",
    "loss = loss_function(outputs, y.unsqueeze(1))  # 타겟에 채널 차원 추가 (B, 1, H, W)\n",
    "\n",
    "# 평가 지표 계산\n",
    "dice_metric(outputs, y_onehot)\n",
    "dice_score = dice_metric.aggregate().item()\n",
    "dice_metric.reset()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Dice Metric:\", dice_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer (학습 루프에서 활용)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 학습 루프 (간단한 예)\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(x)\n",
    "    loss = loss_function(outputs, y_onehot)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dice_metric(outputs, y_onehot)\n",
    "        dice_score = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "        print(f\"Epoch {epoch + 1}, Dice Metric: {dice_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
