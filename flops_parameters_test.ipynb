{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "import torch\n",
    "from torch.profiler import ProfilerActivity\n",
    "from torch.profiler import profile as profilee\n",
    "    \n",
    "\n",
    "def print_model_summary(model, input_size):\n",
    "    input_tensor = torch.randn(input_size)\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    flops, params = profile(model, inputs=(input_tensor,))\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"FLOPs: {flops:,}, GFLOPs: {flops / 1e9:.2f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def profile_model(model, input_size, log_dir='./log'):\n",
    "\n",
    "    input_tensor = torch.randn(input_size)\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # 프로파일링\n",
    "    with profilee(\n",
    "        activities=[\n",
    "            ProfilerActivity.CPU, \n",
    "            ProfilerActivity.CUDA\n",
    "        ],\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(log_dir),  # TensorBoard 연동\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        model(input_tensor)\n",
    "\n",
    "    # 프로파일링 결과 출력\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "x = (1, 1, 96, 96, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "Model: conv3d_block\n",
      "FLOPs: 5,159,780,352.0, GFLOPs: 5.16\n",
      "Parameters: 884,736.0\n",
      "--------------------------------------------------\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (9) must match the size of tensor b (18) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m conv3d_dp_block \u001b[38;5;241m=\u001b[39m conv3d_dp_block(\u001b[38;5;241m128\u001b[39m, feature, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m print_model_summary(conv3d_block, x)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mprint_model_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv3d_dp_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m profile_model(conv3d_block, x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./log\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m profile_model(conv3d_dp_block, x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./log\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mprint_model_summary\u001b[0;34m(model, input_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     10\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m flops, params \u001b[38;5;241m=\u001b[39m \u001b[43mprofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflops\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, GFLOPs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflops\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dust/lib/python3.12/site-packages/thop/profile.py:192\u001b[0m, in \u001b[0;36mprofile\u001b[0;34m(model, inputs, custom_ops, verbose, ret_layer_info, report_missing)\u001b[0m\n\u001b[1;32m    189\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(add_hooks)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdfs_count\u001b[39m(module: nn\u001b[38;5;241m.\u001b[39mModule, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Recursively counts the total operations and parameters of the given PyTorch module and its submodules.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dust/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dust/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mconv3d_dp_block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     26\u001b[0m     depthwise_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepthwise_conv(x)\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointwise_conv(\u001b[43mdepthwise_conv\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mx\u001b[49m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (18) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "x = (1, 128, 18, 18, 18)\n",
    "feature = 256\n",
    "\n",
    "conv3d = nn.Conv3d(1, feature, 3, padding=1, bias=False)\n",
    "\n",
    "class conv3d_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "        super(conv3d_block, self).__init__()\n",
    "        self.d3conv = nn.Conv3d(in_channels, out_channels, 3, padding=1, bias=False)\n",
    "                                    \n",
    "    def forward(self, x):\n",
    "        x = self.d3conv(x)\n",
    "        return x\n",
    "\n",
    "class conv3d_dp_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "        super(conv3d_dp_block, self).__init__()\n",
    "        self.depthwise_conv = nn.Conv3d(in_channels, in_channels, kernel_size, stride, padding,\n",
    "                                      groups=in_channels, bias=bias)\n",
    "        self.pointwise_conv = nn.Conv3d(in_channels, out_channels, 1, 1, 0, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        depthwise_conv = self.depthwise_conv(x)\n",
    "        \n",
    "        x = self.pointwise_conv(depthwise_conv+x)\n",
    "        return x \n",
    "    \n",
    "conv3d_block = conv3d_block(128, feature, 3, 2, 1, False)\n",
    "conv3d_dp_block = conv3d_dp_block(128, feature, 3, 2, 1, False)\n",
    "\n",
    "print_model_summary(conv3d_block, x)\n",
    "print_model_summary(conv3d_dp_block, x)\n",
    "profile_model(conv3d_block, x, './log')\n",
    "profile_model(conv3d_dp_block, x, './log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_prelu() for <class 'torch.nn.modules.activation.PReLU'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose3d'>.\n",
      "Model: UNet_CBAM\n",
      "FLOPs: 34,904,951,936.0, GFLOPs: 34.90\n",
      "Parameters: 1,965,635.0\n",
      "--------------------------------------------------\n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                    aten::convolution         3.23%     327.200us        26.88%       2.719ms     339.900us     264.000us         2.05%       3.636ms     454.500us             8  \n",
      "                   aten::_convolution         9.28%     938.200us        23.65%       2.392ms     299.000us     572.000us         4.44%       3.372ms     421.500us             8  \n",
      "                            aten::cat         7.80%     788.900us        14.72%       1.489ms     372.300us       2.252ms        17.48%       2.936ms     734.000us             4  \n",
      "                  aten::instance_norm         2.71%     274.100us        26.29%       2.659ms     443.150us     126.000us         0.98%       2.673ms     445.500us             6  \n",
      "                     aten::batch_norm         2.66%     269.200us        22.79%       2.305ms     384.200us     287.000us         2.23%       2.461ms     410.167us             6  \n",
      "         aten::_batch_norm_impl_index         5.44%     550.100us        20.13%       2.036ms     339.333us     575.000us         4.46%       2.174ms     362.333us             6  \n",
      "                         aten::conv3d         1.12%     113.000us        18.12%       1.833ms     366.660us     168.000us         1.30%       2.009ms     401.800us             5  \n",
      "               aten::conv_transpose3d         0.54%      55.100us        10.42%       1.054ms     351.333us      78.000us         0.61%       1.873ms     624.333us             3  \n",
      "    aten::cudnn_convolution_transpose         5.35%     540.700us         5.35%     540.700us     180.233us       1.631ms        12.66%       1.631ms     543.667us             3  \n",
      "              aten::native_batch_norm        11.20%       1.133ms        14.45%       1.462ms     243.650us     997.000us         7.74%       1.562ms     260.333us             6  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 10.115ms\n",
      "Self CUDA time total: 12.886ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import UNet_CBAM\n",
    "model = UNet_CBAM(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=7,\n",
    "    channels=[32, 64, 128, 256],\n",
    "    strides=(2,2,2),\n",
    "\n",
    ").to(device)\n",
    "print_model_summary(model, x)\n",
    "profile_model(model, x, './log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_prelu() for <class 'torch.nn.modules.activation.PReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose3d'>.\n",
      "Model: UNet\n",
      "FLOPs: 34,904,825,856.0, GFLOPs: 34.90\n",
      "Parameters: 1,948,909.0\n",
      "--------------------------------------------------\n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                    aten::convolution         3.98%     180.900us        32.04%       1.456ms     208.029us     176.000us         3.13%       2.829ms     404.143us             7  \n",
      "                   aten::_convolution         6.98%     317.100us        28.06%       1.275ms     182.186us     163.000us         2.90%       2.653ms     379.000us             7  \n",
      "               aten::conv_transpose3d         0.74%      33.600us        10.51%     477.400us     159.133us      89.000us         1.58%       1.659ms     553.000us             3  \n",
      "                  aten::instance_norm         4.89%     222.200us        40.37%       1.834ms     305.733us     187.000us         3.33%       1.608ms     268.000us             6  \n",
      "    aten::cudnn_convolution_transpose         5.07%     230.600us         5.07%     230.600us      76.867us       1.432ms        25.50%       1.432ms     477.333us             3  \n",
      "                         aten::conv3d         2.06%      93.700us        24.34%       1.106ms     276.525us      73.000us         1.30%       1.332ms     333.000us             4  \n",
      "                     aten::batch_norm         2.42%     110.000us        33.68%       1.531ms     255.100us      58.000us         1.03%       1.323ms     220.500us             6  \n",
      "         aten::_batch_norm_impl_index         4.30%     195.600us        31.26%       1.421ms     236.767us     138.000us         2.46%       1.265ms     210.833us             6  \n",
      "              aten::native_batch_norm        18.81%     854.900us        26.44%       1.201ms     200.217us     619.000us        11.02%       1.065ms     177.500us             6  \n",
      "              aten::cudnn_convolution        11.26%     511.800us        11.26%     511.800us     127.950us     884.000us        15.74%     884.000us     221.000us             4  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.544ms\n",
      "Self CUDA time total: 5.616ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\thop\\vision\\calc_func.py:53: UserWarning: This API is being deprecated\n",
      "  warnings.warn(\"This API is being deprecated\")\n"
     ]
    }
   ],
   "source": [
    "from src.models import UNet\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=7,\n",
    "    channels=[32, 64, 128, 256],\n",
    "    strides=(2,2,2),\n",
    "\n",
    ").to(device)\n",
    "print_model_summary(model, x)\n",
    "profile_model(model, x, './log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformerblock: <class 'src.models.deformer_lka_blocks.TransformerBlock_3D_single_deform_LKA'>\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using skip connection in decoder: True\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using skip connection in decoder: True\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using skip connection in decoder: True\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using LKA Attention with one deformable layer\n",
      "Using skip connection in decoder: True\n",
      "D_LKA_Net Summary:\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm3d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm3d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose3d'>.\n",
      "Model: D_LKA_Net\n",
      "FLOPs: 172,432,889,856.0, GFLOPs: 172.43\n",
      "Parameters: 25,235,530.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.models import  D_LKA_Net\n",
    "from src.models.deformer_lka_blocks import TransformerBlock\n",
    "\n",
    "model = D_LKA_Net(\n",
    "    in_channels=x[1],\n",
    "    out_channels=7,\n",
    "    \n",
    "    ).to(device)\n",
    "# Print summaries\n",
    "print(\"D_LKA_Net Summary:\")\n",
    "print_model_summary(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))  # GPU 이름 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinCSPUNET3Plus Summary:\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose3d'>.\n",
      "Model: MiTUnet\n",
      "FLOPs: 62,496,175,104.0, GFLOPs: 62.50\n",
      "Parameters: 12,031,811.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.models import MiTUnet\n",
    "# (7, 1, 3))\n",
    "x = (1, 1, 96, 96, 96)\n",
    "model = MiTUnet(\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size = 16,\n",
    "    heads=(1, 2, 2, 4, 8),\n",
    "    ff_expansion=(2, 8, 8, 4, 4),\n",
    "    reduction_ratio=(16, 8, 4, 2, 1),\n",
    "    num_layers=(1,1,2,2,2),\n",
    "    channels=1,\n",
    "    stage_kernel_stride_pad = ((3,1,1), (3, 2, 1), (3, 2, 1), (3, 2, 1), (3, 2, 1)),\n",
    "    \n",
    "    spatial_dims=3,\n",
    "    out_channels=7,\n",
    "    norm_name=\"instance\",\n",
    "    act_name = (\"leakyrelu \", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "    n=2,\n",
    ")\n",
    "\n",
    "# Print summaries\n",
    "print(\"SwinCSPUNET3Plus Summary:\")\n",
    "print_model_summary(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinCSPUNET3Plus Summary:\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose3d'>.\n",
      "Model: MiTCSPUnet\n",
      "FLOPs: 80,926,995,456.0, GFLOPs: 80.93\n",
      "Parameters: 15,698,243.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.models import MiTCSPUnet\n",
    "# (7, 1, 3))\n",
    "x = (1, 1, 96, 96, 96)\n",
    "model = MiTCSPUnet(\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size = 16,\n",
    "    heads=(1, 2, 2, 4, 8),\n",
    "    ff_expansion=(2, 8, 8, 4, 4),\n",
    "    reduction_ratio=(16, 8, 4, 2, 1),\n",
    "    num_layers=(1,1,2,2,2),\n",
    "    channels=1,\n",
    "    stage_kernel_stride_pad = ((3,1,1), (3, 2, 1), (3, 2, 1), (3, 2, 1), (3, 2, 1)),\n",
    "    \n",
    "    spatial_dims=3,\n",
    "    out_channels=7,\n",
    "    norm_name=\"instance\",\n",
    "    act_name = (\"leakyrelu \", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "    n=2,\n",
    ")\n",
    "\n",
    "# Print summaries\n",
    "print(\"SwinCSPUNET3Plus Summary:\")\n",
    "print_model_summary(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer Summary:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_model_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Print summaries\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSwinTransformer Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mprint_model_summary\u001b[49m(block, x)\n\u001b[1;32m     25\u001b[0m block \u001b[38;5;241m=\u001b[39m UnetResBlock(\n\u001b[1;32m     26\u001b[0m         spatial_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     27\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39min_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m      \n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Print summaries\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_model_summary' is not defined"
     ]
    }
   ],
   "source": [
    "# from src.models import CSPBlock, UnetResBlock\n",
    "\n",
    "\n",
    "# in_channels = 64\n",
    "# out_channels = 128\n",
    "# imgsz  = 96\n",
    "# block = CSPBlock(\n",
    "#         spatial_dims=3,\n",
    "#         in_channels=in_channels,  # 입력 채널 수정\n",
    "#         out_channels=out_channels,\n",
    "#         kernel_size=3,\n",
    "#         stride=2,\n",
    "#         norm_name=\"batch\",\n",
    "#         act_name=(\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "#         dropout=None,\n",
    "#         split_ratio=0.5,\n",
    "#         n=2\n",
    "#     )\n",
    "# x = (1, in_channels, imgsz, imgsz, imgsz)\n",
    "\n",
    "# # Print summaries\n",
    "# print(\"SwinTransformer Summary:\")\n",
    "# print_model_summary(block, x)\n",
    "\n",
    "# block = UnetResBlock(\n",
    "#         spatial_dims=3,\n",
    "#         in_channels=in_channels,\n",
    "#         out_channels=out_channels,\n",
    "#         kernel_size=3,\n",
    "#         stride=2,\n",
    "#         norm_name=\"batch\",\n",
    "#         act_name=(\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "#         dropout=None,\n",
    "     \n",
    "#     )\n",
    "\n",
    "# # Print summaries\n",
    "# print(\"UnetResBlock Summary:\")\n",
    "# print_model_summary(block, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinCSPUNETR_mix Summary:\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose3d'>.\n",
      "Model: SwinCSPUNETR_unet\n",
      "FLOPs: 290,186,906,136.0, GFLOPs: 290.19\n",
      "Parameters: 43,524,919.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.models.swincspunetr_unet import SwinCSPUNETR_unet\n",
    "x = (1, 1, 96, 96, 96)\n",
    "swin_unetr = SwinCSPUNETR_unet(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=1,\n",
    "    out_channels=7,\n",
    "    feature_size=48,\n",
    "    depths=(2, 2, 2, 2),\n",
    "    num_heads=(3, 6, 12, 24),\n",
    "    norm_name=\"instance\",\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    normalize=True,\n",
    "    use_checkpoint=True,\n",
    "    spatial_dims=3,\n",
    "    downsample=\"merging\",\n",
    "    use_v2=True,\n",
    "    n=2,\n",
    ")\n",
    "\n",
    "# Print summaries\n",
    "print(\"SwinCSPUNETR_unet Summary:\")\n",
    "print_model_summary(swin_unetr, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byungwanlim/miniconda3/envs/UM2/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: src.models.swincspunetr SwinCSPUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinCSPUNETR Summary:\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose3d'>.\n",
      "Model: SwinCSPUNETR\n",
      "FLOPs: 289,518,045,720.0, GFLOPs: 289.52\n",
      "Parameters: 62,104,375.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.models.swincspunetr import SwinCSPUNETR\n",
    "x = (1, 1, 96, 96, 96)\n",
    "swin_unetr = SwinCSPUNETR(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=1,\n",
    "    out_channels=7,\n",
    "    feature_size=48,\n",
    "    depths=(2, 2, 2, 2),\n",
    "    num_heads=(3, 6, 12, 24),\n",
    "    norm_name=\"instance\",\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    normalize=True,\n",
    "    use_checkpoint=True,\n",
    "    spatial_dims=3,\n",
    "    downsample=\"merging\",\n",
    "    use_v2=True,\n",
    "    n=2,\n",
    ")\n",
    "\n",
    "# Print summaries\n",
    "print(\"SwinCSPUNETR Summary:\")\n",
    "print_model_summary(swin_unetr, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwoo/anaconda3/envs/dust/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete SwinUNETR Summary:\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv3d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm3d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose3d'>.\n",
      "torch.Size([1, 48, 48, 48, 48])\n",
      "torch.Size([1, 96, 24, 24, 24])\n",
      "torch.Size([1, 192, 12, 12, 12])\n",
      "torch.Size([1, 384, 6, 6, 6])\n",
      "torch.Size([1, 768, 3, 3, 3])\n",
      "enc0: torch.Size([1, 48, 96, 96, 96])\n",
      "enc1 torch.Size([1, 48, 48, 48, 48])\n",
      "torch.Size([1, 96, 24, 24, 24])\n",
      "torch.Size([1, 192, 12, 12, 12])\n",
      "torch.Size([1, 768, 3, 3, 3])\n",
      "Model: SwinUNETR\n",
      "FLOPs: 355,370,190,360.0, GFLOPs: 355.37\n",
      "Parameters: 72,564,583.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example model\n",
    "import torch.nn as nn\n",
    "from monai.networks.nets import SwinUNETR, SwinTransformer\n",
    "\n",
    "# SwinTransformer 테스트\n",
    "swin_transformer = SwinTransformer(\n",
    "    in_chans=1,\n",
    "    embed_dim=48,\n",
    "    window_size=(7, 7, 7),\n",
    "    patch_size=(2, 2, 2),\n",
    "    depths=(2, 2, 2, 2),\n",
    "    num_heads=(3, 6, 12, 24),\n",
    "    mlp_ratio=4.0,\n",
    "    qkv_bias=True,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    drop_path_rate=0.0,\n",
    "    norm_layer=nn.LayerNorm,\n",
    "    use_checkpoint=True,\n",
    "    spatial_dims=3,\n",
    "    downsample=\"merging\",\n",
    "    use_v2=True\n",
    ")\n",
    "\n",
    "# 전체 SwinUNETR 모델\n",
    "swin_unetr = SwinUNETR(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=1,\n",
    "    out_channels=7,\n",
    "    feature_size=48,\n",
    "    depths=(2, 2, 2, 2),\n",
    "    num_heads=(3, 6, 12, 24),\n",
    "    norm_name=\"instance\",\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    normalize=True,\n",
    "    use_checkpoint=True,\n",
    "    spatial_dims=3,\n",
    "    downsample=\"merging\",\n",
    "    use_v2=True\n",
    ")\n",
    "\n",
    "# Input sizes\n",
    "swin_transformer_input = (1, 1, 96, 96, 96)\n",
    "swin_unetr_input = (1, 1, 96, 96, 96)\n",
    "\n",
    "# # Print summaries\n",
    "# print(\"SwinTransformer Summary:\")\n",
    "# print_model_summary(swin_transformer, swin_transformer_input)\n",
    "\n",
    "print(\"\\nComplete SwinUNETR Summary:\")\n",
    "print_model_summary(swin_unetr, swin_unetr_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([1, 48, 48, 48, 48])\n",
    "torch.Size([1, 96, 24, 24, 24])\n",
    "torch.Size([1, 192, 12, 12, 12])\n",
    "torch.Size([1, 384, 6, 6, 6])\n",
    "torch.Size([1, 768, 3, 3, 3])\n",
    "enc0: torch.Size([1, 48, 96, 96, 96])\n",
    "enc1 torch.Size([1, 48, 48, 48, 48])\n",
    "torch.Size([1, 96, 24, 24, 24])\n",
    "torch.Size([1, 192, 12, 12, 12])\n",
    "torch.Size([1, 768, 3, 3, 3])\n",
    "Model: SwinUNETR\n",
    "FLOPs: 329,543,087,640.0, GFLOPs: 329.54\n",
    "Parameters: 61,989,223.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
