{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.4.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: c:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.14.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.19.1+cu121\n",
      "tqdm version: 4.66.5\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR, SwinUNETR\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 비율: {0: 0.0, 1: 0.16393442622950818, 2: 0.01639344262295082, 3: 0.2459016393442623, 4: 0.16393442622950818, 5: 0.2459016393442623, 6: 0.16393442622950818}\n",
      "최종 합계: 1.0\n",
      "클래스 비율 리스트: [0.0, 0.16393442622950818, 0.01639344262295082, 0.2459016393442623, 0.16393442622950818, 0.2459016393442623, 0.16393442622950818]\n"
     ]
    }
   ],
   "source": [
    "class_info = {\n",
    "    0: {\"name\": \"background\", \"weight\": 0},  # weight 없음\n",
    "    1: {\"name\": \"apo-ferritin\", \"weight\": 1000},\n",
    "    2: {\"name\": \"beta-amylase\", \"weight\": 100}, # 4130\n",
    "    3: {\"name\": \"beta-galactosidase\", \"weight\": 1500}, #3080\n",
    "    4: {\"name\": \"ribosome\", \"weight\": 1000},\n",
    "    5: {\"name\": \"thyroglobulin\", \"weight\": 1500},\n",
    "    6: {\"name\": \"virus-like-particle\", \"weight\": 1000},\n",
    "}\n",
    "\n",
    "# 가중치에 비례한 비율 계산\n",
    "raw_ratios = {\n",
    "    k: (v[\"weight\"] if v[\"weight\"] is not None else 0.01)  # 가중치 비례, None일 경우 기본값a\n",
    "    for k, v in class_info.items()\n",
    "}\n",
    "total = sum(raw_ratios.values())\n",
    "ratios = {k: v / total for k, v in raw_ratios.items()}\n",
    "\n",
    "# 최종 합계가 1인지 확인\n",
    "final_total = sum(ratios.values())\n",
    "print(\"클래스 비율:\", ratios)\n",
    "print(\"최종 합계:\", final_total)\n",
    "\n",
    "# 비율을 리스트로 변환\n",
    "ratios_list = [ratios[k] for k in sorted(ratios.keys())]\n",
    "print(\"클래스 비율 리스트:\", ratios_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.dataset import create_dataloaders\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, CropForegroundd, GaussianSmoothd, ScaleIntensityd,\n",
    "    RandSpatialCropd, RandRotate90d, RandFlipd, RandGaussianNoised,\n",
    "    ToTensord, RandCropByLabelClassesd\n",
    ")\n",
    "from monai.transforms import CastToTyped\n",
    "import numpy as np\n",
    "\n",
    "train_img_dir = \"./datasets/train/images\"\n",
    "train_label_dir = \"./datasets/train/labels\"\n",
    "val_img_dir = \"./datasets/val/images\"\n",
    "val_label_dir = \"./datasets/val/labels\"\n",
    "# DATA CONFIG\n",
    "img_size =  96 # Match your patch size\n",
    "img_depth = img_size\n",
    "n_classes = 7\n",
    "batch_size = 1 # 13.8GB GPU memory required for 128x128 img size\n",
    "num_samples = batch_size # 한 이미지에서 뽑을 샘플 수\n",
    "loader_batch = 1\n",
    "num_repeat = 60\n",
    "accumulation_steps = 16\n",
    "\n",
    "# MODEL CONFIG\n",
    "feature_size = 48\n",
    "use_checkpoint = True\n",
    "use_v2 = True\n",
    "drop_rate= 0.25\n",
    "attn_drop_rate = 0.25\n",
    "\n",
    "# TRAINING CONFIG\n",
    "num_epochs = 4000\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "# LOSS\n",
    "warmup_epochs = 5\n",
    "schedule_epochs = 10\n",
    "warmup_ce = 8.0\n",
    "warmup_tv = 0.1\n",
    "warmup_hd = 0.1\n",
    "ce_end = 0.2\n",
    "tv_end = 0.4\n",
    "hd_end = 0.4\n",
    "include_background = False\n",
    "reduction = \"mean\"\n",
    "softmax = True\n",
    "tversky_alpha = 0.52  # Tversky loss의 alpha 값 = lamda\n",
    "tversky_beta = 1.0 - tversky_alpha\n",
    "tversky_smooth = 1e-5\n",
    "tv_boost = 1.2\n",
    "hd_boost = 1.2\n",
    "\n",
    "\n",
    "class_weights = None\n",
    "class_weights = torch.tensor([0.0001, 1, 0.001, 1.1, 1, 1.1, 1], dtype=torch.float32)  # 클래스별 가중치\n",
    "\n",
    "# INIT\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "best_val_fbeta_score = 0\n",
    "\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    CastToTyped(keys=[\"image\"], dtype=np.float16),\n",
    "    GaussianSmoothd(\n",
    "        keys=[\"image\"],      # 변환을 적용할 키\n",
    "        sigma=[1.0, 1.0, 1.0]  # 각 축(x, y, z)의 시그마 값\n",
    "        ),\n",
    "])\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 24/24 [00:39<00:00,  1.63s/it]\n",
      "Loading dataset: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = None, None\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_img_dir, \n",
    "    train_label_dir, \n",
    "    val_img_dir, \n",
    "    val_label_dir, \n",
    "    non_random_transforms = non_random_transforms, \n",
    "    random_transforms = random_transforms, \n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0,train_num_repeat=num_repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://monai.io/model-zoo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_31412\\3143408423.py:8: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from monai.losses import TverskyLoss, LogHausdorffDTLoss\n",
    "\n",
    "class AdaptiveCombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    요구사항:\n",
    "    1) Warm-up 단계 (0 ~ warmup_epochs):\n",
    "       - CE 위주로 학습(예: CE=1.0), Tversky/HD는 작게(예: 0.1)\n",
    "    2) Warm-up 종료 시점:\n",
    "       - 해당 시점 손실값 참고해 Kendall(\\log\\sigma) 초기값/가중치 보정\n",
    "    3) Kendall(2018) 식:\n",
    "       - (1/(2σ_i^2))*L_i + log(σ_i^2)\n",
    "       - Tversky/HD는 더 강조(상수 w_i를 크게) + CE 최소 한도 유지\n",
    "    4) Warm-up 이후 스케줄링:\n",
    "       - CE 점진 감소(1.0→0.3), Tversky/HD 점진 증가(0.1→1.0)\n",
    "       - 필요시 Tversky/HD를 추가로 boost\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # 1) Warm-up & 스케줄\n",
    "        warmup_epochs: int = 5,\n",
    "        schedule_epochs: int = 10,\n",
    "\n",
    "        # 2) Warm-up 시 고정 가중치\n",
    "        warmup_ce=1.0,\n",
    "        warmup_tv=0.1,\n",
    "        warmup_hd=0.1,\n",
    "\n",
    "        # 3) 스케줄에서의 최종 가중치\n",
    "        ce_end=0.3,\n",
    "        tv_end=1.0,\n",
    "        hd_end=1.0,\n",
    "\n",
    "        # 4) Tversky/HD를 추가로 더 키우고 싶다면 boost 사용\n",
    "        #    (예: 2.0이면 TV/HD가 2배 더 강조)\n",
    "        tv_boost=1.2,\n",
    "        hd_boost=1.2,\n",
    "\n",
    "        # MONAI Loss 설정\n",
    "        include_background=True,\n",
    "        reduction=\"mean\",\n",
    "        softmax=True,\n",
    "\n",
    "        # Tversky 파라미터\n",
    "        tversky_alpha=0.52,\n",
    "        tversky_beta=None,  # None이면 1 - alpha\n",
    "        tversky_smooth=1e-5,\n",
    "\n",
    "        # Kendall(2018) 식 사용 여부\n",
    "        use_uncertainty=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.schedule_epochs = schedule_epochs\n",
    "\n",
    "        # Warm-up 고정 가중치\n",
    "        self.warmup_ce = warmup_ce\n",
    "        self.warmup_tv = warmup_tv\n",
    "        self.warmup_hd = warmup_hd\n",
    "\n",
    "        # Warm-up 이후 스케줄 start/end\n",
    "        self.ce_start, self.ce_end = warmup_ce, ce_end\n",
    "        self.tv_start, self.tv_end = warmup_tv, tv_end\n",
    "        self.hd_start, self.hd_end = warmup_hd, hd_end\n",
    "\n",
    "        # Tversky/HD 추가 배율(강조)\n",
    "        self.tv_boost = tv_boost\n",
    "        self.hd_boost = hd_boost\n",
    "\n",
    "        self.use_uncertainty = use_uncertainty\n",
    "\n",
    "        if tversky_beta is None:\n",
    "            tversky_beta = 1.0 - tversky_alpha\n",
    "\n",
    "        # 배경 채널 무시 설정\n",
    "        if not include_background:\n",
    "            ignore_index = 0\n",
    "        else:\n",
    "            ignore_index = -100\n",
    "\n",
    "        # (1) 개별 손실 정의\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.tversky_loss = TverskyLoss(\n",
    "            alpha=tversky_alpha,\n",
    "            beta=tversky_beta,\n",
    "            smooth_nr=tversky_smooth,\n",
    "            smooth_dr=tversky_smooth,\n",
    "            softmax=softmax,\n",
    "            reduction=reduction,\n",
    "            include_background=include_background\n",
    "        )\n",
    "        self.haus_loss = LogHausdorffDTLoss(\n",
    "            softmax=softmax,\n",
    "            reduction=reduction,\n",
    "            include_background=include_background\n",
    "        )\n",
    "\n",
    "        # (2) Kendall을 위한 log_sigma 파라미터\n",
    "        if self.use_uncertainty:\n",
    "            self.log_sigma_ce   = nn.Parameter(torch.zeros(1))\n",
    "            self.log_sigma_tv   = nn.Parameter(torch.zeros(1))\n",
    "            self.log_sigma_haus = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # (3) 현재 epoch, warm-up 손실 기록\n",
    "        self.current_epoch = 0\n",
    "        self.last_warmup_ce   = 0.0\n",
    "        self.last_warmup_tv   = 0.0\n",
    "        self.last_warmup_haus = 0.0\n",
    "\n",
    "    def set_epoch(self, epoch: int):\n",
    "        \"\"\"학습 루프에서 매 epoch마다 호출하여 현재 epoch 갱신.\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def record_warmup_losses(self, ce_val, tv_val, hd_val):\n",
    "        \"\"\"\n",
    "        Warm-up 단계 손실값을 기록 -> 이후 log_sigma 초기값 조정 등 활용 가능\n",
    "        \"\"\"\n",
    "        self.last_warmup_ce   = ce_val\n",
    "        self.last_warmup_tv   = tv_val\n",
    "        self.last_warmup_haus = hd_val\n",
    "\n",
    "    def end_of_warmup_init(self):\n",
    "        \"\"\"\n",
    "        Warm-up → Kendall 전환 시점에서,\n",
    "        warm-up 손실값 등을 참고해 log_sigma 등 초기 설정\n",
    "        \"\"\"\n",
    "        if self.use_uncertainty:\n",
    "            with torch.no_grad():\n",
    "                # 예시: warm-up에서 CE가 안정적, TV/HD가 크면\n",
    "                # TV/HD 강조 위해 log_sigma를 음수로 조정\n",
    "                self.log_sigma_ce[0]   = 0.0   # CE\n",
    "                self.log_sigma_tv[0]   = -0.5  # TV\n",
    "                self.log_sigma_haus[0] = -0.5  # HD\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        preds: (B, C, D, H, W) - logit\n",
    "        targets: (B, D, H, W) - 정수 라벨\n",
    "        \"\"\"\n",
    "        # 1) 개별 손실 계산\n",
    "        loss_ce   = self.ce_loss(preds, targets)\n",
    "        loss_tv   = self.tversky_loss(preds, targets)\n",
    "        loss_haus = self.haus_loss(preds, targets)\n",
    "\n",
    "        # 2) Warm-up 단계\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            total_loss = (\n",
    "                self.warmup_ce * loss_ce\n",
    "                + self.warmup_tv * loss_tv\n",
    "                + self.warmup_hd * loss_haus\n",
    "            )\n",
    "        else:\n",
    "            # Warm-up 이후 → Kendall + 스케줄\n",
    "            # (a) ratio 계산\n",
    "            progress = self.current_epoch - self.warmup_epochs\n",
    "            ratio = float(progress) / float(self.schedule_epochs)\n",
    "            ratio = max(0.0, min(1.0, ratio))  # 0 ~ 1\n",
    "\n",
    "            # (b) CE, TV, HAUS 선형 보간 가중치\n",
    "            w_ce   = self.ce_start + (self.ce_end - self.ce_start)*ratio\n",
    "            w_tv   = self.tv_start + (self.tv_end - self.tv_start)*ratio\n",
    "            w_haus = self.hd_start + (self.hd_end - self.hd_start)*ratio\n",
    "\n",
    "            \n",
    "\n",
    "            if self.use_uncertainty:\n",
    "                # (c) Kendall 식\n",
    "                sigma_ce   = torch.exp(self.log_sigma_ce)\n",
    "                sigma_tv   = torch.exp(self.log_sigma_tv)\n",
    "                sigma_haus = torch.exp(self.log_sigma_haus)\n",
    "\n",
    "                ce_term   = (1.0/(2.0*sigma_ce**2))   * loss_ce   + torch.log(sigma_ce**2)\n",
    "                tv_term   = (1.0/(2.0*sigma_tv**2))   * loss_tv   + torch.log(sigma_tv**2)\n",
    "                haus_term = (1.0/(2.0*sigma_haus**2)) * loss_haus + torch.log(sigma_haus**2)\n",
    "\n",
    "                tv_term   = tv_term   * self.tv_boost\n",
    "                haus_term = haus_term * self.hd_boost\n",
    "                \n",
    "                # (d) 최종 합산 (TV/HD > CE가 되도록 w_tv, w_haus를 크게)\n",
    "                total_loss = w_ce*ce_term + w_tv*tv_term + w_haus*haus_term\n",
    "            else:\n",
    "                # Kendall 미사용 시 단순 가중합\n",
    "                total_loss = w_ce*loss_ce + w_tv*loss_tv + w_haus*loss_haus\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=(img_depth, img_size, img_size),\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    feature_size=feature_size,\n",
    "    use_checkpoint=True,\n",
    "    drop_rate = drop_rate,\n",
    "    attn_drop_rate = attn_drop_rate,\n",
    "    use_v2 = use_v2,\n",
    ").to(device)\n",
    "# Pretrained weights 불러오기\n",
    "# if use_checkpoint:\n",
    "#     pretrain_path = \"./swin_unetr_btcv_segmentation/models/model.pt\"\n",
    "#     weight = torch.load(pretrain_path, map_location=device)\n",
    "\n",
    "#     # 출력 레이어의 키를 제외한 나머지 가중치만 로드\n",
    "#     filtered_weights = {k: v for k, v in weight.items() if \"out.conv.conv\" not in k}\n",
    "\n",
    "#     # strict=False로 로드하여 불일치하는 부분 무시\n",
    "#     model.load_state_dict(filtered_weights, strict=False)\n",
    "#     print(\"Filtered weights loaded successfully. Output layer will be trained from scratch.\")\n",
    "\n",
    "\n",
    "# Loss function\n",
    "criterion = AdaptiveCombinedLoss(\n",
    "    warmup_epochs=warmup_epochs,\n",
    "    schedule_epochs=schedule_epochs,\n",
    "    warmup_ce=warmup_ce,\n",
    "    warmup_tv=warmup_tv,\n",
    "    warmup_hd=warmup_hd,\n",
    "    ce_end=ce_end,\n",
    "    tv_end=tv_end,\n",
    "    hd_end=hd_end,\n",
    "    include_background=include_background,\n",
    "    reduction=reduction,\n",
    "    tversky_alpha=tversky_alpha,\n",
    "    tversky_beta=tversky_beta,\n",
    "    tversky_smooth=tversky_smooth,\n",
    "    tv_boost=tv_boost,\n",
    "    hd_boost=hd_boost,\n",
    ")\n",
    "\n",
    "pretrain_str = \"yes\" if use_checkpoint else \"no\"\n",
    "weight_str = \"weighted\" if class_weights is not None else \"\"\n",
    "if tv_boost == hd_boost == 1.0:\n",
    "    boost_str = f\"b{tv_boost:.2f}\"\n",
    "else:\n",
    "    boost_str = f\"tvb{tv_boost:.2f}_hb{hd_boost:.2f}\"\n",
    "# 체크포인트 디렉토리 및 파일 설정\n",
    "checkpoint_base_dir = Path(\"./model_checkpoints\")\n",
    "folder_name = f\"SwinUNETRv2_CETVHF_{weight_str}_f{feature_size}s{img_size}lr{lr:.0e}_T-a{tversky_alpha:.2f}b{tversky_beta:.2f}Wc{warmup_ce}_Wt{warmup_tv}Wh{warmup_hd}_We{warmup_epochs}_Se{schedule_epochs}_{boost_str}_b{batch_size}_r{num_repeat}\"\n",
    "checkpoint_dir = checkpoint_base_dir / folder_name\n",
    "optimizer = optim.AdamW(list(model.parameters()) + list(criterion.parameters()), lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "# 체크포인트 디렉토리 생성\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    best_model_path = checkpoint_dir / 'best_model.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"기존 best model 발견: {best_model_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(best_model_path, map_location=device)\n",
    "            # 체크포인트 내부 키 검증\n",
    "            required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch', 'best_val_loss']\n",
    "            if all(k in checkpoint for k in required_keys):\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_val_loss = checkpoint['best_val_loss']\n",
    "                print(\"기존 학습된 가중치를 성공적으로 로드했습니다.\")\n",
    "                checkpoint= None\n",
    "            else:\n",
    "                raise ValueError(\"체크포인트 파일에 필요한 key가 없습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"체크포인트 파일을 로드하는 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 96, 96, 96]) torch.Size([1, 1, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwoow070840\u001b[0m (\u001b[33mwaooang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Workspace\\czll\\wandb\\run-20250101_192412-reth8plz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/reth8plz' target=\"_blank\">SwinUNETRv2_CETVHF_weighted_f48s96lr1e-03_T-a0.52b0.48Wc8.0_Wt0.1Wh0.1_We5_Se10_tvb1.20_hb1.20_b1_r60</a></strong> to <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/reth8plz' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR/runs/reth8plz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = folder_name\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project='czii_SwinUnetR',  # 프로젝트 이름 설정\n",
    "    name=run_name,         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': tversky_alpha,\n",
    "        \"cross_entropy_weight\": warmup_ce,\n",
    "        \"tversky_weight\": warmup_tv,\n",
    "        \"hausdorff_weight\": warmup_hd,\n",
    "        \"cross_entropy_weight_end\": ce_end,\n",
    "        \"tversky_weight_end\": tv_end,\n",
    "        \"hausdorff_weight_end\": hd_end,\n",
    "        \"tversky_weight_boost\": tv_boost,\n",
    "        \"hausdorff_weight_boost\": hd_boost,\n",
    "        \"include_background\": include_background,\n",
    "        \"wramup_epochs\": warmup_epochs,\n",
    "        \"schedule_epochs\": schedule_epochs,\n",
    "        \"include_background\": include_background,\n",
    "        \"reduction\": reduction,\n",
    "        'feature_size': feature_size,\n",
    "        'img_size': img_size,\n",
    "        'sampling_ratio': ratios_list,\n",
    "        'device': device.type,\n",
    "        \"checkpoint_dir\": str(folder_name),\n",
    "        \"class_weights\": class_weights.tolist() if class_weights is not None else None,\n",
    "        \"use_checkpoint\": use_checkpoint,\n",
    "        \"drop_rate\": drop_rate,\n",
    "        \"attn_drop_rate\": attn_drop_rate,\n",
    "        \"use_v2\": use_v2,\n",
    "        \"accumulation_steps\": accumulation_steps,\n",
    "        \"num_repeat\": num_repeat,\n",
    "        \n",
    "        # 필요한 하이퍼파라미터 추가\n",
    "    }\n",
    ")\n",
    "# 모델을 wandb에 연결\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric\n",
    "    \n",
    "def processing(batch_data, model, criterion, device):\n",
    "    images = batch_data['image'].to(device)  # Input 이미지 (B, 1, 96, 96, 96)\n",
    "    labels = batch_data['label'].to(device)  # 라벨 (B, 96, 96, 96)\n",
    "\n",
    "    labels = labels.squeeze(1)  # (B, 1, 96, 96, 96) → (B, 96, 96, 96)\n",
    "    labels = labels.long()  # 라벨을 정수형으로 변환\n",
    "\n",
    "    # 원핫 인코딩 (B, H, W, D) → (B, num_classes, H, W, D)\n",
    "    \n",
    "    labels_onehot = torch.nn.functional.one_hot(labels, num_classes=n_classes)\n",
    "    labels_onehot = labels_onehot.permute(0, 4, 1, 2, 3).float()  # (B, num_classes, H, W, D)\n",
    "\n",
    "    # 모델 예측\n",
    "    outputs = model(images)  # outputs: (B, num_classes, H, W, D)\n",
    "\n",
    "    # Loss 계산\n",
    "    # loss = criterion(outputs, labels_onehot)\n",
    "    loss = criterion(outputs, labels_onehot)\n",
    "    return loss, outputs, labels, outputs.argmax(dim=1)\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, accumulation_steps=4):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()  # 그래디언트 초기화\n",
    "    with tqdm(train_loader, desc='Training') as pbar:\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "            # 손실 계산\n",
    "            loss, _, _, _ = processing(batch_data, model, criterion, device)\n",
    "\n",
    "            # 그래디언트를 계산하고 누적\n",
    "            loss = loss / accumulation_steps  # 그래디언트 누적을 위한 스케일링\n",
    "            loss.backward()  # 그래디언트 계산 및 누적\n",
    "            \n",
    "            # 그래디언트 업데이트 (accumulation_steps마다 한 번)\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                optimizer.step()  # 파라미터 업데이트\n",
    "                optimizer.zero_grad()  # 누적된 그래디언트 초기화\n",
    "            \n",
    "            # 손실값 누적 (스케일링 복구)\n",
    "            epoch_loss += loss.item() * accumulation_steps  # 실제 손실값 반영\n",
    "            pbar.set_postfix(loss=loss.item() * accumulation_steps)  # 실제 손실값 출력\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    wandb.log({'train_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device, epoch, calculate_dice_interval):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    class_dice_scores = {i: [] for i in range(n_classes)}\n",
    "    class_f_beta_scores = {i: [] for i in range(n_classes)}\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc='Validation') as pbar:\n",
    "            for batch_data in pbar:\n",
    "                loss, _, labels, preds = processing(batch_data, model, criterion, device)\n",
    "                val_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "                # 각 클래스별 Dice 점수 계산\n",
    "                if epoch % calculate_dice_interval == 0:\n",
    "                    for i in range(n_classes):\n",
    "                        pred_i = (preds == i)\n",
    "                        label_i = (labels == i)\n",
    "                        dice_score = (2.0 * torch.sum(pred_i & label_i)) / (torch.sum(pred_i) + torch.sum(label_i) + 1e-8)\n",
    "                        class_dice_scores[i].append(dice_score.item())\n",
    "                        precision = (torch.sum(pred_i & label_i) + 1e-8) / (torch.sum(pred_i) + 1e-8)\n",
    "                        recall = (torch.sum(pred_i & label_i) + 1e-8) / (torch.sum(label_i) + 1e-8)\n",
    "                        f_beta_score = (1 + 4**2) * (precision * recall) / (4**2 * precision + recall + 1e-8)\n",
    "                        class_f_beta_scores[i].append(f_beta_score.item())\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    # 에포크별 평균 손실 로깅\n",
    "    wandb.log({'val_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    \n",
    "    # 각 클래스별 평균 Dice 점수 출력\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation Dice Score\")\n",
    "        all_classes_dice_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_dice = np.mean(class_dice_scores[i])\n",
    "            wandb.log({f'class_{i}_dice_score': mean_dice, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_dice:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_dice_scores.append(mean_dice)\n",
    "            \n",
    "        print()\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation F-beta Score\")\n",
    "        all_classes_fbeta_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_fbeta = np.mean(class_f_beta_scores[i])\n",
    "            wandb.log({f'class_{i}_f_beta_score': mean_fbeta, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_fbeta:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_fbeta_scores.append(mean_fbeta)\n",
    "        print()\n",
    "        overall_mean_dice = np.mean(all_classes_dice_scores)\n",
    "        overall_mean_fbeta = np.mean(all_classes_fbeta_scores)\n",
    "        wandb.log({'overall_mean_f_beta_score': overall_mean_fbeta, 'overall_mean_dice_score': overall_mean_dice, 'epoch': epoch + 1})\n",
    "        print(f\"\\nOverall Mean Dice Score: {overall_mean_dice:.4f}\\nOverall Mean F-beta Score: {overall_mean_fbeta:.4f}\\n\")\n",
    "\n",
    "    if overall_mean_fbeta is None:\n",
    "        overall_mean_fbeta = 0\n",
    "\n",
    "    return val_loss / len(val_loader), overall_mean_fbeta\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, \n",
    "    device, start_epoch, best_val_loss, best_val_fbeta_score, calculate_dice_interval=1,\n",
    "    accumulation_steps=4\n",
    "):\n",
    "    \"\"\"\n",
    "    모델을 학습하고 검증하는 함수\n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        train_loader: 학습 데이터 로더\n",
    "        val_loader: 검증 데이터 로더\n",
    "        criterion: 손실 함수\n",
    "        optimizer: 최적화 알고리즘\n",
    "        num_epochs: 총 학습 epoch 수\n",
    "        patience: early stopping 기준\n",
    "        device: GPU/CPU 장치\n",
    "        start_epoch: 시작 epoch\n",
    "        best_val_loss: 이전 최적 validation loss\n",
    "        best_val_fbeta_score: 이전 최적 validation f-beta score\n",
    "        calculate_dice_interval: Dice 점수 계산 주기\n",
    "    \"\"\"\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train One Epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            criterion=criterion, \n",
    "            optimizer=optimizer, \n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            accumulation_steps= accumulation_steps\n",
    "        )\n",
    "        \n",
    "        scheduler.step(train_loss)\n",
    "        # Validate One Epoch\n",
    "        val_loss, overall_mean_fbeta_score = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=epoch, \n",
    "            calculate_dice_interval=calculate_dice_interval\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F-beta: {overall_mean_fbeta_score:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss and overall_mean_fbeta_score > best_val_fbeta_score:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_fbeta_score = overall_mean_fbeta_score\n",
    "            epochs_no_improve = 0\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            print(f\"========================================================\")\n",
    "            print(f\"SUPER Best model saved. Loss:{best_val_loss:.4f}, Score:{best_val_fbeta_score:.4f}\")\n",
    "            print(f\"========================================================\")\n",
    "\n",
    "        # Early stopping 조건 체크\n",
    "        if val_loss >= best_val_loss and overall_mean_fbeta_score <= best_val_fbeta_score:\n",
    "            epochs_no_improve += 1\n",
    "        else:\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'last.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            break\n",
    "        # if epochs_no_improve%6 == 0:\n",
    "        #     # 손실이 개선되지 않았으므로 lambda 감소\n",
    "        #     new_lamda = max(criterion.lamda - 0.01, 0.1)  # 최소값은 0.1로 설정\n",
    "        #     criterion.set_lamda(new_lamda)\n",
    "        #     print(f\"Validation loss did not improve. Reducing lambda to {new_lamda:.4f}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1440 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ground truth has differing shape (torch.Size([1, 95, 96, 96])) from input (torch.Size([1, 6, 96, 96, 96]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_val_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_val_fbeta_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_fbeta_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalculate_dice_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m     \u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[32], line 132\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device, start_epoch, best_val_loss, best_val_fbeta_score, calculate_dice_interval, accumulation_steps)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Train One Epoch\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Validate One Epoch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 25\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device, epoch, accumulation_steps)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# 손실 계산\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m         loss, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m# 그래디언트를 계산하고 누적\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps  \u001b[38;5;66;03m# 그래디언트 누적을 위한 스케일링\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 14\u001b[0m, in \u001b[0;36mprocessing\u001b[1;34m(batch_data, model, criterion, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)  \u001b[38;5;66;03m# outputs: (B, num_classes, H, W, D)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 손실 계산\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, outputs, labels, outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 146\u001b[0m, in \u001b[0;36mAdaptiveCombinedLoss.forward\u001b[1;34m(self, preds, targets)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# 1) 개별 손실 계산\u001b[39;00m\n\u001b[0;32m    145\u001b[0m loss_ce   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mce_loss(preds, targets)\n\u001b[1;32m--> 146\u001b[0m loss_tv   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtversky_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m loss_haus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhaus_loss(preds, targets)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# 2) Warm-up 단계\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\monai\\losses\\tversky.py:135\u001b[0m, in \u001b[0;36mTverskyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground truth has differing shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) from input (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m p0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    138\u001b[0m p1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p0\n",
      "\u001b[1;31mAssertionError\u001b[0m: ground truth has differing shape (torch.Size([1, 95, 96, 96])) from input (torch.Size([1, 6, 96, 96, 96]))"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    start_epoch=start_epoch,\n",
    "    best_val_loss=best_val_loss,\n",
    "    best_val_fbeta_score=best_val_fbeta_score,\n",
    "    calculate_dice_interval=1,\n",
    "    accumulation_steps = accumulation_steps\n",
    "     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (879943805.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    if:\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fs6utwyo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cc3ae761af43baae5aca60b5d2e7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>class_0_dice_score</td><td>▁</td></tr><tr><td>class_0_f_beta_score</td><td>▁</td></tr><tr><td>class_1_dice_score</td><td>▁</td></tr><tr><td>class_1_f_beta_score</td><td>▁</td></tr><tr><td>class_2_dice_score</td><td>▁</td></tr><tr><td>class_2_f_beta_score</td><td>▁</td></tr><tr><td>class_3_dice_score</td><td>▁</td></tr><tr><td>class_3_f_beta_score</td><td>▁</td></tr><tr><td>class_4_dice_score</td><td>▁</td></tr><tr><td>class_4_f_beta_score</td><td>▁</td></tr><tr><td>class_5_dice_score</td><td>▁</td></tr><tr><td>class_5_f_beta_score</td><td>▁</td></tr><tr><td>class_6_dice_score</td><td>▁</td></tr><tr><td>class_6_f_beta_score</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>overall_mean_dice_score</td><td>▁</td></tr><tr><td>overall_mean_f_beta_score</td><td>▁</td></tr><tr><td>val_epoch_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>class_0_dice_score</td><td>0.65703</td></tr><tr><td>class_0_f_beta_score</td><td>0.50748</td></tr><tr><td>class_1_dice_score</td><td>0.53332</td></tr><tr><td>class_1_f_beta_score</td><td>0.64703</td></tr><tr><td>class_2_dice_score</td><td>0.00286</td></tr><tr><td>class_2_f_beta_score</td><td>0.02334</td></tr><tr><td>class_3_dice_score</td><td>0.23703</td></tr><tr><td>class_3_f_beta_score</td><td>0.23033</td></tr><tr><td>class_4_dice_score</td><td>0.65487</td></tr><tr><td>class_4_f_beta_score</td><td>0.62525</td></tr><tr><td>class_5_dice_score</td><td>0.47899</td></tr><tr><td>class_5_f_beta_score</td><td>0.51448</td></tr><tr><td>class_6_dice_score</td><td>0.42545</td></tr><tr><td>class_6_f_beta_score</td><td>0.47197</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>overall_mean_dice_score</td><td>0.42708</td></tr><tr><td>overall_mean_f_beta_score</td><td>0.43141</td></tr><tr><td>val_epoch_loss</td><td>0.7152</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SwinUNETR96_96_lr0.001_lambda0.52_batch2</strong> at: <a href='https://wandb.ai/waooang/czii_SwinUnetR_val/runs/fs6utwyo' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR_val/runs/fs6utwyo</a><br/> View project at: <a href='https://wandb.ai/waooang/czii_SwinUnetR_val' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR_val</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241219_200219-fs6utwyo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fs6utwyo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Workspace\\czll\\wandb\\run-20241219_200454-121l7bn3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/czii_SwinUnetR_val/runs/121l7bn3' target=\"_blank\">SwinUNETR96_96_lr0.001_lambda0.52_batch2</a></strong> to <a href='https://wandb.ai/waooang/czii_SwinUnetR_val' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/czii_SwinUnetR_val' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR_val</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/czii_SwinUnetR_val/runs/121l7bn3' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR_val/runs/121l7bn3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n",
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_21000\\1177025787.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrain_path, map_location=device)\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.38it/s, loss=0.865]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.6570, Class 1: 0.5333, Class 2: 0.0029, Class 3: 0.2370, \n",
      "Class 4: 0.6549, Class 5: 0.4790, Class 6: 0.4255, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.5075, Class 1: 0.6470, Class 2: 0.0233, Class 3: 0.2303, \n",
      "Class 4: 0.6252, Class 5: 0.5145, Class 6: 0.4720, \n",
      "Overall Mean Dice Score: 0.4659\n",
      "Overall Mean F-beta Score: 0.4978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, CropForegroundd, GaussianSmoothd, ScaleIntensityd,\n",
    "    RandSpatialCropd, RandRotate90d, RandFlipd, RandGaussianNoised,\n",
    "    ToTensord, RandCropByLabelClassesd\n",
    ")\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR, SwinUNETR\n",
    "from monai.losses import TverskyLoss\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from src.dataset.dataset import make_val_dataloader\n",
    "\n",
    "val_img_dir = \"./datasets/val/images\"\n",
    "val_label_dir = \"./datasets/val/labels\"\n",
    "img_depth = 96\n",
    "img_size = 96  # Match your patch size\n",
    "n_classes = 7\n",
    "batch_size = 2 # 13.8GB GPU memory required for 128x128 img size\n",
    "num_samples = batch_size # 한 이미지에서 뽑을 샘플 수\n",
    "loader_batch = 1\n",
    "lamda = 0.52\n",
    "\n",
    "wandb.init(\n",
    "    project='czii_SwinUnetR_val',  # 프로젝트 이름 설정\n",
    "    name='SwinUNETR96_96_lr0.001_lambda0.52_batch2',         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': lamda,\n",
    "        'img_size': img_size,\n",
    "        'device': 'cuda',\n",
    "        \"checkpoint_dir\": \"./model_checkpoints/SwinUNETR96_96_lr0.001_lambda0.52_batch2\",\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    GaussianSmoothd(\n",
    "        keys=[\"image\"],      # 변환을 적용할 키\n",
    "        sigma=[1.0, 1.0, 1.0]  # 각 축(x, y, z)의 시그마 값\n",
    "        ),\n",
    "])\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "])\n",
    "\n",
    "val_loader = make_val_dataloader(\n",
    "    val_img_dir, \n",
    "    val_label_dir, \n",
    "    non_random_transforms = non_random_transforms, \n",
    "    random_transforms = random_transforms, \n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0\n",
    ")\n",
    "criterion = TverskyLoss(\n",
    "    alpha= 1 - lamda,  # FP에 대한 가중치\n",
    "    beta=lamda,       # FN에 대한 가중치\n",
    "    include_background=False,  # 배경 클래스 제외\n",
    "    softmax=True\n",
    ")\n",
    "    \n",
    "    \n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "img_size = 96\n",
    "img_depth = img_size\n",
    "n_classes = 7 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrain_path = \"./model_checkpoints/SwinUNETR96_96_lr0.001_lambda0.52_batch2/best_model.pt\"\n",
    "model = SwinUNETR(\n",
    "    img_size=(img_depth, img_size, img_size),\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n",
    "# Pretrained weights 불러오기\n",
    "checkpoint = torch.load(pretrain_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "val_loss, overall_mean_fbeta_score = validate_one_epoch(\n",
    "    model=model, \n",
    "    val_loader=val_loader, \n",
    "    criterion=criterion, \n",
    "    device=device, \n",
    "    epoch=0, \n",
    "    calculate_dice_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.preprocessing import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import Compose, EnsureChannelFirstd, NormalizeIntensityd, Orientationd, GaussianSmoothd\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copick\n",
    "\n",
    "import torch\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file written to ./kaggle/working/copick.config\n",
      "file length: 7\n"
     ]
    }
   ],
   "source": [
    "config_blob = \"\"\"{\n",
    "    \"name\": \"czii_cryoet_mlchallenge_2024\",\n",
    "    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n",
    "    \"version\": \"1.0.0\",\n",
    "\n",
    "    \"pickable_objects\": [\n",
    "        {\n",
    "            \"name\": \"apo-ferritin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"4V1W\",\n",
    "            \"label\": 1,\n",
    "            \"color\": [  0, 117, 220, 128],\n",
    "            \"radius\": 60,\n",
    "            \"map_threshold\": 0.0418\n",
    "        },\n",
    "        {\n",
    "          \"name\" : \"beta-amylase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"8ZRZ\",\n",
    "            \"label\": 2,\n",
    "            \"color\": [255, 255, 255, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578  \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"beta-galactosidase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6X1Q\",\n",
    "            \"label\": 3,\n",
    "            \"color\": [ 76,   0,  92, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ribosome\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6EK0\",\n",
    "            \"label\": 4,\n",
    "            \"color\": [  0,  92,  49, 128],\n",
    "            \"radius\": 150,\n",
    "            \"map_threshold\": 0.0374\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thyroglobulin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6SCJ\",\n",
    "            \"label\": 5,\n",
    "            \"color\": [ 43, 206,  72, 128],\n",
    "            \"radius\": 130,\n",
    "            \"map_threshold\": 0.0278\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"virus-like-particle\",\n",
    "            \"is_particle\": true,\n",
    "            \"label\": 6,\n",
    "            \"color\": [255, 204, 153, 128],\n",
    "            \"radius\": 135,\n",
    "            \"map_threshold\": 0.201\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"membrane\",\n",
    "            \"is_particle\": false,\n",
    "            \"label\": 8,\n",
    "            \"color\": [100, 100, 100, 128]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"background\",\n",
    "            \"is_particle\": false,\n",
    "            \"label\": 9,\n",
    "            \"color\": [10, 150, 200, 128]\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"overlay_root\": \"./kaggle/working/overlay\",\n",
    "\n",
    "    \"overlay_fs_args\": {\n",
    "        \"auto_mkdir\": true\n",
    "    },\n",
    "\n",
    "    \"static_root\": \"./kaggle/input/czii-cryo-et-object-identification/test/static\"\n",
    "}\"\"\"\n",
    "\n",
    "copick_config_path = \"./kaggle/working/copick.config\"\n",
    "preprocessor = Preprocessor(config_blob,copick_config_path=copick_config_path)\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    GaussianSmoothd(\n",
    "        keys=[\"image\"],      # 변환을 적용할 키\n",
    "        sigma=[1.0, 1.0, 1.0]  # 각 축(x, y, z)의 시그마 값\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\monai\\utils\\deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_6248\\2937359115.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrain_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = 96\n",
    "img_depth = img_size\n",
    "n_classes = 7 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrain_path = \"./model_checkpoints/SwinUNETR96_96_lr0.001_lambda0.52_batch2/best_model.pt\"\n",
    "model = SwinUNETR(\n",
    "    img_size=(img_depth, img_size, img_size),\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n",
    "# Pretrained weights 불러오기\n",
    "checkpoint = torch.load(pretrain_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/4 [00:03<?, ?it/s, loss=0.764]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcalculate_dice_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 64\u001b[0m, in \u001b[0;36mvalidate_one_epoch\u001b[1;34m(model, val_loader, criterion, device, epoch, calculate_dice_interval)\u001b[0m\n\u001b[0;32m     61\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 각 클래스별 Dice 점수 계산\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcalculate_dice_interval\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_classes):\n\u001b[0;32m     66\u001b[0m         pred_i \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m i)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: integer modulo by zero"
     ]
    }
   ],
   "source": [
    "val_loss = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=1, \n",
    "            calculate_dice_interval=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 2/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 3/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to: submission.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import Compose, NormalizeIntensity\n",
    "import cc3d\n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    all_coords = np.vstack(all_coords)\n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "    return df\n",
    "\n",
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              2: \"beta-amylase\",\n",
    "              3: \"beta-galactosidase\", \n",
    "              4: \"ribosome\", \n",
    "              5: \"thyroglobulin\", \n",
    "              6: \"virus-like-particle\"}\n",
    "BLOB_THRESHOLD = 200\n",
    "CERTAINTY_THRESHOLD = 0.05\n",
    "\n",
    "classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    location_dfs = []  # DataFrame 리스트로 초기화\n",
    "    \n",
    "    for vol_idx, run in enumerate(preprocessor.root.runs):\n",
    "        print(f\"Processing volume {vol_idx + 1}/{len(preprocessor.root.runs)}\")\n",
    "        tomogram = preprocessor.processing(run=run, task=\"task\")\n",
    "        task_files = [{\"image\": tomogram}]\n",
    "        task_ds = CacheDataset(data=task_files, transform=non_random_transforms)\n",
    "        task_loader = DataLoader(task_ds, batch_size=1, num_workers=0)\n",
    "        \n",
    "        for task_data in task_loader:\n",
    "            images = task_data['image'].to(\"cuda\")\n",
    "            outputs = sliding_window_inference(\n",
    "                inputs=images,\n",
    "                roi_size=(96, 96, 96),  # ROI 크기\n",
    "                sw_batch_size=4,\n",
    "                predictor=model.forward,\n",
    "                overlap=0.1,\n",
    "                sw_device=\"cuda\",\n",
    "                device=\"cpu\",\n",
    "                buffer_steps=1,\n",
    "                buffer_dim=-1\n",
    "            )\n",
    "            outputs = outputs.argmax(dim=1).squeeze(0).cpu().numpy()  # 클래스 채널 예측\n",
    "            location = {}  # 좌표 저장용 딕셔너리\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(outputs == c)  # cc3d 라벨링\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:] * 10.012444  # 스케일 변환\n",
    "                zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]  # 크기 필터링\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])  # 좌표 스왑 (z, y, x -> x, y, z)\n",
    "\n",
    "                location[id_to_name[c]] = xyz  # ID 이름 매칭 저장\n",
    "\n",
    "            # 데이터프레임 변환\n",
    "            df = dict_to_df(location, run.name)\n",
    "            location_dfs.append(df)  # 리스트에 추가\n",
    "        \n",
    "        # if vol_idx == 2:\n",
    "        #     break\n",
    "    \n",
    "    # DataFrame 병합\n",
    "    final_df = pd.concat(location_dfs, ignore_index=True)\n",
    "    \n",
    "    # ID 추가 및 CSV 저장\n",
    "    final_df.insert(loc=0, column='id', value=np.arange(len(final_df)))\n",
    "    final_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved to: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
