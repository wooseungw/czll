{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "import os\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,  \n",
    "    AsDiscrete,  \n",
    "    RandFlipd, \n",
    "    RandRotate90d, \n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_DIR = \"./datasets/train/images\"\n",
    "TRAIN_LABEL_DIR = \"./datasets/train/labels\"\n",
    "VAL_IMG_DIR = \"./datasets/val/images\"\n",
    "VAL_LABEL_DIR = \"./datasets/val/labels\"\n",
    "\n",
    "train_list = os.listdir(TRAIN_IMG_DIR)\n",
    "val_list = os.listdir(VAL_IMG_DIR)\n",
    "train_files = []\n",
    "valid_files = []\n",
    "\n",
    "\n",
    "for name in train_list:\n",
    "    train_image = np.load(os.path.join(TRAIN_IMG_DIR, f\"{name}\"))    \n",
    "    train_label = np.load(os.path.join(TRAIN_LABEL_DIR, f\"{name.replace(\"image\", \"label\")}\"))\n",
    "\n",
    "    train_files.append({\"image\": train_image, \"label\": train_label})    \n",
    "\n",
    "for name in val_list:\n",
    "    valid_image = np.load(os.path.join(VAL_IMG_DIR, f\"{name}\"))\n",
    "    valid_label = np.load(os.path.join(VAL_LABEL_DIR, f\"{name.replace(\"image\", \"label\")}\"))\n",
    "\n",
    "    valid_files.append({\"image\": valid_image, \"label\": valid_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 24/24 [00:01<00:00, 12.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Non-random transforms to be cached\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim='no_channel'),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    # Orientationd(keys=[\"image\", \"label\"], axcodes=\"ASR\")\n",
    "])\n",
    "\n",
    "raw_train_ds = CacheDataset(data=train_files, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "\n",
    "my_num_samples = 1\n",
    "train_batch_size = 1\n",
    "\n",
    "xy_patch = 96\n",
    "z_patch = 3\n",
    "# Random transforms to be applied during training\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[z_patch, xy_patch, xy_patch],\n",
    "        num_classes=7,\n",
    "        num_samples=my_num_samples\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),    \n",
    "])\n",
    "\n",
    "\n",
    "train_ds = Dataset(data=raw_train_ds, transform=random_transforms)\n",
    "\n",
    "\n",
    "# DataLoader remains the same\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 배치 데이터 검증 ===\n",
      "이미지 shape: torch.Size([1, 1, 96, 96, 3])\n",
      "이미지 dtype: torch.float32\n",
      "이미지 값 범위: [-7.705, 3.054]\n",
      "\n",
      "\n",
      "라벨 shape: torch.Size([1, 1, 96, 96, 3])\n",
      "라벨 dtype: torch.uint8\n",
      "라벨 고유값: tensor([0, 6], dtype=torch.uint8)\n",
      "=== 배치 데이터 검증 ===\n",
      "이미지 shape: torch.Size([1, 1, 3, 96, 96])\n",
      "이미지 dtype: torch.float32\n",
      "이미지 값 범위: [-12.571, 1.839]\n",
      "\n",
      "\n",
      "라벨 shape: torch.Size([1, 1, 3, 96, 96])\n",
      "라벨 dtype: torch.uint8\n",
      "라벨 고유값: tensor([0, 1], dtype=torch.uint8)\n",
      "=== 배치 데이터 검증 ===\n",
      "이미지 shape: torch.Size([1, 1, 96, 96, 3])\n",
      "이미지 dtype: torch.float32\n",
      "이미지 값 범위: [-5.037, 5.835]\n",
      "\n",
      "\n",
      "라벨 shape: torch.Size([1, 1, 96, 96, 3])\n",
      "라벨 dtype: torch.uint8\n",
      "라벨 고유값: tensor([0, 3, 6], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 검증 함수\n",
    "def inspect_batch(loader):\n",
    "    # 첫 번째 배치 가져오기\n",
    "    batch = next(iter(loader))\n",
    "    \n",
    "    print(\"=== 배치 데이터 검증 ===\")\n",
    "    print(f\"이미지 shape: {batch['image'].shape}\")\n",
    "    print(f\"이미지 dtype: {batch['image'].dtype}\")\n",
    "    print(f\"이미지 값 범위: [{batch['image'].min():.3f}, {batch['image'].max():.3f}]\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"라벨 shape: {batch['label'].shape}\")\n",
    "    print(f\"라벨 dtype: {batch['label'].dtype}\")\n",
    "    print(f\"라벨 고유값: {torch.unique(batch['label'])}\")\n",
    "\n",
    "# 실행\n",
    "inspect_batch(train_loader)\n",
    "inspect_batch(train_loader)\n",
    "inspect_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 26496/26496 [00:14<00:00, 1778.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "valid_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\")\n",
    "])\n",
    "\n",
    "def load_validation_patches(patch_dir):\n",
    "    \"\"\"\n",
    "    저장된 validation 패치들을 로드하여 데이터셋용 리스트 생성\n",
    "    Args:\n",
    "        patch_dir: 패치가 저장된 디렉토리 (images, labels 서브디렉토리와 coordinates.json 포함)\n",
    "    \"\"\"\n",
    "    patch_dir = Path(patch_dir)\n",
    "    val_patched_data = []\n",
    "    \n",
    "    # coordinates.json 로드\n",
    "    with open(patch_dir / \"coordinates.json\", 'r') as f:\n",
    "        coordinates = json.load(f)\n",
    "    \n",
    "    # 각 패치에 대해\n",
    "    for coord in coordinates:\n",
    "        image = np.load(patch_dir / \"images\" / coord[\"patch_file\"])\n",
    "        label = np.load(patch_dir / \"labels\" / coord[\"patch_file\"])\n",
    "        \n",
    "        val_patched_data.append({\n",
    "            \"image\": image,      # shape: (11, 96, 96)\n",
    "            \"label\": label,      # shape: (96, 96)\n",
    "            \"coords\": coord      # 원본 위치 정보 (옵션)\n",
    "        })\n",
    "    \n",
    "    return val_patched_data\n",
    "\n",
    "# 패치 데이터 로드\n",
    "val_patched_data = load_validation_patches(\"./datasets/val_patches\")\n",
    "\n",
    "# Dataset과 DataLoader 설정\n",
    "valid_ds = CacheDataset(data=val_patched_data, transform=valid_transforms, cache_rate=1.0)\n",
    "\n",
    "valid_batch_size = train_batch_size\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=valid_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 검증 함수\n",
    "# def inspect_batch(loader):\n",
    "#     # 첫 번째 배치 가져오기\n",
    "#     batch = next(iter(loader))\n",
    "    \n",
    "#     print(\"=== 배치 데이터 검증 ===\")\n",
    "#     print(f\"이미지 shape: {batch['image'].shape}\")\n",
    "#     print(f\"이미지 dtype: {batch['image'].dtype}\")\n",
    "#     print(f\"이미지 값 범위: [{batch['image'].min():.3f}, {batch['image'].max():.3f}]\")\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"라벨 shape: {batch['label'].shape}\")\n",
    "#     print(f\"라벨 dtype: {batch['label'].dtype}\")\n",
    "#     print(f\"라벨 고유값: {torch.unique(batch['label'])}\")\n",
    "\n",
    "# # 실행\n",
    "# inspect_batch(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pretrained: grid-size from 14 to 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: 100%|██████████| 24/24 [00:20<00:00,  1.19it/s, loss=0.886]\n",
      "Validation Epoch 1:   5%|▌         | 1456/26496 [05:43<1:44:17,  4.00it/s, val_loss=0.888]"
     ]
    }
   ],
   "source": [
    "from CSANet.CSANet.networks.vit_seg_modeling import VisionTransformer as ViT_seg\n",
    "from CSANet.CSANet.networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.losses import TverskyLoss\n",
    "\n",
    "# Model Configuration\n",
    "vit_name = 'R50-ViT-B_16'\n",
    "config_vit = CONFIGS_ViT_seg[vit_name]\n",
    "config_vit.n_classes = 7  # Your number of classes\n",
    "config_vit.n_skip = 3\n",
    "img_size = 96  # Match your patch size\n",
    "vit_patches_size = 16\n",
    "\n",
    "# Initialize model\n",
    "config_vit.patches.grid = (int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "model = ViT_seg(config_vit, img_size=img_size, num_classes=config_vit.n_classes)\n",
    "model.cuda()\n",
    "\n",
    "# Load pretrained weights\n",
    "model.load_from(weights=np.load(config_vit.real_pretrained_path, allow_pickle=True))\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 40\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "# TverskyLoss 설정\n",
    "criterion = TverskyLoss(\n",
    "    alpha=0.3,  # FP에 대한 가중치\n",
    "    beta=0.7,   # FN에 대한 가중치\n",
    "    include_background=True,\n",
    "    softmax=True\n",
    ")\n",
    "\n",
    "\n",
    "# Training loop 수정\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n",
    "        for batch_data in pbar:\n",
    "            images = batch_data['image'].cuda()\n",
    "            labels = batch_data['label'].cuda()\n",
    "            \n",
    "            # 차원 분리 및 reshape\n",
    "            # print(images.shape)\n",
    "            if images.shape[2] == 96:\n",
    "                images = images.permute(0, 1, 4, 2, 3)\n",
    "                labels = labels.permute(0, 1, 4, 2, 3)\n",
    "                # print(images.shape)\n",
    "            # prev_image, image, next_image = torch.split(images, 1, dim=2)\n",
    "            center = images.shape[2] // 2\n",
    "            prev_image = images[:, :, 0:center, :, :]\n",
    "            image = images[:, :, center:center+1, :, :]\n",
    "            next_image = images[:, :, center+1:, :, :]\n",
    "            \n",
    "            # 필요한 차원 형태로 변환\n",
    "            prev_image = prev_image.squeeze(2)  # [B, C, H, W]\n",
    "            image = image.squeeze(2)\n",
    "            next_image = next_image.squeeze(2)\n",
    "            \n",
    "            # 라벨 처리\n",
    "            labels = labels[:, :, center:center+1, :, :]\n",
    "            labels = labels.squeeze(2)  # [B, 1, H, W]\n",
    "            labels = labels.squeeze(1)  # [B, H, W]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(prev_image, image, next_image)\n",
    "\n",
    "            # 라벨을 long으로 변환하고 one-hot encoding\n",
    "            labels = labels.long()\n",
    "            labels_onehot = torch.nn.functional.one_hot(labels, num_classes=config_vit.n_classes)  # [B, H, W, C]\n",
    "            # 차원 순서 변경: [B, H, W, C] -> [B, C, H, W]\n",
    "            labels_onehot = labels_onehot.permute(0, 3, 1, 2).float()\n",
    "            \n",
    "            loss = criterion(outputs, labels_onehot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    # Training loop는 그대로 유지\n",
    "\n",
    "    # Training loop 내부의 validation과 saving 코드 수정\n",
    "    # Validation\n",
    "    if (epoch + 1) % 1 == 0:  # Validate every 5 epochs\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            val_loop = tqdm(valid_loader, desc=f'Validation Epoch {epoch+1}', leave=False)\n",
    "            for val_data in val_loop:\n",
    "                val_images = val_data['image'].cuda()\n",
    "                val_labels = val_data['label'].cuda()\n",
    "                \n",
    "                # Training과 동일한 방식으로 데이터 처리\n",
    "                val_prev, val_curr, val_next = torch.split(val_images, 1, dim=2)\n",
    "                val_prev = val_prev.squeeze(2)\n",
    "                val_curr = val_curr.squeeze(2)\n",
    "                val_next = val_next.squeeze(2)\n",
    "                \n",
    "                val_labels = val_labels.squeeze(1)\n",
    "                val_labels = val_labels.long()\n",
    "                \n",
    "                val_outputs = model(val_prev, val_curr, val_next)\n",
    "                \n",
    "                val_labels_onehot = torch.nn.functional.one_hot(val_labels, num_classes=config_vit.n_classes)\n",
    "                val_labels_onehot = val_labels_onehot.permute(0, 3, 1, 2).float()\n",
    "                \n",
    "                current_loss = criterion(val_outputs, val_labels_onehot).item()\n",
    "                val_loss += current_loss\n",
    "                val_loop.set_postfix({'val_loss': current_loss})\n",
    "        \n",
    "        print(f'Validation Loss: {val_loss/len(valid_loader):.4f}')\n",
    "        \n",
    "    # Save checkpoint with progress bar\n",
    "    if (epoch + 1) % 10 == 0:  # Save every 10 epochs\n",
    "        with tqdm(total=1, desc=f'Saving checkpoint for epoch {epoch+1}', leave=False) as pbar:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "            }, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            pbar.update(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
