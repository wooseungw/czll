{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/ribosome.json to ./kaggle/working/overlay/ExperimentRuns/TS_5_4/Picks/curation_0_ribosome.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/virus-like-particle.json to ./kaggle/working/overlay/ExperimentRuns/TS_5_4/Picks/curation_0_virus-like-particle.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/beta-galactosidase.json to ./kaggle/working/overlay/ExperimentRuns/TS_5_4/Picks/curation_0_beta-galactosidase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/beta-amylase.json to ./kaggle/working/overlay/ExperimentRuns/TS_5_4/Picks/curation_0_beta-amylase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/apo-ferritin.json to ./kaggle/working/overlay/ExperimentRuns/TS_5_4/Picks/curation_0_apo-ferritin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_5_4/Picks/thyroglobulin.json to ./kaggle/working/overlay/ExperimentRuns/TS_5_4/Picks/curation_0_thyroglobulin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_99_9/Picks/ribosome.json to ./kaggle/working/overlay/ExperimentRuns/TS_99_9/Picks/curation_0_ribosome.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_99_9/Picks/virus-like-particle.json to ./kaggle/working/overlay/ExperimentRuns/TS_99_9/Picks/curation_0_virus-like-particle.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_99_9/Picks/beta-galactosidase.json to ./kaggle/working/overlay/ExperimentRuns/TS_99_9/Picks/curation_0_beta-galactosidase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_99_9/Picks/beta-amylase.json to ./kaggle/working/overlay/ExperimentRuns/TS_99_9/Picks/curation_0_beta-amylase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_99_9/Picks/apo-ferritin.json to ./kaggle/working/overlay/ExperimentRuns/TS_99_9/Picks/curation_0_apo-ferritin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_99_9/Picks/thyroglobulin.json to ./kaggle/working/overlay/ExperimentRuns/TS_99_9/Picks/curation_0_thyroglobulin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_4/Picks/ribosome.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_4/Picks/curation_0_ribosome.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_4/Picks/virus-like-particle.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_4/Picks/curation_0_virus-like-particle.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_4/Picks/beta-galactosidase.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_4/Picks/curation_0_beta-galactosidase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_4/Picks/beta-amylase.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_4/Picks/curation_0_beta-amylase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_4/Picks/apo-ferritin.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_4/Picks/curation_0_apo-ferritin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_4/Picks/thyroglobulin.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_4/Picks/curation_0_thyroglobulin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_73_6/Picks/ribosome.json to ./kaggle/working/overlay/ExperimentRuns/TS_73_6/Picks/curation_0_ribosome.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_73_6/Picks/virus-like-particle.json to ./kaggle/working/overlay/ExperimentRuns/TS_73_6/Picks/curation_0_virus-like-particle.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_73_6/Picks/beta-galactosidase.json to ./kaggle/working/overlay/ExperimentRuns/TS_73_6/Picks/curation_0_beta-galactosidase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_73_6/Picks/beta-amylase.json to ./kaggle/working/overlay/ExperimentRuns/TS_73_6/Picks/curation_0_beta-amylase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_73_6/Picks/apo-ferritin.json to ./kaggle/working/overlay/ExperimentRuns/TS_73_6/Picks/curation_0_apo-ferritin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_73_6/Picks/thyroglobulin.json to ./kaggle/working/overlay/ExperimentRuns/TS_73_6/Picks/curation_0_thyroglobulin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_86_3/Picks/ribosome.json to ./kaggle/working/overlay/ExperimentRuns/TS_86_3/Picks/curation_0_ribosome.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_86_3/Picks/virus-like-particle.json to ./kaggle/working/overlay/ExperimentRuns/TS_86_3/Picks/curation_0_virus-like-particle.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_86_3/Picks/beta-galactosidase.json to ./kaggle/working/overlay/ExperimentRuns/TS_86_3/Picks/curation_0_beta-galactosidase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_86_3/Picks/beta-amylase.json to ./kaggle/working/overlay/ExperimentRuns/TS_86_3/Picks/curation_0_beta-amylase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_86_3/Picks/apo-ferritin.json to ./kaggle/working/overlay/ExperimentRuns/TS_86_3/Picks/curation_0_apo-ferritin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_86_3/Picks/thyroglobulin.json to ./kaggle/working/overlay/ExperimentRuns/TS_86_3/Picks/curation_0_thyroglobulin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_6/Picks/ribosome.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_6/Picks/curation_0_ribosome.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_6/Picks/virus-like-particle.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_6/Picks/curation_0_virus-like-particle.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_6/Picks/beta-galactosidase.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_6/Picks/curation_0_beta-galactosidase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_6/Picks/beta-amylase.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_6/Picks/curation_0_beta-amylase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_6/Picks/apo-ferritin.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_6/Picks/curation_0_apo-ferritin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_6_6/Picks/thyroglobulin.json to ./kaggle/working/overlay/ExperimentRuns/TS_6_6/Picks/curation_0_thyroglobulin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_69_2/Picks/ribosome.json to ./kaggle/working/overlay/ExperimentRuns/TS_69_2/Picks/curation_0_ribosome.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_69_2/Picks/virus-like-particle.json to ./kaggle/working/overlay/ExperimentRuns/TS_69_2/Picks/curation_0_virus-like-particle.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_69_2/Picks/beta-galactosidase.json to ./kaggle/working/overlay/ExperimentRuns/TS_69_2/Picks/curation_0_beta-galactosidase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_69_2/Picks/beta-amylase.json to ./kaggle/working/overlay/ExperimentRuns/TS_69_2/Picks/curation_0_beta-amylase.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_69_2/Picks/apo-ferritin.json to ./kaggle/working/overlay/ExperimentRuns/TS_69_2/Picks/curation_0_apo-ferritin.json\n",
      "Copied ./kaggle/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns/TS_69_2/Picks/thyroglobulin.json to ./kaggle/working/overlay/ExperimentRuns/TS_69_2/Picks/curation_0_thyroglobulin.json\n"
     ]
    }
   ],
   "source": [
    "# Make a copick project\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "config_blob = \"\"\"{\n",
    "    \"name\": \"czii_cryoet_mlchallenge_2024\",\n",
    "    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n",
    "    \"version\": \"1.0.0\",\n",
    "\n",
    "    \"pickable_objects\": [\n",
    "        {\n",
    "            \"name\": \"apo-ferritin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"4V1W\",\n",
    "            \"label\": 1,\n",
    "            \"color\": [  0, 117, 220, 128],\n",
    "            \"radius\": 60,\n",
    "            \"map_threshold\": 0.0418\n",
    "        },\n",
    "        {\n",
    "          \"name\" : \"beta-amylase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"8ZRZ\",\n",
    "            \"label\": 2,\n",
    "            \"color\": [255, 255, 255, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578  \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"beta-galactosidase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6X1Q\",\n",
    "            \"label\": 3,\n",
    "            \"color\": [ 76,   0,  92, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ribosome\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6EK0\",\n",
    "            \"label\": 4,\n",
    "            \"color\": [  0,  92,  49, 128],\n",
    "            \"radius\": 150,\n",
    "            \"map_threshold\": 0.0374\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thyroglobulin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6SCJ\",\n",
    "            \"label\": 5,\n",
    "            \"color\": [ 43, 206,  72, 128],\n",
    "            \"radius\": 130,\n",
    "            \"map_threshold\": 0.0278\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"virus-like-particle\",\n",
    "            \"is_particle\": true,\n",
    "            \"label\": 6,\n",
    "            \"color\": [255, 204, 153, 128],\n",
    "            \"radius\": 135,\n",
    "            \"map_threshold\": 0.201\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"membrane\",\n",
    "            \"is_particle\": false,\n",
    "            \"label\": 8,\n",
    "            \"color\": [100, 100, 100, 128]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"background\",\n",
    "            \"is_particle\": false,\n",
    "            \"label\": 9,\n",
    "            \"color\": [10, 150, 200, 128]\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"overlay_root\": \"./kaggle/working/overlay\",\n",
    "\n",
    "    \"overlay_fs_args\": {\n",
    "        \"auto_mkdir\": true\n",
    "    },\n",
    "\n",
    "    \"static_root\": \"./kaggle/input/czii-cryo-et-object-identification/train/static\"\n",
    "}\"\"\"\n",
    "\n",
    "copick_config_path = \"./kaggle/working/copick.config\"\n",
    "output_overlay = \"./kaggle/working/overlay\"\n",
    "\n",
    "\n",
    "with open(copick_config_path, \"w\") as f:\n",
    "    f.write(config_blob)\n",
    "    \n",
    "# Update the overlay\n",
    "# Define source and destination directories\n",
    "source_dir = './kaggle/input/czii-cryo-et-object-identification/train/overlay'\n",
    "destination_dir = './kaggle/working/overlay'\n",
    "\n",
    "# Walk through the source directory\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    # Create corresponding subdirectories in the destination\n",
    "    relative_path = os.path.relpath(root, source_dir)\n",
    "    target_dir = os.path.join(destination_dir, relative_path)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy and rename each file\n",
    "    for file in files:\n",
    "        if file.startswith(\"curation_0_\"):\n",
    "            new_filename = file\n",
    "        else:\n",
    "            new_filename = f\"curation_0_{file}\"\n",
    "            \n",
    "        \n",
    "        # Define full paths for the source and destination files\n",
    "        source_file = os.path.join(root, file)\n",
    "        destination_file = os.path.join(target_dir, new_filename)\n",
    "        \n",
    "        # Copy the file with the new name\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        print(f\"Copied {source_file} to {destination_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/vt4hj2ln3hs6c97_qllsy7lh0000gn/T/ipykernel_15094/1521813977.py:5: DeprecationWarning: config_type not found in config file, defaulting to filesystem\n",
      "  root = copick.from_file(copick_config_path)\n"
     ]
    }
   ],
   "source": [
    "import copick\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "root = copick.from_file(copick_config_path)\n",
    "\n",
    "copick_user_name = \"copickUtils\"\n",
    "copick_segmentation_name = \"paintedPicks\"\n",
    "voxel_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing \"ctfdeconvolved\" data...\n",
      "Processed volume 0 for \"ctfdeconvolved\" dataset.\n",
      "Processed volume 1 for \"ctfdeconvolved\" dataset.\n",
      "Processed volume 2 for \"ctfdeconvolved\" dataset.\n",
      "Processed volume 3 for \"ctfdeconvolved\" dataset.\n",
      "Processed volume 4 for \"ctfdeconvolved\" dataset.\n",
      "Processed volume 5 for \"ctfdeconvolved\" dataset.\n",
      "Processed volume 6 for \"ctfdeconvolved\" dataset.\n",
      "Processing \"denoised\" data...\n",
      "Processed volume 0 for \"denoised\" dataset.\n",
      "Processed volume 1 for \"denoised\" dataset.\n",
      "Processed volume 2 for \"denoised\" dataset.\n",
      "Processed volume 3 for \"denoised\" dataset.\n",
      "Processed volume 4 for \"denoised\" dataset.\n",
      "Processed volume 5 for \"denoised\" dataset.\n",
      "Processed volume 6 for \"denoised\" dataset.\n",
      "Processing \"isonetcorrected\" data...\n",
      "Processed volume 0 for \"isonetcorrected\" dataset.\n",
      "Processed volume 1 for \"isonetcorrected\" dataset.\n",
      "Processed volume 2 for \"isonetcorrected\" dataset.\n",
      "Processed volume 3 for \"isonetcorrected\" dataset.\n",
      "Processed volume 4 for \"isonetcorrected\" dataset.\n",
      "Processed volume 5 for \"isonetcorrected\" dataset.\n",
      "Processed volume 6 for \"isonetcorrected\" dataset.\n",
      "Processing \"wbp\" data...\n",
      "Processed volume 0 for \"wbp\" dataset.\n",
      "Processed volume 1 for \"wbp\" dataset.\n",
      "Processed volume 2 for \"wbp\" dataset.\n",
      "Processed volume 3 for \"wbp\" dataset.\n",
      "Processed volume 4 for \"wbp\" dataset.\n",
      "Processed volume 5 for \"wbp\" dataset.\n",
      "Processed volume 6 for \"wbp\" dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define tomogram types\n",
    "tomo_tpye_list = [\"ctfdeconvolved\", \"denoised\", \"isonetcorrected\", \"wbp\"]\n",
    "\n",
    "# Configuration for directories\n",
    "train_label_dir = Path('./datasets/labels/train')\n",
    "train_image_dir = Path('./datasets/images/train')\n",
    "val_label_dir = Path('./datasets/labels/val')\n",
    "val_image_dir = Path('./datasets/images/val')\n",
    "\n",
    "for dir_path in [train_label_dir, train_image_dir, val_label_dir, val_image_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Iterate over all tomogram types\n",
    "for tomo_type in tomo_tpye_list:\n",
    "    print(f\"Processing \\\"{tomo_type}\\\" data...\")\n",
    "    for vol_idx, run in enumerate(root.runs):\n",
    "        # Load image and label data\n",
    "        tomogram = run.get_voxel_spacing(voxel_size).get_tomogram(tomo_type).numpy()\n",
    "        segmentation = run.get_segmentations(\n",
    "            name=copick_segmentation_name,\n",
    "            user_id=copick_user_name,\n",
    "            voxel_size=voxel_size,\n",
    "            is_multilabel=True\n",
    "        )[0].numpy()\n",
    "\n",
    "        data_dict = {\"image\": tomogram, \"label\": segmentation}\n",
    "\n",
    "        # Determine dataset type (train/val)\n",
    "        is_test = (vol_idx == len(root.runs) - 1)\n",
    "        label_dir = val_label_dir if is_test else train_label_dir\n",
    "        image_dir = val_image_dir if is_test else train_image_dir\n",
    "\n",
    "        # Save slices for current tomogram type\n",
    "        for slice_idx in range(data_dict[\"image\"].shape[0]):  # Iterate over slices\n",
    "            base_filename = f\"{tomo_type}_vol_{vol_idx:01d}_slice_{slice_idx:03d}\"\n",
    "\n",
    "            # Save label as PNG\n",
    "            label_slice = data_dict[\"label\"][slice_idx]\n",
    "            plt.imsave(label_dir / f\"{base_filename}.png\", label_slice, cmap='gray')\n",
    "\n",
    "            # Normalize and save image\n",
    "            slice_img = data_dict[\"image\"][slice_idx]\n",
    "            norm_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
    "            plt.imsave(image_dir / f\"{base_filename}.png\", norm_img, cmap='gray')\n",
    "\n",
    "        print(f\"Processed volume {vol_idx} for \\\"{tomo_type}\\\" dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(image_dir)\n\u001b[1;32m      2\u001b[0m label_dir \u001b[38;5;241m=\u001b[39m Path(label_dir) \u001b[38;5;28;01mif\u001b[39;00m label_dir \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Collect sorted file paths for images\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "image_dir = Path(image_dir)\n",
    "label_dir = Path(label_dir) if label_dir else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collect sorted file paths for images\n",
    "image_files = sorted(list(image_dir.glob(\"*.png\")))\n",
    "image_files[:194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 11, 224, 224]), Label shape: torch.Size([2, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "class MultiChannelCryoETDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, label_dir=None, num_channels=11, slice_size=(224, 224), stride=112, transform=None, default_label=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): 이미지 디렉토리 경로.\n",
    "            label_dir (str): 라벨 디렉토리 경로 (Optional).\n",
    "            num_channels (int): 사용할 채널(슬라이스) 수.\n",
    "            slice_size (tuple): 슬라이스 크기 (W, H).\n",
    "            stride (int): 슬라이싱 시 stride.\n",
    "            transform (callable): 데이터 변환 함수.\n",
    "            default_label (int): 라벨이 없을 때의 기본값.\n",
    "        \"\"\"\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.label_dir = Path(label_dir) if label_dir else None\n",
    "        self.num_channels = num_channels\n",
    "        self.slice_width, self.slice_height = slice_size\n",
    "        self.stride = stride\n",
    "        self.transform = transform\n",
    "        self.default_label = default_label\n",
    "\n",
    "        # 볼륨별 슬라이스 그룹화\n",
    "        self.volume_slices = self._group_slices_by_volume()\n",
    "        self.slices = self._generate_slices()\n",
    "\n",
    "    def _group_slices_by_volume(self):\n",
    "        \"\"\"\n",
    "        볼륨별로 슬라이스를 정리하여 그룹화합니다.\n",
    "        \"\"\"\n",
    "        volume_groups = defaultdict(list)\n",
    "        for file_path in sorted(self.image_dir.glob(\"*.png\")):\n",
    "            parts = file_path.stem.split(\"_\")\n",
    "            volume_id = parts[2]  # 예: \"vol_0\"\n",
    "            slice_id = int(parts[4])  # 예: \"slice_001\"\n",
    "            volume_groups[volume_id].append((slice_id, file_path))\n",
    "        \n",
    "        for key in volume_groups:\n",
    "            volume_groups[key] = sorted(volume_groups[key], key=lambda x: x[0])\n",
    "        \n",
    "        return volume_groups\n",
    "\n",
    "    def _generate_slices(self):\n",
    "        \"\"\"\n",
    "        슬라이싱 가능한 모든 범위를 생성합니다.\n",
    "        \"\"\"\n",
    "        slices = []\n",
    "        for volume_id, slice_list in self.volume_slices.items():\n",
    "            for i in range(0, len(slice_list) - self.num_channels + 1):\n",
    "                slice_range = slice_list[i:i + self.num_channels]\n",
    "                slices.append({\"volume_id\": volume_id, \"slice_range\": slice_range})\n",
    "        return slices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slice_info = self.slices[idx]\n",
    "        slice_range = slice_info[\"slice_range\"]\n",
    "        # volume_id = slice_info[\"volume_id\"]  # 볼륨 ID 추가\n",
    "\n",
    "        # 다채널 이미지 생성\n",
    "        channels = []\n",
    "        for _, file_path in slice_range:\n",
    "            img = np.array(Image.open(file_path).convert(\"L\"), dtype=np.float32)\n",
    "            img = self._resize_slice(img)\n",
    "            channels.append(img)\n",
    "\n",
    "        input_tensor = torch.tensor(np.stack(channels, axis=0))  # (num_channels, W, H)\n",
    "\n",
    "        # 라벨 처리\n",
    "        if self.label_dir:\n",
    "            label_path = self.label_dir / slice_range[0][1].name\n",
    "            label = np.array(Image.open(label_path).convert(\"L\"), dtype=np.int64)\n",
    "            label = self._resize_slice(label)\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            label_tensor = torch.full((self.slice_width, self.slice_height), self.default_label, dtype=torch.long)\n",
    "\n",
    "        # Transform 적용\n",
    "        if self.transform:\n",
    "            input_tensor, label_tensor = self.transform(input_tensor, label_tensor)\n",
    "\n",
    "        # 볼륨 ID와 함께 반환\n",
    "        return input_tensor, label_tensor, \n",
    "\n",
    "    def _resize_slice(self, slice_img):\n",
    "        \"\"\"\n",
    "        슬라이스 크기를 `slice_size`에 맞게 리샘플링합니다.\n",
    "        \"\"\"\n",
    "        # Ensure slice_img is in a compatible format (uint8 or float32)\n",
    "        if slice_img.dtype not in [np.uint8, np.float32]:\n",
    "            slice_img = slice_img.astype(np.float32)\n",
    "        \n",
    "        slice_img = Image.fromarray(slice_img)\n",
    "        slice_img = slice_img.resize((self.slice_width, self.slice_height), Image.BILINEAR)\n",
    "        return np.array(slice_img, dtype=np.float32)\n",
    "    \n",
    "# 데이터셋 경로 설정\n",
    "image_dir = \"./datasets/images/train\"\n",
    "label_dir = \"./datasets/labels/train\"\n",
    "\n",
    "# Dataset과 DataLoader 초기화\n",
    "dataset = MultiChannelCryoETDataset(image_dir=image_dir, label_dir=label_dir, num_channels=11)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 데이터 확인\n",
    "for inputs, labels in dataloader:\n",
    "    \n",
    "    print(f\"Input shape: {inputs.shape}, Label shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "model = UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=11,  # 입력 채널\n",
    "    out_channels=6,  # 클래스 수\n",
    "    channels=(64, 128, 256, 512, 1024),\n",
    "    strides=(2, 2, 2, 2)\n",
    ")\n",
    "\n",
    "# 데이터 확인 및 학습 루프\n",
    "for inputs, labels in dataloader:\n",
    "    outputs = model(inputs)  # Forward pass\n",
    "    print(f\"Model output shape: {outputs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = DiceLoss(to_onehot_y=True, softmax=True)  # Dice Loss with softmax\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))  # Compute Dice Loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6931\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "Epoch [5/5], Loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the model (U-Net)\n",
    "# class DoubleConv(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DoubleConv, self).__init__()\n",
    "#         self.double_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(UNet, self).__init__()\n",
    "#         self.enc1 = DoubleConv(in_channels, 64)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "#         self.enc2 = DoubleConv(64, 128)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "#         self.enc3 = DoubleConv(128, 256)\n",
    "#         self.pool3 = nn.MaxPool2d(2)\n",
    "#         self.bridge = DoubleConv(256, 512)\n",
    "#         self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "#         self.dec3 = DoubleConv(512, 256)\n",
    "#         self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "#         self.dec2 = DoubleConv(256, 128)\n",
    "#         self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.dec1 = DoubleConv(128, 64)\n",
    "#         self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         enc1 = self.enc1(x)\n",
    "#         enc2 = self.enc2(self.pool1(enc1))\n",
    "#         enc3 = self.enc3(self.pool2(enc2))\n",
    "#         bridge = self.bridge(self.pool3(enc3))\n",
    "#         dec3 = self.dec3(torch.cat([self.upconv3(bridge), enc3], dim=1))\n",
    "#         dec2 = self.dec2(torch.cat([self.upconv2(dec3), enc2], dim=1))\n",
    "#         dec1 = self.dec1(torch.cat([self.upconv1(dec2), enc1], dim=1))\n",
    "#         return self.out_conv(dec1)\n",
    "\n",
    "\n",
    "# # Instantiate model, loss, and optimizer\n",
    "# model = UNet(in_channels=11, out_channels=2)  # 2 classes for segmentation\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# # Training Loop\n",
    "# for epoch in range(5):  # 5 epochs for demonstration\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)  # Forward pass\n",
    "#         loss = criterion(outputs, targets)  # Compute loss\n",
    "#         loss.backward()  # Backpropagation\n",
    "#         optimizer.step()  # Update weights\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/5], Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
