{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.5.1+cu124\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.25.0\n",
      "scipy version: 1.15.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.20.1+cu124\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.1.1\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n",
      "클래스 비율: {0: 0.0, 1: 0.16393442622950818, 2: 0.01639344262295082, 3: 0.2459016393442623, 4: 0.16393442622950818, 5: 0.2459016393442623, 6: 0.16393442622950818}\n",
      "최종 합계: 1.0\n",
      "클래스 비율 리스트: [0.0, 0.16393442622950818, 0.01639344262295082, 0.2459016393442623, 0.16393442622950818, 0.2459016393442623, 0.16393442622950818]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    NormalizeIntensityd,\n",
    "    GaussianSmoothd,\n",
    "    ScaleIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    RandGaussianNoised,\n",
    "    ToTensord,\n",
    "    RandCropByLabelClassesd,\n",
    "    RandCropd,\n",
    "    RandGaussianSmoothd,\n",
    "    CastToTyped\n",
    ")\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "from src.models import UNet_CBAM\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "print_config()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import TverskyLoss\n",
    "from src.dataset.dataset import create_dataloaders\n",
    "from src.dataset.dataset_csv import create_dataloaders_from_csv\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "\n",
    "class_info = {\n",
    "    0: {\"name\": \"background\", \"weight\": 0},  # weight 없음\n",
    "    1: {\"name\": \"apo-ferritin\", \"weight\": 1000},\n",
    "    2: {\"name\": \"beta-amylase\", \"weight\": 100}, # 4130\n",
    "    3: {\"name\": \"beta-galactosidase\", \"weight\": 1500}, #3080\n",
    "    4: {\"name\": \"ribosome\", \"weight\": 1000},\n",
    "    5: {\"name\": \"thyroglobulin\", \"weight\": 1500},\n",
    "    6: {\"name\": \"virus-like-particle\", \"weight\": 1000},\n",
    "}\n",
    "\n",
    "# 가중치에 비례한 비율 계산\n",
    "raw_ratios = {\n",
    "    k: (v[\"weight\"] if v[\"weight\"] is not None else 0.01)  # 가중치 비례, None일 경우 기본값a\n",
    "    for k, v in class_info.items()\n",
    "}\n",
    "total = sum(raw_ratios.values())\n",
    "ratios = {k: v / total for k, v in raw_ratios.items()}\n",
    "\n",
    "# 최종 합계가 1인지 확인\n",
    "final_total = sum(ratios.values())\n",
    "print(\"클래스 비율:\", ratios)\n",
    "print(\"최종 합계:\", final_total)\n",
    "\n",
    "# 비율을 리스트로 변환\n",
    "ratios_list = [ratios[k] for k in sorted(ratios.keys())]\n",
    "print(\"클래스 비율 리스트:\", ratios_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import TverskyLoss\n",
    "\n",
    "# DynamicTverskyLoss 클래스 정의\n",
    "class DynamicTverskyLoss(TverskyLoss):\n",
    "    def __init__(self, lamda=0.5, **kwargs):\n",
    "        super().__init__(alpha=1 - lamda, beta=lamda, **kwargs)\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.lamda = lamda\n",
    "        self.alpha = 1 - lamda\n",
    "        self.beta = lamda\n",
    "\n",
    "\n",
    "# CombinedCETverskyLoss 클래스\n",
    "class CombinedCETverskyLoss(nn.Module):\n",
    "    def __init__(self, lamda=0.5, ce_weight=0.5, n_classes=7, class_weights=None, ignore_index=-1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "        # CrossEntropyLoss에서 클래스별 가중치를 적용\n",
    "        self.ce = nn.CrossEntropyLoss(weight=class_weights, ignore_index=self.ignore_index, reduction='mean', **kwargs)\n",
    "        \n",
    "        # TverskyLoss\n",
    "        self.tversky = DynamicTverskyLoss(lamda=lamda, reduction=\"mean\",softmax=True, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        # CrossEntropyLoss는 정수형 클래스 인덱스를 사용\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "\n",
    "        # TverskyLoss 계산 (원핫 인코딩된 라벨을 사용)\n",
    "        \n",
    "        tversky_loss = self.tversky(inputs, targets)\n",
    "\n",
    "        # 최종 손실 계산\n",
    "        final_loss = self.ce_weight * ce_loss + (1 - self.ce_weight) * tversky_loss\n",
    "        return final_loss\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.tversky.set_lamda(lamda)\n",
    "\n",
    "    @property\n",
    "    def lamda(self):\n",
    "        return self.tversky.lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Optional, Tuple, Sequence\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 0) DFLoss (분포 기반 좌표 학습)\n",
    "# -----------------------------------------------------------------------------------\n",
    "class DFLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    DFL (Distribution Focal Loss), 좌표 offset을 0~reg_max 범위로 보고,\n",
    "    floor와 floor+1 bin 사이를 나누어 CrossEntropy를 계산하는 방식.\n",
    "    \"\"\"\n",
    "    def __init__(self, reg_max=16) -> None:\n",
    "        super().__init__()\n",
    "        self.reg_max = reg_max\n",
    "\n",
    "    def forward(self, pred_dist: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        pred_dist: (N, reg_max)  # 각 coordinate별 reg_max 로짓\n",
    "        target: (N,)  # 실값 (0 <= t < reg_max)\n",
    "        \n",
    "        return: (N,) shape의 loss (coordinate별)\n",
    "        \"\"\"\n",
    "        # clamp to valid range (ex: 0 ~ reg_max-1)\n",
    "        target = target.clamp_(0, self.reg_max - 1 - 0.01)\n",
    "\n",
    "        tl = target.long()       # floor index\n",
    "        tr = tl + 1              # floor+1\n",
    "        wl = tr - target         # weight for left bin\n",
    "        wr = 1 - wl              # weight for right bin\n",
    "\n",
    "        # Cross Entropy를 각각 bin에 대해 계산\n",
    "        # pred_dist.shape = (N, reg_max)\n",
    "        # CE에는 레이블이 정수여야 하므로, tl, tr 둘을 이용해 2번 계산\n",
    "        loss_l = F.cross_entropy(pred_dist, tl, reduction=\"none\") * wl\n",
    "        # tr이 reg_max를 넘어갈 수 있으므로 clamp\n",
    "        tr = tr.clamp(max=self.reg_max - 1)\n",
    "        loss_r = F.cross_entropy(pred_dist, tr, reduction=\"none\") * wr\n",
    "\n",
    "        # 두 bin 손실 합산\n",
    "        loss = loss_l + loss_r  # (N,)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ------------------------- Fixed Sphere IoU Functions -------------------------\n",
    "def sphere_intersection_volume(r1: torch.Tensor, r2: torch.Tensor, d: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    두 구(반지름 r1, r2), 중심 거리 d에서 교집합 부피를 반환.\n",
    "    r1, r2, d는 동일한 shape를 가져야 함.\n",
    "    \"\"\"\n",
    "    no_overlap = (d >= (r1 + r2))\n",
    "    contained = (d <= (r1 - r2).abs())\n",
    "    \n",
    "    inter_vol = torch.zeros_like(d)\n",
    "    v1 = (4.0 / 3.0) * math.pi * (r1 ** 3)\n",
    "    v2 = (4.0 / 3.0) * math.pi * (r2 ** 3)\n",
    "    \n",
    "    # Case 1: 완전히 포함\n",
    "    inside_mask = contained & (~no_overlap)\n",
    "    inter_vol[inside_mask] = torch.min(v1[inside_mask], v2[inside_mask])\n",
    "    \n",
    "    # Case 2: 부분 교차\n",
    "    partial = ~(no_overlap | contained)\n",
    "    if partial.any():\n",
    "        dp = d[partial]\n",
    "        r1p = r1[partial]\n",
    "        r2p = r2[partial]\n",
    "        term = (r1p + r2p - dp) ** 2\n",
    "        numerator = math.pi * term * (dp ** 2 + 2 * dp * (r1p + r2p) - 3 * (r1p ** 2 + r2p ** 2) + 6 * r1p * r2p)\n",
    "        vol = numerator / (12.0 * dp + 1e-9)\n",
    "        vol = torch.clamp_min(vol, 0.)\n",
    "        inter_vol[partial] = vol\n",
    "    \n",
    "    return inter_vol\n",
    "\n",
    "def sphere_iou(sphere1: torch.Tensor, sphere2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    sphere1: (N, 4) (x, y, z, r)\n",
    "    sphere2: (M, 4) (x, y, z, r)\n",
    "    return: IoU (N, M)\n",
    "    \"\"\"\n",
    "    c1, r1 = sphere1[..., :3], sphere1[..., 3].clamp_min(1e-6)  # 반경이 0이 되지 않도록 방지\n",
    "    c2, r2 = sphere2[..., :3], sphere2[..., 3].clamp_min(1e-6)\n",
    "\n",
    "    d = torch.norm(c1 - c2, dim=-1)  # (...,)\n",
    "\n",
    "    # 반경이 d와 동일한 shape으로 브로드캐스팅되도록 처리\n",
    "    if r1.dim() < d.dim():\n",
    "        r1 = r1.unsqueeze(-1)\n",
    "    if r2.dim() < d.dim():\n",
    "        r2 = r2.unsqueeze(-1)\n",
    "    r1 = r1.expand_as(d)\n",
    "    r2 = r2.expand_as(d)\n",
    "\n",
    "    # 교집합 볼륨 계산\n",
    "    inter_vol = sphere_intersection_volume(r1, r2, d)\n",
    "\n",
    "    # 개별 구의 볼륨\n",
    "    v1 = (4.0 / 3.0) * math.pi * (r1 ** 3)\n",
    "    v2 = (4.0 / 3.0) * math.pi * (r2 ** 3)\n",
    "\n",
    "    # 합집합 볼륨\n",
    "    union_vol = v1 + v2 - inter_vol\n",
    "\n",
    "    # NaN 방지 (union_vol이 0이 되는 경우를 방지)\n",
    "    return inter_vol / (union_vol + 1e-6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 2) dist2sphere (DFL -> (x,y,z,r)) 함수\n",
    "# -----------------------------------------------------------------------------------\n",
    "def dist2sphere(pred_dist: torch.Tensor, anchors_3d: torch.Tensor, reg_max: int):\n",
    "    \"\"\"\n",
    "    pred_dist: (B, A, 4*reg_max) => 4 coords * reg_max bins\n",
    "    anchors_3d: (B, A, 3) or (A,3)\n",
    "    return: (B, A, 4) => (x, y, z, r)\n",
    "    \"\"\"\n",
    "    B, A, C = pred_dist.shape\n",
    "    assert C == 4 * reg_max, f\"pred_dist shape mismatch: {C} vs 4*{reg_max}\"\n",
    "\n",
    "    dist_4d = pred_dist.view(B, A, 4, reg_max)  # (B,A,4,reg_max)\n",
    "    dist_prob = F.softmax(dist_4d, dim=3)       # (B,A,4,reg_max)\n",
    "    bin_index = torch.arange(reg_max, device=pred_dist.device, dtype=pred_dist.dtype)\n",
    "    dist_val = (dist_prob * bin_index).sum(dim=3)  # (B,A,4)\n",
    "\n",
    "    # anchors_3d shape = (B,A,3) or broadcast\n",
    "    xyz = anchors_3d[..., :3] + dist_val[..., :3]\n",
    "    r   = dist_val[..., 3:].clamp_min(0.)\n",
    "    return torch.cat([xyz, r], dim=-1)  # (B,A,4)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 3) 개선된 TaskAlignedAssigner3D (margin 적용)\n",
    "# -----------------------------------------------------------------------------------\n",
    "def select_candidates_in_spheres(\n",
    "    anc_points: torch.Tensor,\n",
    "    gt_spheres: torch.Tensor,\n",
    "    margin_factor: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    anchor center가 구 중심에서 거리 <= (r * (1 + margin_factor)) 이면 candidate\n",
    "    anc_points: (n_anchors, 3) 또는 (B, n_anchors, 3)\n",
    "    gt_spheres: (B, n_max_boxes, 4)\n",
    "    returns: (B, n_max_boxes, n_anchors) bool/float\n",
    "    \"\"\"\n",
    "    B, n_g, _ = gt_spheres.shape\n",
    "    n_anchors = anc_points.shape[0]\n",
    "\n",
    "    if n_g == 0:\n",
    "        return torch.zeros((B, 0, n_anchors), device=anc_points.device)  # GT가 없을 때 예외 처리\n",
    "\n",
    "    centers = gt_spheres[..., :3]  # (B, n_g, 3)\n",
    "    radii = gt_spheres[..., 3].clamp_min(0.)  # (B, n_g)\n",
    "\n",
    "    # 차원 확장\n",
    "    cent_3d = centers.unsqueeze(2)  # (B, n_g, 1, 3)\n",
    "\n",
    "    if anc_points.dim() == 2:\n",
    "        anc_3d = anc_points.unsqueeze(0).unsqueeze(0)  # (1, 1, nA, 3)\n",
    "    else:\n",
    "        anc_3d = anc_points.unsqueeze(1)  # (B, 1, nA, 3)\n",
    "\n",
    "    d = (anc_3d - cent_3d).pow(2).sum(-1).sqrt()  # (B, n_g, nA)\n",
    "    r_expanded = radii.unsqueeze(-1) * (1.0 + margin_factor)  # (B, n_g, 1)\n",
    "    mask = (d <= r_expanded)\n",
    "\n",
    "    return mask.float()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class TaskAlignedAssigner3D(nn.Module):\n",
    "    \"\"\"\n",
    "    A task-aligned assigner for 3D sphere detection.\n",
    "    Combine cls^alpha * IoU^beta, pick top-k anchors for each GT,\n",
    "    handle multi-assign by picking the GT with highest IoU for that anchor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        topk=30,\n",
    "        num_classes=1,\n",
    "        alpha=1.0,\n",
    "        beta=6.0,\n",
    "        iou_weight=3.0,\n",
    "        cls_weight=1.0,\n",
    "        margin_factor=0.2,\n",
    "        eps=1e-9\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iou_weight = iou_weight\n",
    "        self.cls_weight = cls_weight\n",
    "        self.margin_factor = margin_factor\n",
    "        self.eps = eps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def assign(\n",
    "        self,\n",
    "        pd_scores: torch.Tensor,    # (B, A, nc) raw cls logits\n",
    "        pd_spheres: torch.Tensor,   # (B, A, 4)\n",
    "        anc_points: torch.Tensor,   # (A, 3) 실제 anchor center\n",
    "        gt_spheres: torch.Tensor,   # (B, n_max_boxes, 4)\n",
    "        gt_classes: torch.Tensor,   # (B, n_max_boxes)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            fg_mask: (B, A) bool\n",
    "            box_target: (B, A, 4)\n",
    "            cls_target: (B, A, nc)\n",
    "        \"\"\"\n",
    "        B, A, nc = pd_scores.shape\n",
    "        n_max_boxes = gt_spheres.shape[1]\n",
    "        device = pd_scores.device\n",
    "\n",
    "        if n_max_boxes == 0:\n",
    "            # no GT\n",
    "            fg_mask = torch.zeros((B, A), dtype=torch.bool, device=device)\n",
    "            box_target = torch.zeros((B, A, 4), device=device)\n",
    "            cls_target = torch.zeros((B, A, nc), device=device)\n",
    "            return fg_mask, box_target, cls_target\n",
    "\n",
    "        # 1) select_candidates_in_spheres => margin_factor 적용\n",
    "        #    anc_points: (A,3), gt_spheres: (B, n_max, 4)\n",
    "        mask_in_spheres = select_candidates_in_spheres(\n",
    "            anc_points, gt_spheres, margin_factor=self.margin_factor\n",
    "        )  # shape (B, n_max, A)\n",
    "\n",
    "        # 2) compute alignment metric => (cls^alpha * iou^beta)\n",
    "        align_metric, overlaps = self.get_box_metrics(\n",
    "            pd_scores, pd_spheres, gt_classes, gt_spheres, mask_in_spheres\n",
    "        )\n",
    "\n",
    "        # 3) top-k mask\n",
    "        mask_topk = self.select_topk_candidates(align_metric, topk_mask=None)\n",
    "        mask_pos = mask_in_spheres * mask_topk  # (B,n_max,A)\n",
    "\n",
    "        # 4) select_highest_overlaps => multi-assign 해소\n",
    "        target_gt_idx, fg_mask, mask_pos = self.select_highest_overlaps(\n",
    "            mask_pos, overlaps, n_max_boxes\n",
    "        )\n",
    "        # 5) get_targets => (cls_target, box_target)\n",
    "        target_labels, target_spheres, target_scores = self.get_targets(\n",
    "            gt_classes, gt_spheres, target_gt_idx, fg_mask\n",
    "        )\n",
    "\n",
    "        return fg_mask, target_spheres, target_scores\n",
    "\n",
    "    def get_box_metrics(\n",
    "        self,\n",
    "        pd_scores: torch.Tensor,\n",
    "        pd_spheres: torch.Tensor,\n",
    "        gt_classes: torch.Tensor,\n",
    "        gt_spheres: torch.Tensor,\n",
    "        mask_in_spheres: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        overlap => sphere_iou\n",
    "        cls => pd_scores.sigmoid() for each gt class\n",
    "        align_metric = cls^alpha * iou^beta\n",
    "        \"\"\"\n",
    "        B, A, nc = pd_scores.shape\n",
    "        device = pd_scores.device\n",
    "        n_max_boxes = gt_spheres.shape[1]\n",
    "\n",
    "        overlaps = torch.zeros((B, n_max_boxes, A), device=device)\n",
    "        bbox_scores = torch.zeros((B, n_max_boxes, A), device=device, dtype=pd_scores.dtype)\n",
    "\n",
    "        # compute iou\n",
    "        for b_i in range(B):\n",
    "            # pd_spheres[b_i]: (A,4)\n",
    "            # gt_spheres[b_i]: (n_max_boxes,4)\n",
    "            # sphere_iou => (A,n_max_boxes)\n",
    "            ps = pd_spheres[b_i].unsqueeze(1)  # (A,1,4)\n",
    "            gs = gt_spheres[b_i].unsqueeze(0)  # (1,n_max_boxes,4)\n",
    "            ious = sphere_iou(ps, gs)          # (A,n_max_boxes)\n",
    "            overlaps[b_i] = ious.transpose(0,1)  # => (n_max_boxes,A)\n",
    "\n",
    "        # compute cls score => pd_scores.sigmoid()\n",
    "        pd_sig = pd_scores.sigmoid()\n",
    "        for b_i in range(B):\n",
    "            for g_i in range(n_max_boxes):\n",
    "                if mask_in_spheres[b_i, g_i].any():\n",
    "                    cls_idx = int(gt_classes[b_i, g_i].item())\n",
    "                    if 0 <= cls_idx < nc:\n",
    "                        # bbox_scores[b_i,g_i,:] = pd_sig[b_i,:, cls_idx]\n",
    "                        bbox_scores[b_i, g_i] = pd_sig[b_i, :, cls_idx]\n",
    "\n",
    "        # align_metric\n",
    "        #   => (cls^alpha) * (iou^beta)\n",
    "        #   => mask_in_spheres=0 인 곳은 0으로\n",
    "        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
    "        align_metric = align_metric * mask_in_spheres  # outside sphere => 0\n",
    "        return align_metric, overlaps\n",
    "\n",
    "    def select_topk_candidates(self, metrics: torch.Tensor, topk_mask: Optional[torch.Tensor]=None, largest=True):\n",
    "        \"\"\"\n",
    "        metrics: (B, n_max_boxes, A)\n",
    "        => pick top-k anchors per GT box\n",
    "        return: (B, n_max_boxes, A) mask\n",
    "        \"\"\"\n",
    "        B, n_m, A = metrics.shape\n",
    "        val, idxs = torch.topk(metrics, self.topk, dim=-1, largest=largest)  # (B,n_m,topk)\n",
    "        if topk_mask is None:\n",
    "            topk_mask = (val > self.eps)\n",
    "\n",
    "        mask = torch.zeros_like(metrics)\n",
    "        for b_i in range(B):\n",
    "            for g_i in range(n_m):\n",
    "                valid_k = topk_mask[b_i, g_i]  # (topk,)\n",
    "                sel_ind = idxs[b_i, g_i, valid_k]  # anchor indices\n",
    "                mask[b_i, g_i, sel_ind] = 1.0\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def select_highest_overlaps(self, mask_pos, overlaps, n_max_boxes):\n",
    "        \"\"\"\n",
    "        mask_pos: (B, n_max_boxes, A)\n",
    "        overlaps: (B, n_max_boxes, A)\n",
    "        => 한 anchor가 여러 GT에 할당된 경우 => overlaps 최대인 GT만\n",
    "        return:\n",
    "          target_gt_idx: (B,A)\n",
    "          fg_mask: (B,A)\n",
    "          mask_pos => updated\n",
    "        \"\"\"\n",
    "        fg_mask = mask_pos.sum(dim=1)  # (B,A)\n",
    "        if fg_mask.max() > 1:\n",
    "            # multi-assign => pick max overlap GT\n",
    "            max_overlaps_idx = overlaps.argmax(dim=1)  # (B,A)\n",
    "            new_mask = torch.zeros_like(mask_pos)\n",
    "            B_, _, A_ = mask_pos.shape\n",
    "            for b_i in range(B_):\n",
    "                for a_i in range(A_):\n",
    "                    g_i = max_overlaps_idx[b_i,a_i]\n",
    "                    new_mask[b_i, g_i, a_i] = 1.0\n",
    "            mask_pos = new_mask\n",
    "            fg_mask = mask_pos.sum(dim=1)\n",
    "\n",
    "        # target_gt_idx\n",
    "        target_gt_idx = mask_pos.argmax(dim=1)  # (B,A)\n",
    "        return target_gt_idx, fg_mask.bool(), mask_pos\n",
    "\n",
    "    def get_targets(self, gt_classes, gt_spheres, target_gt_idx, fg_mask):\n",
    "        \"\"\"\n",
    "        Return (assigned_labels, assigned_spheres, cls_target)\n",
    "        \"\"\"\n",
    "        B, A = fg_mask.shape\n",
    "        device = gt_classes.device\n",
    "        n_max_boxes = gt_spheres.shape[1]\n",
    "\n",
    "        # gather idx\n",
    "        batch_ind = torch.arange(B, device=device).view(-1,1).expand(B,A)\n",
    "        gather_idx = target_gt_idx + batch_ind*n_max_boxes  # (B,A)\n",
    "\n",
    "        gt_spheres_2d = gt_spheres.view(B*n_max_boxes, 4)\n",
    "        gt_classes_2d = gt_classes.view(B*n_max_boxes)\n",
    "\n",
    "        assigned_labels  = gt_classes_2d[gather_idx.reshape(-1)].view(B,A)\n",
    "        assigned_spheres= gt_spheres_2d[gather_idx.reshape(-1)].view(B,A,4)\n",
    "\n",
    "        # build cls_target\n",
    "        nc = self.num_classes\n",
    "        cls_target = torch.zeros((B, A, nc), device=device)\n",
    "        for b_i in range(B):\n",
    "            valid_anchors = fg_mask[b_i].nonzero(as_tuple=True)[0]  # (some_count,)\n",
    "            for a_i in valid_anchors:\n",
    "                c_idx = int(assigned_labels[b_i,a_i].item())\n",
    "                if 0 <= c_idx < nc:\n",
    "                    cls_target[b_i,a_i,c_idx] = 1.0\n",
    "\n",
    "        return assigned_labels, assigned_spheres, cls_target\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 4) 최종 Loss\n",
    "# -----------------------------------------------------------------------------------\n",
    "class YOLOv8SphereLoss3D_Advanced(nn.Module):\n",
    "    \"\"\"\n",
    "    개선된 3D YOLO Loss with:\n",
    "      - Top-k TaskAlignedAssigner3D (with margin_factor)\n",
    "      - Sphere IoU\n",
    "      - Classification (BCE)\n",
    "      - DFL\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 7,\n",
    "        reg_max: int = 16,\n",
    "        iou_weight: float = 3.0,\n",
    "        cls_weight: float = 1.0,\n",
    "        dfl_weight: float = 1.0,\n",
    "        topk: int = 30,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 6.0,\n",
    "        margin_factor: float = 0.1,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_max = reg_max\n",
    "        self.iou_weight = iou_weight\n",
    "        self.cls_weight = cls_weight\n",
    "        self.dfl_weight = dfl_weight\n",
    "        self.topk = topk\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.margin_factor = margin_factor\n",
    "        self.device = device\n",
    "\n",
    "        # 새 TaskAlignedAssigner3D\n",
    "        self.assigner = TaskAlignedAssigner3D(\n",
    "            topk=topk,\n",
    "            num_classes=num_classes,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            iou_weight=iou_weight,\n",
    "            cls_weight=cls_weight,\n",
    "            margin_factor=margin_factor\n",
    "        )\n",
    "        # DFL 모듈\n",
    "        self.dfl = DFLoss(reg_max=reg_max)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pred_dist: torch.Tensor,   # (B, A, 4*reg_max)\n",
    "        pred_cls: torch.Tensor,    # (B, A, nc)\n",
    "        anchors_3d: torch.Tensor,  # (B, A, 3) or (A,3)\n",
    "        gt_spheres: List[torch.Tensor],  # list of (n_g, 4)\n",
    "        gt_classes: List[torch.Tensor],  # list of (n_g,)\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        device = pred_dist.device\n",
    "        B, A, _ = pred_dist.shape\n",
    "\n",
    "        # decode => (x,y,z,r)\n",
    "        pred_sphere = dist2sphere(pred_dist, anchors_3d, self.reg_max)  # (B,A,4)\n",
    "        print(\"pred_sphere:\", pred_sphere.shape)\n",
    "        # GT stack\n",
    "        max_g = max(x.shape[0] for x in gt_spheres) if len(gt_spheres) else 0\n",
    "        if max_g == 0:\n",
    "            # no GT => background\n",
    "            cls_loss_bg = F.binary_cross_entropy_with_logits(\n",
    "                pred_cls, torch.zeros_like(pred_cls), reduction='mean')\n",
    "            return dict(\n",
    "                loss_total=cls_loss_bg,\n",
    "                loss_box=torch.tensor(0., device=device),\n",
    "                loss_cls=cls_loss_bg.detach(),\n",
    "                loss_dfl=torch.tensor(0., device=device)\n",
    "            )\n",
    "\n",
    "        # pad\n",
    "        s_list, c_list = [], []\n",
    "        for b in range(B):\n",
    "            s_ = gt_spheres[b].to(device)\n",
    "            c_ = gt_classes[b].to(device)\n",
    "            pad_s = torch.zeros((max_g, 4), device=device)\n",
    "            pad_c = torch.zeros((max_g,), device=device)\n",
    "            if s_.shape[0] > 0:\n",
    "                pad_s[:s_.shape[0]] = s_\n",
    "                pad_c[:c_.shape[0]] = c_\n",
    "            s_list.append(pad_s)\n",
    "            c_list.append(pad_c)\n",
    "        stacked_spheres = torch.stack(s_list, dim=0)  # (B,max_g,4)\n",
    "        stacked_classes = torch.stack(c_list, dim=0)  # (B,max_g)\n",
    "\n",
    "        # Assigner 호출 부분 수정\n",
    "        fg_mask, box_target, cls_target = self.assigner.assign(\n",
    "            pd_scores=pred_cls,         # (B, A, nc)\n",
    "            pd_spheres=pred_sphere,     # (B, A, 4)\n",
    "            anc_points=anchors_3d,      # (A,3) or (B,A,3)\n",
    "            gt_spheres=stacked_spheres, # (B, max_g,4)\n",
    "            gt_classes=stacked_classes  # (B, max_g)\n",
    "        )\n",
    "        # FG 마스크가 전부 False인지 체크\n",
    "        if fg_mask.sum() == 0:\n",
    "            print(\"⚠️ FG 마스크가 전부 False입니다. GT와 예측값이 매칭되지 않았을 수 있습니다.\")\n",
    "            print(f\"GT 개수: {max_g}, Anchor 개수: {A}\")\n",
    "        # foreground의 총 개수 계산\n",
    "        pos_count = int(fg_mask.sum().item())  # 혹은 fg_mask.sum().item()\n",
    "\n",
    "        # classification loss\n",
    "        cls_loss_full = F.binary_cross_entropy_with_logits(\n",
    "            pred_cls, cls_target, reduction='none'\n",
    "        )  # (B,A,nc)\n",
    "        cls_loss = cls_loss_full.mean()\n",
    "\n",
    "        if pos_count < 1:\n",
    "            # no fg\n",
    "            return dict(\n",
    "                loss_total=cls_loss,\n",
    "                loss_box=torch.tensor(0., device=device),\n",
    "                loss_cls=cls_loss.detach(),\n",
    "                loss_dfl=torch.tensor(0., device=device)\n",
    "            )\n",
    "\n",
    "        # box loss => sphere IoU\n",
    "        pred_fg = pred_sphere[fg_mask]   # (pos_count,4)\n",
    "        box_fg  = box_target[fg_mask]    # (pos_count,4)\n",
    "        ious_fg = sphere_iou(pred_fg, box_fg)\n",
    "        box_loss = (1. - ious_fg).mean()\n",
    "\n",
    "        # DFL loss\n",
    "        # offsets_fg = box_fg - anchors_3d[fg_mask] (for x,y,z), r as is\n",
    "        if anchors_3d.dim()==2 and anchors_3d.shape[0]==A:\n",
    "            # broadcast B\n",
    "            anchors_to_sub = anchors_3d.unsqueeze(0).expand(B,-1,-1)[fg_mask]\n",
    "        else:\n",
    "            anchors_to_sub = anchors_3d[fg_mask]\n",
    "        offsets_fg = torch.zeros_like(box_fg)\n",
    "        offsets_fg[..., :3] = box_fg[..., :3] - anchors_to_sub[..., :3]\n",
    "        offsets_fg[..., 3]  = box_fg[..., 3]\n",
    "\n",
    "        pred_dist_fg = pred_dist[fg_mask].view(-1,4,self.reg_max)  # (pos_count,4,reg_max)\n",
    "\n",
    "        dfl_val = 0.\n",
    "        for coord_i in range(4):\n",
    "            pd_coord = pred_dist_fg[:, coord_i, :]    # (pos_count, reg_max)\n",
    "            gt_coord = offsets_fg[:, coord_i]         # (pos_count,)\n",
    "            loss_coord = self.dfl(pd_coord, gt_coord) # (pos_count,)\n",
    "            dfl_val += loss_coord.mean()\n",
    "        dfl_val /= 4.\n",
    "\n",
    "        # 합산\n",
    "        total_loss = self.iou_weight*box_loss + self.cls_weight*cls_loss + self.dfl_weight*dfl_val\n",
    "\n",
    "        return dict(\n",
    "            loss_total=total_loss,\n",
    "            loss_box=box_loss.detach(),\n",
    "            loss_cls=cls_loss.detach(),\n",
    "            loss_dfl=dfl_val.detach()\n",
    "        )\n",
    "        \n",
    "# ------------------------- Anchor Generation Function -------------------------\n",
    "def generate_anchors_3d(feature_shape: Tuple[int, int, int], stride: int) -> torch.Tensor:\n",
    "    D, H, W = feature_shape\n",
    "    grid_z, grid_y, grid_x = torch.meshgrid(\n",
    "        torch.arange(D, dtype=torch.float32),\n",
    "        torch.arange(H, dtype=torch.float32),\n",
    "        torch.arange(W, dtype=torch.float32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    grid_x = (grid_x + 0.5) * stride\n",
    "    grid_y = (grid_y + 0.5) * stride\n",
    "    grid_z = (grid_z + 0.5) * stride\n",
    "    anchors = torch.stack([grid_x, grid_y, grid_z], dim=-1).reshape(-1, 3)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cc3d\n",
    "from monai.data import MetaTensor\n",
    "\n",
    "def extract_and_convert_3d_bounding_boxes(labels, min_size=5, normalize=True, clip_boxes=True):\n",
    "    \"\"\"\n",
    "    YOLOv8SphereLoss3D_Advanced에서 사용할 수 있도록 GT 변환.\n",
    "\n",
    "    Args:\n",
    "        labels (MetaTensor): (B, 1, D, H, W) 형태의 라벨 텐서\n",
    "        min_size (int): 최소 객체 크기 필터링\n",
    "        normalize (bool): 0~1 정규화 여부\n",
    "        clip_boxes (bool): 크롭된 영역을 반영하여 bounding box 조정\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[torch.Tensor], List[torch.Tensor]]: (gt_spheres, gt_classes)\n",
    "            - gt_spheres: List[Tensor] -> (cx, cy, cz, radius)\n",
    "            - gt_classes: List[Tensor] -> (class_id,)\n",
    "    \"\"\"\n",
    "    batch_size, _, d, h, w = labels.shape\n",
    "    gt_spheres = []\n",
    "    gt_classes = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        label_map = labels[b, 0].cpu().numpy()  # (D, H, W)\n",
    "        crop_coords = labels.meta.get(\"crop_coords\", (0, d, 0, h, 0, w))  # 크롭된 영역 좌표\n",
    "        d_min, d_max, h_min, h_max, w_min, w_max = crop_coords\n",
    "\n",
    "        objects = []\n",
    "        for class_id in np.unique(label_map):\n",
    "            if class_id == 0:\n",
    "                continue  # 배경 무시\n",
    "\n",
    "            binary_mask = (label_map == class_id).astype(np.uint8)\n",
    "            labeled, num_features = cc3d.connected_components(binary_mask, connectivity=26, return_N=True)\n",
    "\n",
    "            for obj_id in range(1, num_features + 1):\n",
    "                coords = np.array(np.where(labeled == obj_id))  # (3, num_points)\n",
    "\n",
    "                if coords.shape[1] < min_size:\n",
    "                    continue  # 너무 작은 객체 필터링\n",
    "\n",
    "                z_min, y_min, x_min = coords.min(axis=1)\n",
    "                z_max, y_max, x_max = coords.max(axis=1)\n",
    "\n",
    "                # Clipping 적용 (Bounding Box가 크롭된 영역을 벗어나지 않도록 보정)\n",
    "                if clip_boxes:\n",
    "                    z_min = max(z_min, d_min)\n",
    "                    z_max = min(z_max, d_max)\n",
    "                    y_min = max(y_min, h_min)\n",
    "                    y_max = min(y_max, h_max)\n",
    "                    x_min = max(x_min, w_min)\n",
    "                    x_max = min(x_max, w_max)\n",
    "\n",
    "                # 중심 좌표 및 크기 계산\n",
    "                x_center = (x_min + x_max) / 2\n",
    "                y_center = (y_min + y_max) / 2\n",
    "                z_center = (z_min + z_max) / 2\n",
    "                r_x = (x_max - x_min) / 2\n",
    "                r_y = (y_max - y_min) / 2\n",
    "                r_z = (z_max - z_min) / 2\n",
    "                radius = max(r_x, r_y, r_z)\n",
    "\n",
    "                # 정규화 (0~1 범위)\n",
    "                if normalize:\n",
    "                    x_center = (x_center - w_min) / (w_max - w_min)\n",
    "                    y_center = (y_center - h_min) / (h_max - h_min)\n",
    "                    z_center = (z_center - d_min) / (d_max - d_min)\n",
    "                    radius /= max(w_max - w_min, h_max - h_min, d_max - d_min)  # 최대 길이로 정규화\n",
    "\n",
    "                objects.append([int(class_id), x_center, y_center, z_center, radius])\n",
    "\n",
    "        # YOLOv8 Loss 입력 형식으로 변환\n",
    "        gt_spheres.append(torch.tensor([o[1:] for o in objects], dtype=torch.float32) if objects else torch.empty((0, 4)))\n",
    "        gt_classes.append(torch.tensor([o[0] for o in objects], dtype=torch.long) if objects else torch.empty((0,), dtype=torch.long))\n",
    "\n",
    "    return gt_spheres, gt_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from collections.abc import Sequence\n",
    "\n",
    "# 불필요한 import 제거\n",
    "# from monai.networks.blocks.convolutions import Convolution, ResidualUnit\n",
    "# from monai.networks.layers.simplelayers import SkipConnection\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "\n",
    "from src.models.unet_block import Encoder, Decoder, get_conv_layer\n",
    "from src.models.cbam import CBAM3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def autopad(k, p=None):  \n",
    "    \"\"\"\n",
    "    3D 컨볼루션에서 padding을 자동 계산해주기 위한 헬퍼 함수 (optional).\n",
    "    커널 사이즈가 정수 하나라면 (k,k,k)로 확장, 패딩도 동일하게 적용.\n",
    "    \"\"\"\n",
    "    # 예: k가 3이면 (3,3,3), p=None이면 자동으로 (1,1,1) 세팅\n",
    "    if isinstance(k, int):\n",
    "        k = (k, k, k)\n",
    "    if p is None:\n",
    "        # same padding\n",
    "        p = tuple((ki // 2) for ki in k)\n",
    "    return k, p\n",
    "\n",
    "\n",
    "class YOLO3DHead(nn.Module):\n",
    "    \"\"\"\n",
    "    3D YOLO Detection Head 예시 스케치\n",
    "    ----------------------------------\n",
    "    입력  : (B, in_channels, D, H, W)\n",
    "    출력  : (B, out_channels, D, H, W)\n",
    "      - out_channels = nc + 4*reg_max\n",
    "        (클래스 확률 nc개 + dist 분포(dx,dy,dz,dr) * reg_max)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Sequence[int] | int,\n",
    "        stride: int,\n",
    "        norm: tuple | str,\n",
    "        act: tuple ,\n",
    "        reg_max: int = 16,\n",
    "        dropout: tuple | str | float | None = None,\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.reg_max = reg_max\n",
    "        # 최종 출력 채널 수\n",
    "        self.out_channels = 1 + out_channels + 4 * reg_max\n",
    "\n",
    "        # if hidden_ch is None:\n",
    "        #     hidden_ch = in_channels  # 간단히 동일 채널로\n",
    "\n",
    "        # 예시로 2개의 Conv3dBlock을 쌓고 마지막 Conv3d(커널1)로 out_channels를 뽑도록 구성\n",
    "        # 필요에 따라 더 복잡한 모듈(C3, SPP, Residual 등)로 확장 가능\n",
    "        \n",
    "        self.conv_out = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            conv_only=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape = (B, in_channels, D, H, W)\n",
    "        return  = (B, out_channels, D, H, W)\n",
    "        \"\"\"\n",
    "        # 3) 최종 1x1x1 Conv -> (nc + 4*reg_max) 채널\n",
    "        out = self.conv_out(x)\n",
    "        return out\n",
    "\n",
    "class c_Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Sequence[int] | int,\n",
    "        stride: int,\n",
    "        norm_name: tuple | str,\n",
    "        act_name: tuple | str = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "        dropout: tuple | str | float | None = None,\n",
    "        conv_only=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            norm=norm_name,\n",
    "            act=act_name,\n",
    "            dropout=dropout,\n",
    "            conv_only=conv_only,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "        self.cbam = CBAM3D(channels=out_channels, reduction=8, spatial_kernel_size=3)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.cbam(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SegDetect_Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        channels: Sequence[int],\n",
    "        strides: Sequence[int],\n",
    "        kernel_size: Sequence[int] | int = 3,\n",
    "        up_kernel_size: Sequence[int] | int = 3,\n",
    "        act: tuple | str = Act.PRELU,\n",
    "        norm: tuple | str = Norm.INSTANCE,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "        adn_ordering: str = \"NDA\",\n",
    "        use_det: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if len(channels) < 2:\n",
    "            raise ValueError(\"the length of `channels` should be no less than 2.\")\n",
    "        \n",
    "        # 기존 코드와 동일한 검사\n",
    "        delta = len(strides) - (len(channels) - 1)\n",
    "        if delta < 0:\n",
    "            raise ValueError(\"the length of `strides` should equal `len(channels) - 1`.\")\n",
    "        if delta > 0:\n",
    "            warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n",
    "        \n",
    "        if isinstance(kernel_size, Sequence) and len(kernel_size) != spatial_dims:\n",
    "            raise ValueError(\"the length of `kernel_size` should equal to `dimensions`.\")\n",
    "        if isinstance(up_kernel_size, Sequence) and len(up_kernel_size) != spatial_dims:\n",
    "            raise ValueError(\"the length of `up_kernel_size` should equal to `dimensions`.\")\n",
    "\n",
    "        self.dimensions = spatial_dims\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_kernel_size = up_kernel_size\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.adn_ordering = adn_ordering\n",
    "        self.use_det = use_det\n",
    "        # ---------------------\n",
    "        # Encoder\n",
    "        # ---------------------\n",
    "        self.encoder1 = Encoder(\n",
    "            spatial_dims,\n",
    "            in_channels,\n",
    "            channels[0],\n",
    "            kernel_size,\n",
    "            strides[0],\n",
    "            norm,\n",
    "            act,\n",
    "            dropout,\n",
    "        )\n",
    "        self.encoder2 = Encoder(\n",
    "            spatial_dims,\n",
    "            channels[0],\n",
    "            channels[1],\n",
    "            kernel_size,\n",
    "            strides[1],\n",
    "            norm,\n",
    "            act,\n",
    "            dropout,\n",
    "        )\n",
    "        self.encoder3 = Encoder(\n",
    "            spatial_dims,\n",
    "            channels[1],\n",
    "            channels[2],\n",
    "            kernel_size,\n",
    "            strides[2],\n",
    "            norm,\n",
    "            act,\n",
    "            dropout,\n",
    "        )\n",
    "        # encoder4는 strides[3]를 사용할 수 있도록 strides에 4개 값을 넣어주거나, 아래처럼 stride=1로 따로 설정 가능\n",
    "        # 여기서는 strides에 4개 값을 넣어준다고 가정함\n",
    "        self.bottleneck = Encoder(\n",
    "            spatial_dims,\n",
    "            channels[2],\n",
    "            channels[3],\n",
    "            kernel_size,\n",
    "            1, \n",
    "            norm,\n",
    "            act,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "        # self.cbam = CBAM3D(channels=channels[3], reduction=8, spatial_kernel_size=3)\n",
    "        # ---------------------\n",
    "        # Decoder\n",
    "        # ---------------------\n",
    "        self.decoder3 = c_Decoder(\n",
    "            spatial_dims,\n",
    "            channels[3] + channels[2],\n",
    "            channels[1],\n",
    "            up_kernel_size,\n",
    "            strides[2],\n",
    "            norm,\n",
    "            act,\n",
    "            dropout,\n",
    "        )\n",
    "        self.decoder2 = c_Decoder(\n",
    "            spatial_dims,\n",
    "            channels[1] + channels[1],\n",
    "            channels[0],\n",
    "            up_kernel_size,\n",
    "            strides[1],\n",
    "            norm,\n",
    "            act,\n",
    "            dropout,\n",
    "        )\n",
    "        self.decoder1 = c_Decoder(\n",
    "            spatial_dims,\n",
    "            channels[0] + channels[0],\n",
    "            out_channels,\n",
    "            up_kernel_size,\n",
    "            strides[0],\n",
    "            conv_only=True,\n",
    "            norm_name=None,\n",
    "        )\n",
    "        \n",
    "        self.dect_head = YOLO3DHead(\n",
    "            spatial_dims,\n",
    "            channels[0],\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            norm=None,\n",
    "            act=None,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.encoder3(x2)\n",
    "        \n",
    "        x4 = self.bottleneck(x3)\n",
    "        x_det = self.dect_head(x4)\n",
    "\n",
    "        x = self.decoder3(x4, x3)\n",
    "        x = self.decoder2(x, x2)\n",
    "        x = self.decoder1(x, x1)\n",
    "        \n",
    "        if self.use_det == True:\n",
    "            return x, x_det\n",
    "        else:\n",
    "            return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 실행 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CONFIG\n",
    "train_loader, val_loader = None, None\n",
    "train_csv = \"./datasets/train.csv\"\n",
    "val_csv = \"./datasets/val.csv\"\n",
    "img_size =  64 # Match your patch size\n",
    "img_depth = 32\n",
    "n_classes = 7\n",
    "batch_size = 32 # 13.8GB GPU memory required for 128x128 img size\n",
    "loader_batch = 2\n",
    "num_samples = batch_size // loader_batch # 한 이미지에서 뽑을 샘플 수\n",
    "num_repeat = 8\n",
    "val_num_repeat = 10\n",
    "\n",
    "# MODEL CONFIG\n",
    "feature_size = [32, 64, 128, 256]\n",
    "use_checkpoint = True\n",
    "dropout= 0.25\n",
    "\n",
    "# LOSS CONFIG\n",
    "lamda = 0.5\n",
    "ce_weight = 0.4\n",
    "class_weights = None\n",
    "# class_weights = torch.tensor([0.0001, 1, 0.001, 1.1, 1, 1.1, 1], dtype=torch.float32)  # 클래스별 가중치\n",
    "reg_max=16\n",
    "iou_weight = 3.0\n",
    "cls_weight = 1.0\n",
    "dfl_weight = 1.0\n",
    "\n",
    "det_weight = 1\n",
    "# TRAIN CONFIG\n",
    "num_epochs = 4000\n",
    "lr = 0.001\n",
    "accumulation_steps = 1\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "best_val_fbeta_score = 0\n",
    "\n",
    "sigma = 1.5\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    \n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    # GaussianSmoothd(\n",
    "    #     keys=[\"image\"],      # 변환을 적용할 키\n",
    "    #     sigma=[sigma, sigma, sigma]  # 각 축(x, y, z)의 시그마 값\n",
    "    #     ),\n",
    "])\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    # RandGaussianSmoothd(\n",
    "    # keys=[\"image\"],      # 변환을 적용할 키\n",
    "    # sigma_x = (0.5, sigma), # 각 축(x, y, z)의 시그마 값\n",
    "    # sigma_y = (0.5, sigma),\n",
    "    # sigma_z = (0.5, sigma),\n",
    "    # prob=0.5,\n",
    "    # ),\n",
    "])\n",
    "val_random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    # RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    # RandGaussianSmoothd(\n",
    "    # keys=[\"image\"],      # 변환을 적용할 키\n",
    "    # sigma_x = (0.0, sigma), # 각 축(x, y, z)의 시그마 값\n",
    "    # sigma_y = (0.0, sigma),\n",
    "    # sigma_z = (0.0, sigma),\n",
    "    # prob=1.0,\n",
    "    # ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 40/40 [00:04<00:00,  9.17it/s]\n",
      "Loading dataset: 100%|██████████| 3/3 [00:00<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = create_dataloaders_from_csv(\n",
    "    train_csv,\n",
    "    val_csv, \n",
    "    train_non_random_transforms = non_random_transforms, \n",
    "    val_non_random_transforms=non_random_transforms,\n",
    "    train_random_transforms=random_transforms,\n",
    "    val_random_transforms=val_random_transforms,\n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0,train_num_repeat=num_repeat, val_num_repeat=val_num_repeat\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========== 1. 데이터 로드 ==========\n",
    "batch = next(iter(val_loader))\n",
    "img = batch[\"image\"]\n",
    "label = batch[\"label\"]\n",
    "\n",
    "print(img.shape)\n",
    "num_b = 18\n",
    "num_slice = 32\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "B, _, D, H, W = img.shape  # 배치 크기, 깊이(D), 높이(H), 너비(W)\n",
    "stride = 4  # feature map stride\n",
    "feature_shape = (D // stride, H // stride, W // stride)\n",
    "\n",
    "# 3D 앵커 생성\n",
    "anchors_3d = generate_anchors_3d(feature_shape, stride).to(img.device)  # (A,3)\n",
    "A = anchors_3d.shape[0]\n",
    "anchors_3d = anchors_3d.unsqueeze(0).expand(B, -1, -1)  # (B, A, 3)\n",
    "\n",
    "for i in range(num_slice):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img[num_b, 0, i, :, :].cpu().numpy(), cmap='gray')\n",
    "    ax.imshow(label[num_b, 0, i, :, :].cpu().numpy(), alpha=0.3)\n",
    "    ax.set_title(f'{np.unique(label[num_b, 0, i, :, :].cpu().numpy())}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 2. GT 변환 ==========\n",
    "# GT 변환\n",
    "gt_spheres, gt_classes = extract_and_convert_3d_bounding_boxes(label, min_size=10, normalize=False)\n",
    "\n",
    "def pad_gt(gt_spheres, gt_classes, max_g):\n",
    "    \"\"\"배치 내 최대 GT 개수를 맞추기 위해 padding 적용\"\"\"\n",
    "    s_list, c_list = [], []\n",
    "    for b in range(B):\n",
    "        s_ = gt_spheres[b].to(img.device)\n",
    "        c_ = gt_classes[b].to(img.device)\n",
    "        pad_s = torch.zeros((max_g, 4), device=img.device)\n",
    "        pad_c = torch.zeros((max_g,), device=img.device)\n",
    "        if s_.shape[0] > 0:\n",
    "            pad_s[:s_.shape[0]] = s_\n",
    "            pad_c[:c_.shape[0]] = c_\n",
    "        s_list.append(pad_s)\n",
    "        c_list.append(pad_c)\n",
    "    return torch.stack(s_list, dim=0), torch.stack(c_list, dim=0)\n",
    "\n",
    "# GT 패딩 적용\n",
    "max_g = max(x.shape[0] for x in gt_spheres) if len(gt_spheres) else 0\n",
    "stacked_spheres, stacked_classes = pad_gt(gt_spheres, gt_classes, max_g)\n",
    "\n",
    "# ========== 2. 모델 예측값 생성 (더미 데이터) ==========\n",
    "pred_dist = torch.randn(B, A, 4 * 16, device=img.device)  # (-∞, +∞) 범위\n",
    "pred_dist = (pred_dist - pred_dist.min()) / (pred_dist.max() - pred_dist.min())  # [0,1] 정규화\n",
    "\n",
    "pred_cls = torch.randn(B, A, 7, device=img.device)  # Binary classification (1 class)\n",
    "\n",
    "# ========== 3. 손실 함수 계산 ==========\n",
    "loss_fn = YOLOv8SphereLoss3D_Advanced(num_classes=7, reg_max=16, device=img.device)\n",
    "loss_dict = loss_fn(pred_dist, pred_cls, anchors_3d, stacked_spheres, stacked_classes)\n",
    "\n",
    "# ========== 4. 손실 출력 ==========\n",
    "for key, value in loss_dict.items():\n",
    "    print(f\"{key}: {value.item():.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_22604\\523503606.py:144: UserWarning: `len(strides) > len(channels) - 1`, the last 1 values of strides will not be used.\n",
      "  warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SegDetect_Unet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    channels=feature_size,\n",
    "    strides=[2, 2, 2, 1],\n",
    "    kernel_size=3,\n",
    "    up_kernel_size=3,\n",
    "    act=Act.PRELU,\n",
    "    norm=Norm.INSTANCE,\n",
    "    dropout=dropout,\n",
    "    use_det=True,\n",
    ").to(device)\n",
    "seg_cirteion = CombinedCETverskyLoss(\n",
    "    lamda=lamda,\n",
    "    ce_weight=ce_weight,\n",
    "    n_classes=n_classes,\n",
    "    class_weights=class_weights,\n",
    ").to(device)\n",
    "det_criterion = YOLOv8SphereLoss3D_Advanced(\n",
    "    num_classes=n_classes, \n",
    "    reg_max=reg_max, \n",
    "    iou_weight = iou_weight,\n",
    "    cls_weight = cls_weight,\n",
    "    dfl_weight = dfl_weight,\n",
    "    device=device)\n",
    "pretrain_str = \"yes\" if use_checkpoint else \"no\"\n",
    "weight_str = \"weighted\" if class_weights is not None else \"\"\n",
    "\n",
    "# 모델 이름 설정\n",
    "model_name = model.__class__.__name__\n",
    "\n",
    "# 체크포인트 디렉토리 및 파일 설정\n",
    "checkpoint_base_dir = Path(\"./model_checkpoints\")\n",
    "folder_name = f\"{model_name}_denoiiso_maxf{feature_size[-1]}_{img_depth}x{img_size}x{img_size}_e{num_epochs}_lr{lr}\"\n",
    "checkpoint_dir = checkpoint_base_dir / folder_name\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "# 체크포인트 디렉토리 생성\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    best_model_path = checkpoint_dir / 'best_model.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"기존 best model 발견: {best_model_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(best_model_path, map_location=device)\n",
    "            # 체크포인트 내부 키 검증\n",
    "            required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch', 'best_val_loss']\n",
    "            if all(k in checkpoint for k in required_keys):\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_val_loss = checkpoint['best_val_loss']\n",
    "                print(\"기존 학습된 가중치를 성공적으로 로드했습니다.\")\n",
    "                checkpoint = None\n",
    "            else:\n",
    "                raise ValueError(\"체크포인트 파일에 필요한 key가 없습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"체크포인트 파일을 로드하는 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwoow070840\u001b[0m (\u001b[33mwaooang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Workspace\\czll\\wandb\\run-20250201_071218-swjy8j00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/swjy8j00' target=\"_blank\">Detect_Unet_denoiiso_maxf256_32x64x64_e4000_lr0.001</a></strong> to <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/swjy8j00' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR/runs/swjy8j00</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = folder_name\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project='czii_SwinUnetR',  # 프로젝트 이름 설정\n",
    "    name=run_name,         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': lamda,\n",
    "        \"cross_entropy_weight\": ce_weight,\n",
    "        'feature_size': feature_size,\n",
    "        'img_size': img_size,\n",
    "        'sampling_ratio': ratios_list,\n",
    "        'device': device.type,\n",
    "        \"checkpoint_dir\": str(checkpoint_dir),\n",
    "        \"class_weights\": class_weights.tolist() if class_weights is not None else None,\n",
    "        \"use_checkpoint\": use_checkpoint,\n",
    "        \"dropout\": dropout,        \n",
    "        \"accumulation_steps\": accumulation_steps,\n",
    "        \"num_repeat\": num_repeat,\n",
    "        \n",
    "        # 필요한 하이퍼파라미터 추가\n",
    "    }\n",
    ")\n",
    "# 모델을 wandb에 연결\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def decode_predictions(pred_dist, pred_cls, pred_conf, reg_max=16, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    YOLO 모델의 예측 결과를 Detection 라벨로 변환.\n",
    "\n",
    "    Args:\n",
    "        pred_dist (Tensor): (B, A, 4 * reg_max) → Distance 분포\n",
    "        pred_cls (Tensor): (B, A, num_classes) → 클래스 점수\n",
    "        pred_conf (Tensor): (B, A, 1) → Confidence Score\n",
    "        reg_max (int): 회귀 최대 거리 값\n",
    "        conf_threshold (float): Confidence Threshold (기본값: 0.5)\n",
    "\n",
    "    Returns:\n",
    "        List[Tensor]: 각 배치별 Detection 결과 (N, 4) → (xc, yc, zc, r)\n",
    "    \"\"\"\n",
    "    B, A, num_classes = pred_cls.shape\n",
    "    decoded_spheres = []\n",
    "\n",
    "    # Confidence Filtering\n",
    "    conf_mask = pred_conf.squeeze(-1) > conf_threshold  \n",
    "\n",
    "    for b in range(B):\n",
    "        if conf_mask[b].sum() == 0:\n",
    "            decoded_spheres.append(torch.empty((0, 4)))  \n",
    "            continue\n",
    "\n",
    "        valid_anchors = conf_mask[b].nonzero(as_tuple=True)[0]  \n",
    "\n",
    "        # Distance 회귀 값 변환 (Softmax)\n",
    "        distances = F.softmax(pred_dist[b, valid_anchors].reshape(-1, 4, reg_max), dim=-1)\n",
    "        distances = torch.sum(distances * torch.arange(reg_max, device=distances.device).float(), dim=-1)\n",
    "\n",
    "        # 중심 좌표 및 반지름 계산\n",
    "        xc, yc, zc = valid_anchors // (A // 3) % 3  \n",
    "        r = distances[:, -1]  \n",
    "\n",
    "        # 정규화 (0~1)\n",
    "        xc /= pred_cls.shape[1]\n",
    "        yc /= pred_cls.shape[1]\n",
    "        zc /= pred_cls.shape[1]\n",
    "        r /= reg_max\n",
    "\n",
    "        decoded_spheres.append(torch.stack([xc, yc, zc, r], dim=-1))  \n",
    "\n",
    "    return decoded_spheres\n",
    "\n",
    "def compute_fbeta_iou(pred_spheres, gt_spheres, beta=2, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    F-beta Score 및 mIoU 계산 (Sphere IoU 기반).\n",
    "\n",
    "    Args:\n",
    "        pred_spheres (Tensor): (N, 4) 예측된 Detection 결과 (xc, yc, zc, r)\n",
    "        gt_spheres (Tensor): (M, 4) Ground Truth Detection 결과\n",
    "        beta (float): F-beta Score의 beta 값\n",
    "        iou_threshold (float): IoU Threshold (기본값: 0.5)\n",
    "\n",
    "    Returns:\n",
    "        fbeta_score (float): F-beta Score\n",
    "        mean_iou (float): Mean IoU (mIoU)\n",
    "    \"\"\"\n",
    "    if pred_spheres.shape[0] == 0 or gt_spheres.shape[0] == 0:\n",
    "        return 0.0, 0.0  \n",
    "\n",
    "    iou_matrix = sphere_iou(pred_spheres, gt_spheres)  \n",
    "\n",
    "    matched_gt = iou_matrix.max(dim=1)[0] > iou_threshold\n",
    "    TP = matched_gt.sum().item()\n",
    "    FP = pred_spheres.shape[0] - TP\n",
    "    FN = gt_spheres.shape[0] - TP\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    fbeta_score = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + 1e-8)\n",
    "\n",
    "    valid_iou = iou_matrix.max(dim=1)[0][matched_gt].mean().item() if TP > 0 else 0.0\n",
    "\n",
    "    return fbeta_score, valid_iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def processing(batch_data, model, seg_criterion, det_criterion, device, det_weight=0.5,reg_max=16, stride=8):\n",
    "    \"\"\"\n",
    "    YOLO 스타일 3D Object Detection을 위한 processing 함수.\n",
    "\n",
    "    Args:\n",
    "        batch_data: \n",
    "            {\n",
    "                'image': (B, C, D, H, W),\n",
    "                'label': (B, 1, D, H, W) → Ground Truth Segmentation 라벨\n",
    "            }\n",
    "        model: YOLO Detection 모델\n",
    "        criterion: YOLOv8SphereLoss3D_Advanced\n",
    "        device: GPU/CPU\n",
    "        reg_max: Distance 회귀 파라미터\n",
    "        stride: 앵커 생성 시 기준 stride\n",
    "\n",
    "    Returns:\n",
    "        total_loss, (pred_dist, pred_cls, pred_conf)\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1) 배치에서 이미지 및 GT 라벨 가져오기\n",
    "    # -----------------------------------------------------------\n",
    "    images = batch_data['image'].to(device)  # (B, C, D, H, W)\n",
    "    labels = batch_data['label'].to(device)  # (B, 1, D, H, W)\n",
    "    \n",
    "    B, _, D, H, W = images.shape\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2) YOLO 모델 추론\n",
    "    # -----------------------------------------------------------\n",
    "    x_seg,x_det = model(images)  # (B, 1 + num_classes + 4*reg_max, D, H, W)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3) 예측값 분리\n",
    "    # -----------------------------------------------------------\n",
    "    num_classes = det_criterion.num_classes  # 클래스 개수\n",
    "\n",
    "    # Confidence Score (B, 1, D, H, W)\n",
    "    pred_conf = x_det[:, :1, ...]  \n",
    "\n",
    "    # Class Probabilities (B, num_classes, D, H, W)\n",
    "    pred_cls = x_det[:, 1:num_classes + 1, ...]\n",
    "\n",
    "    # Distance Predictions (B, 4 * reg_max, D, H, W) → (cx, cy, cz, r)\n",
    "    pred_dist = x_det[:, num_classes + 1:, ...]\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4) YOLO 스타일로 Reshape\n",
    "    # -----------------------------------------------------------\n",
    "    A = (D // stride) * (H // stride) * (W // stride)  # 총 앵커 개수\n",
    "\n",
    "    # Shape 변환 (YOLO 스타일)\n",
    "    pred_conf = pred_conf.permute(0, 2, 3, 4, 1).reshape(B, -1, 1)  # (B, A, 1)\n",
    "    pred_cls = pred_cls.permute(0, 2, 3, 4, 1).reshape(B, -1, num_classes)  # (B, A, num_classes)\n",
    "    pred_dist = pred_dist.permute(0, 2, 3, 4, 1).reshape(B, -1, 4 * reg_max)  # (B, A, 4 * reg_max)\n",
    "\n",
    "    # Debugging 출력 추가\n",
    "    print(f\"[DEBUG] pred_conf.shape: {pred_conf.shape}\")  # 확인용\n",
    "    print(f\"[DEBUG] pred_cls.shape: {pred_cls.shape}\")\n",
    "    print(f\"[DEBUG] pred_dist.shape: {pred_dist.shape}\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 5) 3D 앵커 생성\n",
    "    # -----------------------------------------------------------\n",
    "    feature_shape = (D // stride, H // stride, W // stride)\n",
    "    anchors_3d = generate_anchors_3d(feature_shape, stride).to(device)  # (A, 3)\n",
    "    anchors_3d = anchors_3d.unsqueeze(0).expand(B, -1, -1)  # (B, A, 3)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 6) GT 변환 (Segmentation → Bounding Box)\n",
    "    # -----------------------------------------------------------\n",
    "    gt_spheres, gt_classes = extract_and_convert_3d_bounding_boxes(labels, min_size=10, normalize=False)\n",
    "\n",
    "    def pad_gt(gt_spheres, gt_classes, max_g):\n",
    "        \"\"\"배치 내 최대 GT 개수를 맞추기 위해 padding 적용\"\"\"\n",
    "        s_list, c_list = [], []\n",
    "        for b in range(B):\n",
    "            s_ = gt_spheres[b].to(device)\n",
    "            c_ = gt_classes[b].to(device)\n",
    "            pad_s = torch.zeros((max_g, 4), device=device)\n",
    "            pad_c = torch.zeros((max_g,), device=device)\n",
    "            if s_.shape[0] > 0:\n",
    "                pad_s[:s_.shape[0]] = s_\n",
    "                pad_c[:c_.shape[0]] = c_\n",
    "            s_list.append(pad_s)\n",
    "            c_list.append(pad_c)\n",
    "        return torch.stack(s_list, dim=0), torch.stack(c_list, dim=0)\n",
    "\n",
    "    # GT 패딩 적용\n",
    "    max_g = max(x.shape[0] for x in gt_spheres) if len(gt_spheres) else 0\n",
    "    stacked_spheres, stacked_classes = pad_gt(gt_spheres, gt_classes, max_g)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 7) YOLO Loss 계산\n",
    "    # -----------------------------------------------------------\n",
    "    loss_dict = det_criterion(\n",
    "        pred_dist=pred_dist,\n",
    "        pred_cls=pred_cls,\n",
    "        anchors_3d=anchors_3d,\n",
    "        gt_spheres=stacked_spheres,\n",
    "        gt_classes=stacked_classes\n",
    "    )\n",
    "    seg_loss = seg_criterion(x_seg, labels)\n",
    "\n",
    "    # Loss 값 가져오기\n",
    "    det_loss = loss_dict['loss_total']\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    total_loss = (1-det_weight)*seg_loss + det_weight*det_loss    \n",
    "\n",
    "    return total_loss, (pred_dist, pred_cls, pred_conf)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, seg_criterion, det_criterion, optimizer, device, epoch, det_weight,accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    한 epoch 동안 모델을 학습하는 함수 (Detection 라벨 기반).\n",
    "    \n",
    "    Args:\n",
    "        model: 학습할 YOLO 3D Detection 모델\n",
    "        train_loader: DataLoader (Detection 데이터셋 기반)\n",
    "        criterion: YOLOv8SphereLoss3D_Advanced 손실 함수\n",
    "        optimizer: 최적화 알고리즘\n",
    "        device: GPU/CPU 장치\n",
    "        epoch: 현재 학습 epoch\n",
    "        accumulation_steps: 그래디언트 누적 스텝 (default: 4)\n",
    "\n",
    "    Returns:\n",
    "        평균 학습 손실 (float)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()  # 그래디언트 초기화\n",
    "\n",
    "    with tqdm(train_loader, desc=f'Training Epoch {epoch+1}') as pbar:\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "            # 손실 계산 (processing 함수 활용)\n",
    "            total_loss, _ = processing(batch_data, model, seg_criterion, det_criterion, device)\n",
    "\n",
    "            # 그래디언트 누적을 위한 스케일링\n",
    "            total_loss = total_loss / accumulation_steps  \n",
    "            total_loss.backward()  # 그래디언트 계산 및 누적\n",
    "            \n",
    "            # 그래디언트 업데이트 (accumulation_steps마다 한 번)\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                optimizer.step()  # 파라미터 업데이트\n",
    "                optimizer.zero_grad()  # 누적된 그래디언트 초기화\n",
    "            \n",
    "            # 손실값 누적\n",
    "            epoch_loss += total_loss.item() * accumulation_steps  # 실제 손실값 반영\n",
    "            pbar.set_postfix(loss=total_loss.item() * accumulation_steps)  # 현재 손실 출력\n",
    "\n",
    "    # 에폭 평균 손실 계산\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # Wandb 로그 기록\n",
    "    wandb.log({'train_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def validate_one_epoch(model, val_loader, seg_criterion, det_criterion, device, epoch, calculate_dice_interval,det_weight,ce_weight):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    fbeta_scores = []\n",
    "    iou_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc=f'Validation Epoch {epoch+1}') as pbar:\n",
    "            for batch_data in pbar:\n",
    "                total_loss, (pred_dist, pred_cls) = processing(batch_data, model, seg_criterion, det_criterion, device)\n",
    "                val_loss += total_loss.item()\n",
    "                pbar.set_postfix(loss=total_loss.item())\n",
    "\n",
    "                # YOLO 예측을 Detection Box로 변환\n",
    "                pred_spheres = decode_predictions(pred_dist, pred_cls)\n",
    "                gt_spheres = batch_data['gt_spheres']  # List[Tensor]\n",
    "\n",
    "                # F-beta Score 및 mIoU 계산\n",
    "                for b in range(len(pred_spheres)):\n",
    "                    fbeta, miou = compute_fbeta_iou(pred_spheres[b], gt_spheres[b])\n",
    "                    fbeta_scores.append(fbeta)\n",
    "                    iou_scores.append(miou)\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_fbeta = np.mean(fbeta_scores)\n",
    "    avg_iou = np.mean(iou_scores)\n",
    "\n",
    "    # Wandb 로깅\n",
    "    wandb.log({'val_epoch_loss': avg_loss, 'epoch': epoch + 1, 'val_fbeta': avg_fbeta, 'val_miou': avg_iou})\n",
    "\n",
    "    print(f\"Validation F-beta: {avg_fbeta:.4f}, Validation mIoU: {avg_iou:.4f}\")\n",
    "    \n",
    "    final_score = avg_fbeta * (1 - ce_weight) + avg_iou * ce_weight\n",
    "    return avg_loss, final_score\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, \n",
    "    device, start_epoch, best_val_loss, best_val_fbeta_score, calculate_dice_interval=1,\n",
    "    det_weight=0.5,\n",
    "    accumulation_steps=4, pretrained=False\n",
    "):\n",
    "    \"\"\"\n",
    "    모델을 학습하고 검증하는 함수\n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        train_loader: 학습 데이터 로더\n",
    "        val_loader: 검증 데이터 로더\n",
    "        criterion: 손실 함수\n",
    "        optimizer: 최적화 알고리즘\n",
    "        num_epochs: 총 학습 epoch 수\n",
    "        patience: early stopping 기준\n",
    "        device: GPU/CPU 장치\n",
    "        start_epoch: 시작 epoch\n",
    "        best_val_loss: 이전 최적 validation loss\n",
    "        best_val_fbeta_score: 이전 최적 validation f-beta score\n",
    "        calculate_dice_interval: Dice 점수 계산 주기\n",
    "    \"\"\"\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train One Epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            criterion=criterion, \n",
    "            optimizer=optimizer, \n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            det_weight=det_weight,\n",
    "            accumulation_steps= accumulation_steps\n",
    "        )\n",
    "        \n",
    "        scheduler.step(train_loss)\n",
    "        # Validate One Epoch\n",
    "        val_loss, overall_mean_fbeta_score = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=epoch, \n",
    "            calculate_dice_interval=calculate_dice_interval,\n",
    "            det_weight=det_weight,\n",
    "            ce_weight=ce_weight\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F-beta: {overall_mean_fbeta_score:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss and overall_mean_fbeta_score > best_val_fbeta_score:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_fbeta_score = overall_mean_fbeta_score\n",
    "            epochs_no_improve = 0\n",
    "            if pretrained:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model_pretrained.pt')\n",
    "            else:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            print(f\"========================================================\")\n",
    "            print(f\"SUPER Best model saved. Loss:{best_val_loss:.4f}, Score:{best_val_fbeta_score:.4f}\")\n",
    "            print(f\"========================================================\")\n",
    "\n",
    "        # Early stopping 조건 체크\n",
    "        if val_loss >= best_val_loss and overall_mean_fbeta_score <= best_val_fbeta_score:\n",
    "            epochs_no_improve += 1\n",
    "        else:\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'last.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            break\n",
    "        # if epochs_no_improve % 6 == 0 & epochs_no_improve != 0:\n",
    "        #     # 손실이 개선되지 않았으므로 lambda 감소\n",
    "        #     new_lamda = max(criterion.lamda - 0.01, 0.35)  # 최소값은 0.1로 설정\n",
    "        #     criterion.set_lamda(new_lamda)\n",
    "        #     print(f\"Validation loss did not improve. Reducing lambda to {new_lamda:.4f}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/204 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_conf.shape: torch.Size([32, 1, 32, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 256, 1]' is invalid for input of size 4194304",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_val_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_val_fbeta_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_fbeta_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalculate_dice_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 214\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device, start_epoch, best_val_loss, best_val_fbeta_score, calculate_dice_interval, accumulation_steps, pretrained)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Train One Epoch\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Validate One Epoch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 129\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device, epoch, accumulation_steps)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;66;03m# 손실 계산 (processing 함수 활용)\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m         total_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;66;03m# 그래디언트 누적을 위한 스케일링\u001b[39;00m\n\u001b[0;32m    132\u001b[0m         total_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m accumulation_steps  \n",
      "Cell \u001b[1;32mIn[32], line 54\u001b[0m, in \u001b[0;36mprocessing\u001b[1;34m(batch_data, model, criterion, device, reg_max, stride)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 4) YOLO 스타일로 Reshape\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     52\u001b[0m A \u001b[38;5;241m=\u001b[39m (D \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m stride) \u001b[38;5;241m*\u001b[39m (H \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m stride) \u001b[38;5;241m*\u001b[39m (W \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m stride)  \u001b[38;5;66;03m# 총 앵커 개수\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m pred_conf \u001b[38;5;241m=\u001b[39m \u001b[43mpred_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, A, 1)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m pred_cls \u001b[38;5;241m=\u001b[39m pred_cls\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, A, num_classes)  \u001b[38;5;66;03m# (B, A, num_classes)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m pred_dist \u001b[38;5;241m=\u001b[39m pred_dist\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, A, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m reg_max)  \u001b[38;5;66;03m# (B, A, 4 * reg_max)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\monai\\data\\meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\torch\\_tensor.py:1512\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m-> 1512\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[0;32m   1514\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[32, 256, 1]' is invalid for input of size 4194304"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    seg_cirteion=seg_cirteion,\n",
    "    det_criterion=det_criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    start_epoch=start_epoch,\n",
    "    best_val_loss=best_val_loss,\n",
    "    best_val_fbeta_score=best_val_fbeta_score,\n",
    "    calculate_dice_interval=1,\n",
    "    det_weight = det_weight,\n",
    "    accumulation_steps = accumulation_steps\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "czii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
