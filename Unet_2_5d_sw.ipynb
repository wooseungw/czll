{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from monai.data import Dataset\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, CropForegroundd, GaussianSmoothd, ScaleIntensityd,\n",
    "    RandSpatialCropd, RandRotate90d, RandFlipd, RandGaussianNoised,\n",
    "    ToTensord\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_DIR = \"./datasets/train/images\"\n",
    "TRAIN_LABEL_DIR = \"./datasets/train/labels\"\n",
    "VAL_IMG_DIR = \"./datasets/val/images\"\n",
    "VAL_LABEL_DIR = \"./datasets/val/labels\"\n",
    "\n",
    "train_list = os.listdir(TRAIN_IMG_DIR)\n",
    "val_list = os.listdir(VAL_IMG_DIR)\n",
    "train_files = []\n",
    "valid_files = []\n",
    "\n",
    "\n",
    "for name in train_list:\n",
    "    train_image = np.load(os.path.join(TRAIN_IMG_DIR, f\"{name}\"))    \n",
    "    train_label = np.load(os.path.join(TRAIN_LABEL_DIR, f\"{name.replace(\"image\", \"label\")}\"))\n",
    "\n",
    "    train_files.append({\"image\": train_image, \"label\": train_label})    \n",
    "\n",
    "for name in val_list:\n",
    "    valid_image = np.load(os.path.join(VAL_IMG_DIR, f\"{name}\"))\n",
    "    valid_label = np.load(os.path.join(VAL_LABEL_DIR, f\"{name.replace(\"image\", \"label\")}\"))\n",
    "\n",
    "    valid_files.append({\"image\": valid_image, \"label\": valid_label})\n",
    "\n",
    "\n",
    "class CryoETDataset(Dataset):\n",
    "    def __init__(self, data, transforms, slice_thickness=5, voxel_size=(10, 10, 10), origin=(0, 0, 0)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: 리스트 형태의 데이터셋 ({\"image\": np.ndarray, \"label\": np.ndarray} 형태).\n",
    "            transforms: MONAI 변환 객체.\n",
    "            slice_thickness: 슬라이스 두께.\n",
    "            voxel_size: 각 축의 voxel 크기 (z, x, y).\n",
    "            origin: 데이터 원점 (z, x, y).\n",
    "        \"\"\"\n",
    "        super().__init__(data, transforms)\n",
    "        self.slice_thickness = slice_thickness\n",
    "        self.half_thickness = slice_thickness // 2\n",
    "        self.voxel_size = voxel_size\n",
    "        self.origin = origin\n",
    "        self.slices = []  # 전체 슬라이스 저장\n",
    "\n",
    "        # 모든 슬라이스를 미리 생성\n",
    "        for data_dict in data:\n",
    "            image = data_dict[\"image\"]\n",
    "            label = data_dict.get(\"label\", None)\n",
    "\n",
    "            # Z 축에 패딩 추가\n",
    "            image = np.pad(image, ((self.half_thickness, self.half_thickness), (0, 0), (0, 0)), mode=\"constant\", constant_values=0)\n",
    "            if label is not None:\n",
    "                label = np.pad(label, ((self.half_thickness, self.half_thickness), (0, 0), (0, 0)), mode=\"constant\", constant_values=0)\n",
    "\n",
    "            depth = image.shape[0]\n",
    "\n",
    "            for center_idx in range(self.half_thickness, depth - self.half_thickness):\n",
    "                self.slices.append({\n",
    "                    \"image\": image[center_idx - self.half_thickness:center_idx + self.half_thickness + 1],\n",
    "                    \"label\": label[center_idx] if label is not None else None,\n",
    "                    \"original_index\": center_idx - self.half_thickness,  # 패딩 이전의 원래 인덱스\n",
    "                    \"real_position\": self.compute_voxel_position(center_idx - self.half_thickness, 0, 0),\n",
    "                })\n",
    "\n",
    "    def compute_voxel_position(self, z, x, y):\n",
    "        dz, dx, dy = self.voxel_size\n",
    "        oz, ox, oy = self.origin\n",
    "        z_real = z * dz + oz\n",
    "        x_real = x * dx + ox\n",
    "        y_real = y * dy + oy\n",
    "        return z_real, x_real, y_real\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice_data = self.slices[index]\n",
    "        return {\n",
    "            \"image\": slice_data[\"image\"],  # 슬라이스 (slice_thickness, H, W)\n",
    "            \"label\": slice_data[\"label\"],  # 중앙 라벨 (H, W) 또는 None\n",
    "            \"original_index\": slice_data[\"original_index\"],  # 중앙 슬라이스 인덱스\n",
    "            \"real_position\": slice_data[\"real_position\"],  # 실제 좌표\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 확인\n",
    "def inspect_batch(loader):\n",
    "    batch = next(iter(loader))\n",
    "    print(\"=== 배치 데이터 확인 ===\")\n",
    "    print(f\"Batch image shape: {batch['image'].shape}\")\n",
    "    print(f\"Batch label shape: {batch['label'].shape if batch['label'] is not None else 'None'}\")\n",
    "    print(f\"Image dtype: {batch['image'].dtype}\")\n",
    "    if batch[\"label\"] is not None:\n",
    "        print(f\"Label dtype: {batch['label'].dtype}\")\n",
    "        print(f\"Label unique values: {torch.unique(batch['label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184, 630, 630)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image shape: torch.Size([1, 5, 630, 630])\n",
      "Batch label shape: torch.Size([1, 630, 630])\n",
      "Batch original indices: tensor([113])\n",
      "Batch real positions: [tensor([1130]), tensor([0]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "train_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"SRA\")\n",
    "])\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = CryoETDataset(data=train_files, transforms=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "        \n",
    "for batch in train_loader:\n",
    "    print(f\"Batch image shape: {batch['image'].shape}\")\n",
    "    print(f\"Batch label shape: {batch['label'].shape if batch['label'] is not None else 'None'}\")\n",
    "    print(f\"Batch original indices: {batch['original_index']}\")\n",
    "    print(f\"Batch real positions: {batch['real_position']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 26496/26496 [00:07<00:00, 3439.67it/s]\n"
     ]
    }
   ],
   "source": [
    "val_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    ScaleIntensityd(keys=\"image\", minv=0.0, maxv=1.0),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "val_dataset = CryoETDataset(data=valid_files, transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    ScaleIntensityd(keys=\"image\", minv=0.0, maxv=1.0),\n",
    "    ToTensord(keys=[\"image\"]),\n",
    "])\n",
    "# no_label_dataset = CryoETDataset(data=no_label_data, transforms=no_label_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwoo/anaconda3/envs/dust/lib/python3.12/site-packages/monai/networks/nets/unet.py:130: UserWarning: `len(strides) > len(channels) - 1`, the last 1 values of strides will not be used.\n",
      "  warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "class UNet2_5D_v2(nn.Module):\n",
    "    def __init__(self, init_ch=3 ,out_channels=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 초기 3D 처리 레이어\n",
    "        self.init_3d = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=(init_ch, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 2D UNet\n",
    "        self.unet = UNet(\n",
    "            spatial_dims=2,\n",
    "            in_channels=64,  # 3D 컨볼루션 출력 채널\n",
    "            out_channels=out_channels,\n",
    "            channels=(128, 256, 512, 1024),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 1, 11, H, W)\n",
    "        # 3D 처리\n",
    "        \n",
    "        x = self.init_3d(x)  # (batch, 64, 1, H, W)\n",
    "        x = x.squeeze(2)     # (batch, 64, H, W)\n",
    "        \n",
    "        # 2D UNet\n",
    "        return self.unet(x)\n",
    "\n",
    "# 테스트 코드\n",
    "\n",
    "model = UNet2_5D_v2(init_ch=3,out_channels=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 1, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 1, 3, 96, 96)\n",
    "print(f\"Input Shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 7, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "output = model(x)\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: (8, 6, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Targets shape: torch.Size([1, 1, 3, 96, 96])\n",
      "Outputs shape: torch.Size([1, 7, 96, 96]), Targets shape: torch.Size([1, 1, 3, 96, 96])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ground truth has different shape (torch.Size([1, 7, 3, 96, 96])) from input (torch.Size([1, 7, 96, 96]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, H, W)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Dice Loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dust/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dust/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dust/lib/python3.12/site-packages/monai/losses/dice.py:169\u001b[0m, in \u001b[0;36mDiceLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground truth has different shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) from input (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# reducing only spatial dimensions (not batch nor channels)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m reduce_axis: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape))\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mAssertionError\u001b[0m: ground truth has different shape (torch.Size([1, 7, 3, 96, 96])) from input (torch.Size([1, 7, 96, 96]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from monai.losses import DiceLoss\n",
    "from torch import optim\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = DiceLoss(to_onehot_y=True, softmax=True)  # Dice Loss with softmax\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        # Ensure targets are long integers\n",
    "        targets = batch['label'].long()  # 라벨: (B, H, W)\n",
    "\n",
    "        \n",
    "        print(f\"Modified Targets shape: {targets.shape}\")  # (B, 1, H, W)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch['image'])  # 모델 출력: (B, 7, H, W)\n",
    "        print(f\"Outputs shape: {outputs.shape}, Targets shape: {targets.shape}\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)  # Dice Loss\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
