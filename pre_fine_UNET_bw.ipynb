{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.4.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: c:\\Users\\<username>\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.14.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.18.0\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.19.1+cu121\n",
      "tqdm version: 4.66.5\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.17.2\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.models import UNet_CBAM\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    ")\n",
    "\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "# from src.models.swincspunetr import SwinCSPUNETR\n",
    "# from src.models.swincspunetr_unet import SwinCSPUNETR_unet\n",
    "# from src.models.swincspunetr3plus import SwinCSPUNETR3plus\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 비율: {0: 0.0, 1: 0.16393442622950818, 2: 0.01639344262295082, 3: 0.2459016393442623, 4: 0.16393442622950818, 5: 0.2459016393442623, 6: 0.16393442622950818}\n",
      "최종 합계: 1.0\n",
      "클래스 비율 리스트: [0.0, 0.16393442622950818, 0.01639344262295082, 0.2459016393442623, 0.16393442622950818, 0.2459016393442623, 0.16393442622950818]\n"
     ]
    }
   ],
   "source": [
    "class_info = {\n",
    "    0: {\"name\": \"background\", \"weight\": 0},  # weight 없음\n",
    "    1: {\"name\": \"apo-ferritin\", \"weight\": 1000},\n",
    "    2: {\"name\": \"beta-amylase\", \"weight\": 100}, # 4130\n",
    "    3: {\"name\": \"beta-galactosidase\", \"weight\": 1500}, #3080\n",
    "    4: {\"name\": \"ribosome\", \"weight\": 1000},\n",
    "    5: {\"name\": \"thyroglobulin\", \"weight\": 1500},\n",
    "    6: {\"name\": \"virus-like-particle\", \"weight\": 1000},\n",
    "}\n",
    "\n",
    "# 가중치에 비례한 비율 계산\n",
    "raw_ratios = {\n",
    "    k: (v[\"weight\"] if v[\"weight\"] is not None else 0.01)  # 가중치 비례, None일 경우 기본값a\n",
    "    for k, v in class_info.items()\n",
    "}\n",
    "total = sum(raw_ratios.values())\n",
    "ratios = {k: v / total for k, v in raw_ratios.items()}\n",
    "\n",
    "# 최종 합계가 1인지 확인\n",
    "final_total = sum(ratios.values())\n",
    "print(\"클래스 비율:\", ratios)\n",
    "print(\"최종 합계:\", final_total)\n",
    "\n",
    "# 비율을 리스트로 변환\n",
    "ratios_list = [ratios[k] for k in sorted(ratios.keys())]\n",
    "print(\"클래스 비율 리스트:\", ratios_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.dataset import create_dataloaders, create_dataloaders_bw\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, CropForegroundd, GaussianSmoothd, ScaleIntensityd,\n",
    "    RandSpatialCropd, RandRotate90d, RandFlipd, RandGaussianNoised,\n",
    "    ToTensord, RandCropByLabelClassesd, RandCropd,RandCropByPosNegLabeld, RandGaussianSmoothd\n",
    ")\n",
    "from monai.transforms import CastToTyped\n",
    "import numpy as np\n",
    "\n",
    "train_img_dir = \"./datasets/pretrain_exdata/images\"\n",
    "train_label_dir = \"./datasets/pretrain_exdata/labels\"\n",
    "val_img_dir = \"./datasets/val/images\"\n",
    "val_label_dir = \"./datasets/val/labels\"\n",
    "# DATA CONFIG\n",
    "img_size =  96 # Match your patch size\n",
    "img_depth = 32\n",
    "n_classes = 7\n",
    "batch_size = 16 # 13.8GB GPU memory required for 128x128 img size\n",
    "loader_batch = 1\n",
    "num_samples = batch_size // loader_batch # 한 이미지에서 뽑을 샘플 수\n",
    "num_repeat = 4\n",
    "# MODEL CONFIG\n",
    "num_epochs = 4000\n",
    "lamda = 0.52\n",
    "ce_weight = 0.4\n",
    "lr = 0.001\n",
    "feature_size = 48\n",
    "use_checkpoint = True\n",
    "use_v2 = True\n",
    "drop_rate= 0.25\n",
    "attn_drop_rate = 0.25\n",
    "num_bottleneck = 2\n",
    "# CLASS_WEIGHTS\n",
    "class_weights = None\n",
    "# class_weights = torch.tensor([0.0001, 1, 0.001, 1.1, 1, 1.1, 1], dtype=torch.float32)  # 클래스별 가중치\n",
    "# class_weights = torch.tensor([0.9,1,0.9,1.1,1,1.1,1], dtype=torch.float32)  # 클래스별 가중치\n",
    "# class_weights = torch.tensor([1,1,1,1,1,1,1], dtype=torch.float32)  # 클래스별 가중치\n",
    "sigma = 2.0\n",
    "accumulation_steps = 1\n",
    "# INIT\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "best_val_fbeta_score = 0\n",
    "\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    # GaussianSmoothd(\n",
    "    #     keys=[\"image\"],      # 변환을 적용할 키\n",
    "    #     sigma=[sigma, sigma, sigma]  # 각 축(x, y, z)의 시그마 값\n",
    "    #     ),\n",
    "])\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    RandGaussianSmoothd(\n",
    "    keys=[\"image\"],      # 변환을 적용할 키\n",
    "    sigma_x = (0.0, sigma), # 각 축(x, y, z)의 시그마 값\n",
    "    sigma_y = (0.0, sigma),\n",
    "    sigma_z = (0.0, sigma),\n",
    "    prob=1.0,\n",
    "    ),\n",
    "])\n",
    "val_random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    # RandGaussianSmoothd(\n",
    "    # keys=[\"image\"],      # 변환을 적용할 키\n",
    "    # sigma_x = (0.0, sigma), # 각 축(x, y, z)의 시그마 값\n",
    "    # sigma_y = (0.0, sigma),\n",
    "    # sigma_z = (0.0, sigma),\n",
    "    # prob=1.0,\n",
    "    # ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 51/51 [00:04<00:00, 10.37it/s]\n",
      "Loading dataset: 100%|██████████| 1/1 [00:00<00:00,  6.22it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = None, None\n",
    "train_loader, val_loader = create_dataloaders_bw(\n",
    "    train_img_dir, \n",
    "    train_label_dir, \n",
    "    val_img_dir, \n",
    "    val_label_dir, \n",
    "    non_random_transforms = non_random_transforms, \n",
    "    val_non_random_transforms=non_random_transforms,\n",
    "    random_transforms = random_transforms, \n",
    "    val_random_transforms=val_random_transforms,\n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0,\n",
    "    train_num_repeat=num_repeat,\n",
    "    val_num_repeat=num_repeat,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://monai.io/model-zoo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import TverskyLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DynamicTverskyLoss 클래스 정의\n",
    "class DynamicTverskyLoss(TverskyLoss):\n",
    "    def __init__(self, lamda=0.5, **kwargs):\n",
    "        super().__init__(alpha=1 - lamda, beta=lamda, **kwargs)\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.lamda = lamda\n",
    "        self.alpha = 1 - lamda\n",
    "        self.beta = lamda\n",
    "\n",
    "\n",
    "# CombinedCETverskyLoss 클래스\n",
    "class CombinedCETverskyLoss_cl_weight(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, lamda=0.5, ce_weight=0.5, n_classes=7, class_weights=None, ignore_index=-1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "        # CrossEntropyLoss에서 클래스별 가중치를 적용\n",
    "        self.ce = nn.CrossEntropyLoss(weight=class_weights, ignore_index=self.ignore_index, reduction='mean', **kwargs)\n",
    "        \n",
    "        # TverskyLoss\n",
    "        self.tversky = DynamicTverskyLoss(lamda=lamda, reduction=\"none\",softmax=True, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        # CrossEntropyLoss는 정수형 클래스 인덱스를 사용\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "\n",
    "        # TverskyLoss 계산 (원핫 인코딩된 라벨을 사용)\n",
    "        \n",
    "        tversky_loss = self.tversky(inputs, targets)\n",
    "\n",
    "        # 클래스별 가중치 적용 (Tversky 손실에도 가중치를 곱하기)\n",
    "        class_weights = torch.tensor(self.ce.weight)  # CrossEntropy의 weight를 사용\n",
    "\n",
    "        # Tversky 손실이 (B, num_classes) 형태이므로, 가중치를 클래스 차원에 곱합니다.\n",
    "        tversky_loss = tversky_loss * class_weights.view(1, self.n_classes)\n",
    "\n",
    "        # 최종 손실 계산\n",
    "        final_loss = self.ce_weight * ce_loss + (1 - self.ce_weight) * tversky_loss.mean()  # mean()으로 배치에 대해 평균\n",
    "        return final_loss\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.tversky.set_lamda(lamda)\n",
    "\n",
    "    @property\n",
    "    def lamda(self):\n",
    "        return self.tversky.lamda\n",
    "\n",
    "# CombinedCETverskyLoss 클래스\n",
    "class CombinedCETverskyLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda=0.5, ce_weight=0.5, n_classes=7, class_weights=None, ignore_index=-1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "        # CrossEntropyLoss에서 클래스별 가중치를 적용\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=self.ignore_index, reduction='mean', **kwargs)\n",
    "        \n",
    "        # TverskyLoss\n",
    "        self.tversky = DynamicTverskyLoss(lamda=lamda, reduction=\"mean\",softmax=True, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        # CrossEntropyLoss는 정수형 클래스 인덱스를 사용\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "\n",
    "        # TverskyLoss 계산 (원핫 인코딩된 라벨을 사용)\n",
    "        \n",
    "        tversky_loss = self.tversky(inputs, targets)\n",
    "\n",
    "        # 최종 손실 계산\n",
    "        final_loss = self.ce_weight * ce_loss + (1 - self.ce_weight) * tversky_loss  # mean()으로 배치에 대해 평균\n",
    "        \n",
    "        return final_loss\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.tversky.set_lamda(lamda)\n",
    "\n",
    "    @property\n",
    "    def lamda(self):\n",
    "        return self.tversky.lamda\n",
    "\n",
    "criterion = CombinedCETverskyLoss(\n",
    "    lamda=lamda,\n",
    "    ce_weight=ce_weight,\n",
    "    n_classes=n_classes,\n",
    "    class_weights=class_weights,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from monai.networks.nets import UNet\n",
    "from src.models import DP_UNet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    channels=(32, 64, 128, 256),\n",
    "    strides=(2, 2, 2),\n",
    "    dropout = drop_rate,\n",
    "    norm = Norm.INSTANCE,\n",
    "    act = Act.PRELU,\n",
    ").to(device)\n",
    "\n",
    "pretrain_str = \"yes\" if use_checkpoint else \"no\"\n",
    "weight_str = \"weighted\" if class_weights is not None else \"\"\n",
    "\n",
    "# 체크포인트 디렉토리 및 파일 설정\n",
    "checkpoint_base_dir = Path(\"./model_checkpoints\")\n",
    "folder_name = f\"UNET_randGaus_511_241_noclswt_prelu_instance_f{feature_size}_lr{lr:.0e}_a{lamda:.2f}_b{batch_size}_r{num_repeat}_ce{ce_weight}_ac{accumulation_steps}\"\n",
    "checkpoint_dir = checkpoint_base_dir / folder_name\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "# 체크포인트 디렉토리 생성\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    best_model_path = checkpoint_dir / 'best_model_pretrained.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"기존 best model 발견: {best_model_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(best_model_path, map_location=device)\n",
    "            # 체크포인트 내부 키 검증\n",
    "            required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch', 'best_val_loss', 'best_val_fbeta_score']\n",
    "            if all(k in checkpoint for k in required_keys):\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_val_loss = checkpoint['best_val_loss']\n",
    "                best_val_fbeta_score = checkpoint['best_val_fbeta_score']\n",
    "                print(\"기존 학습된 가중치를 성공적으로 로드했습니다.\")\n",
    "                checkpoint= None\n",
    "            else:\n",
    "                raise ValueError(\"체크포인트 파일에 필요한 key가 없습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"체크포인트 파일을 로드하는 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 96, 96, 96]) torch.Size([16, 1, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpook0612\u001b[0m (\u001b[33mlimbw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Workspace\\czll\\wandb\\run-20250118_215845-h46rbn24</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/limbw/czii_UNet/runs/h46rbn24' target=\"_blank\">UNET_randGaus_511_241_noclswt_prelu_instance_f48_lr1e-03_a0.52_b16_r4_ce0.4_ac1</a></strong> to <a href='https://wandb.ai/limbw/czii_UNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/limbw/czii_UNet' target=\"_blank\">https://wandb.ai/limbw/czii_UNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/limbw/czii_UNet/runs/h46rbn24' target=\"_blank\">https://wandb.ai/limbw/czii_UNet/runs/h46rbn24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = folder_name\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project='czii_UNet',  # 프로젝트 이름 설정\n",
    "    name=run_name,         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': lamda,\n",
    "        \"cross_entropy_weight\": ce_weight,\n",
    "        'feature_size': feature_size,\n",
    "        'img_size': img_size,\n",
    "        'sampling_ratio': ratios_list,\n",
    "        'device': device.type,\n",
    "        \"checkpoint_dir\": str(checkpoint_dir),\n",
    "        \"class_weights\": class_weights.tolist() if class_weights is not None else None,\n",
    "        # \"use_checkpoint\": use_checkpoint,\n",
    "        \"drop_rate\": drop_rate,\n",
    "        # \"attn_drop_rate\": attn_drop_rate,\n",
    "        # \"use_v2\": use_v2,\n",
    "        \"accumulation_steps\": accumulation_steps,\n",
    "        \"num_repeat\": num_repeat,\n",
    "        # \"num_bottleneck\": num_bottleneck,\n",
    "        \n",
    "        # 필요한 하이퍼파라미터 추가\n",
    "    }\n",
    ")\n",
    "# 모델을 wandb에 연결\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric\n",
    "    \n",
    "def processing(batch_data, model, criterion, device):\n",
    "    images = batch_data['image'].to(device)  # Input 이미지 (B, 1, 96, 96, 96)\n",
    "    labels = batch_data['label'].to(device)  # 라벨 (B, 96, 96, 96)\n",
    "\n",
    "    labels = labels.squeeze(1)  # (B, 1, 96, 96, 96) → (B, 96, 96, 96)\n",
    "    labels = labels.long()  # 라벨을 정수형으로 변환\n",
    "\n",
    "    # 원핫 인코딩 (B, H, W, D) → (B, num_classes, H, W, D)\n",
    "    \n",
    "    labels_onehot = torch.nn.functional.one_hot(labels, num_classes=n_classes)\n",
    "    labels_onehot = labels_onehot.permute(0, 4, 1, 2, 3).float()  # (B, num_classes, H, W, D)\n",
    "\n",
    "    # 모델 예측\n",
    "    outputs = model(images)  # outputs: (B, num_classes, H, W, D)\n",
    "\n",
    "    # Loss 계산\n",
    "    loss = criterion(outputs, labels_onehot)\n",
    "    # loss = loss_fn(criterion(outputs, labels_onehot),class_weights=class_weights, device=device)\n",
    "    return loss, outputs, labels, outputs.argmax(dim=1)\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, accumulation_steps=4):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()  # 그래디언트 초기화\n",
    "    with tqdm(train_loader, desc='Training') as pbar:\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "            # 손실 계산\n",
    "            loss, _, _, _ = processing(batch_data, model, criterion, device)\n",
    "\n",
    "            # 그래디언트를 계산하고 누적\n",
    "            loss = loss / accumulation_steps  # 그래디언트 누적을 위한 스케일링\n",
    "            loss.backward()  # 그래디언트 계산 및 누적\n",
    "            \n",
    "            # 그래디언트 업데이트 (accumulation_steps마다 한 번)\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                optimizer.step()  # 파라미터 업데이트\n",
    "                optimizer.zero_grad()  # 누적된 그래디언트 초기화\n",
    "            \n",
    "            # 손실값 누적 (스케일링 복구)\n",
    "            epoch_loss += loss.item() * accumulation_steps  # 실제 손실값 반영\n",
    "            pbar.set_postfix(loss=loss.item() * accumulation_steps)  # 실제 손실값 출력\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    wandb.log({'train_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device, epoch, calculate_dice_interval):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    class_dice_scores = {i: [] for i in range(n_classes)}\n",
    "    class_f_beta_scores = {i: [] for i in range(n_classes)}\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc='Validation') as pbar:\n",
    "            for batch_data in pbar:\n",
    "                loss, _, labels, preds = processing(batch_data, model, criterion, device)\n",
    "                val_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "                # 각 클래스별 Dice 점수 계산\n",
    "                if epoch % calculate_dice_interval == 0:\n",
    "                    for i in range(n_classes):\n",
    "                        pred_i = (preds == i)\n",
    "                        label_i = (labels == i)\n",
    "                        dice_score = (2.0 * torch.sum(pred_i & label_i)) / (torch.sum(pred_i) + torch.sum(label_i) + 1e-8)\n",
    "                        class_dice_scores[i].append(dice_score.item())\n",
    "                        precision = (torch.sum(pred_i & label_i) + 1e-8) / (torch.sum(pred_i) + 1e-8)\n",
    "                        recall = (torch.sum(pred_i & label_i) + 1e-8) / (torch.sum(label_i) + 1e-8)\n",
    "                        f_beta_score = (1 + 4**2) * (precision * recall) / (4**2 * precision + recall + 1e-8)\n",
    "                        class_f_beta_scores[i].append(f_beta_score.item())\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    # 에포크별 평균 손실 로깅\n",
    "    wandb.log({'val_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    \n",
    "    # 각 클래스별 평균 Dice 점수 출력\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation Dice Score\")\n",
    "        all_classes_dice_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_dice = np.mean(class_dice_scores[i])\n",
    "            wandb.log({f'class_{i}_dice_score': mean_dice, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_dice:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_dice_scores.append(mean_dice)\n",
    "            \n",
    "        print()\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation F-beta Score\")\n",
    "        all_classes_fbeta_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_fbeta = np.mean(class_f_beta_scores[i])\n",
    "            wandb.log({f'class_{i}_f_beta_score': mean_fbeta, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_fbeta:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_fbeta_scores.append(mean_fbeta)\n",
    "                \n",
    "        print()\n",
    "        overall_mean_dice = np.mean(all_classes_dice_scores)\n",
    "        overall_mean_fbeta = np.mean(all_classes_fbeta_scores)\n",
    "        wandb.log({'overall_mean_f_beta_score': overall_mean_fbeta, 'overall_mean_dice_score': overall_mean_dice, 'epoch': epoch + 1})\n",
    "        print(f\"\\nOverall Mean Dice Score: {overall_mean_dice:.4f}\\nOverall Mean F-beta Score: {overall_mean_fbeta:.4f}\\n\")\n",
    "\n",
    "    if overall_mean_fbeta is None:\n",
    "        overall_mean_fbeta = 0\n",
    "\n",
    "    return val_loss / len(val_loader), overall_mean_fbeta\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, \n",
    "    device, start_epoch, best_val_loss, best_val_fbeta_score, calculate_dice_interval=1,\n",
    "    accumulation_steps=4, pretrained=False\n",
    "):\n",
    "    \"\"\"\n",
    "    모델을 학습하고 검증하는 함수\n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        train_loader: 학습 데이터 로더\n",
    "        val_loader: 검증 데이터 로더\n",
    "        criterion: 손실 함수\n",
    "        optimizer: 최적화 알고리즘\n",
    "        num_epochs: 총 학습 epoch 수\n",
    "        patience: early stopping 기준\n",
    "        device: GPU/CPU 장치\n",
    "        start_epoch: 시작 epoch\n",
    "        best_val_loss: 이전 최적 validation loss\n",
    "        best_val_fbeta_score: 이전 최적 validation f-beta score\n",
    "        calculate_dice_interval: Dice 점수 계산 주기\n",
    "    \"\"\"\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train One Epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            criterion=criterion, \n",
    "            optimizer=optimizer, \n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            accumulation_steps= accumulation_steps\n",
    "        )\n",
    "        \n",
    "        scheduler.step(train_loss)\n",
    "        # Validate One Epoch\n",
    "        val_loss, overall_mean_fbeta_score = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=epoch, \n",
    "            calculate_dice_interval=calculate_dice_interval\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F-beta: {overall_mean_fbeta_score:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss and overall_mean_fbeta_score > best_val_fbeta_score:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_fbeta_score = overall_mean_fbeta_score\n",
    "            epochs_no_improve = 0\n",
    "            if pretrained:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model_pretrained.pt')\n",
    "            else:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            print(f\"========================================================\")\n",
    "            print(f\"SUPER Best model saved. Loss:{best_val_loss:.4f}, Score:{best_val_fbeta_score:.4f}\")\n",
    "            print(f\"========================================================\")\n",
    "\n",
    "        # Early stopping 조건 체크\n",
    "        if val_loss >= best_val_loss and overall_mean_fbeta_score <= best_val_fbeta_score:\n",
    "            epochs_no_improve += 1\n",
    "        else:\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'last.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            break\n",
    "        # if epochs_no_improve % 6 == 0 & epochs_no_improve != 0:\n",
    "        #     # 손실이 개선되지 않았으므로 lambda 감소\n",
    "        #     new_lamda = max(criterion.lamda - 0.01, 0.35)  # 최소값은 0.1로 설정\n",
    "        #     criterion.set_lamda(new_lamda)\n",
    "        #     print(f\"Validation loss did not improve. Reducing lambda to {new_lamda:.4f}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 16/204 [00:32<03:05,  1.01it/s, loss=0.899]"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    start_epoch=start_epoch,\n",
    "    best_val_loss=best_val_loss,\n",
    "    best_val_fbeta_score=best_val_fbeta_score,\n",
    "    calculate_dice_interval=1,\n",
    "    accumulation_steps = accumulation_steps,\n",
    "    pretrained=True,\n",
    "     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 24/24 [00:02<00:00, 11.52it/s]\n",
      "Loading dataset: 100%|██████████| 1/1 [00:00<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 96, 96, 96]) torch.Size([16, 1, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "train_img_dir = './datasets/public_data/images'\n",
    "train_label_dir = './datasets/public_data/labels'\n",
    "\n",
    "train_loader, val_loader = None, None\n",
    "train_loader, val_loader = create_dataloaders_bw(\n",
    "    train_img_dir, \n",
    "    train_label_dir, \n",
    "    val_img_dir, \n",
    "    val_label_dir, \n",
    "    non_random_transforms = non_random_transforms, \n",
    "    val_non_random_transforms=non_random_transforms,\n",
    "    random_transforms = random_transforms, \n",
    "    val_random_transforms=val_random_transforms,\n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0,\n",
    "    train_num_repeat=num_repeat\n",
    "    )\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 best model 발견: model_checkpoints\\UNET_randGaus_511_241_noclswt_f48_d96s96_numb2_lr1e-03_a0.52_b0.48_b16_r4_ce0.4_ac1\\best_model_pretrained.pt\n",
      "기존 학습된 가중치를 성공적으로 로드했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pook0\\AppData\\Local\\Temp\\ipykernel_7604\\3722362581.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if checkpoint_dir.exists():\n",
    "    best_model_path = checkpoint_dir / 'best_model_pretrained.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"기존 best model 발견: {best_model_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(best_model_path, map_location=device)\n",
    "            # 체크포인트 내부 키 검증\n",
    "            required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch', 'best_val_loss', 'best_val_fbeta_score']\n",
    "            if all(k in checkpoint for k in required_keys):\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_val_loss = checkpoint['best_val_loss']\n",
    "                best_val_fbeta_score = checkpoint['best_val_fbeta_score']\n",
    "                print(\"기존 학습된 가중치를 성공적으로 로드했습니다.\")\n",
    "                checkpoint= None\n",
    "            else:\n",
    "                raise ValueError(\"체크포인트 파일에 필요한 key가 없습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"체크포인트 파일을 로드하는 중 오류 발생: {e}\")\n",
    "            \n",
    "lr = lr/2\n",
    "            \n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpook0612\u001b[0m (\u001b[33mlimbw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Workspace\\czll\\wandb\\run-20250117_221155-vhcnftut</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/limbw/czii_UNet/runs/vhcnftut' target=\"_blank\">UNET_randGaus_511_241_noclswt_f48_d96s96_numb2_lr1e-03_a0.52_b0.48_b16_r4_ce0.4_ac1</a></strong> to <a href='https://wandb.ai/limbw/czii_UNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/limbw/czii_UNet' target=\"_blank\">https://wandb.ai/limbw/czii_UNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/limbw/czii_UNet/runs/vhcnftut' target=\"_blank\">https://wandb.ai/limbw/czii_UNet/runs/vhcnftut</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = folder_name\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project='czii_UNet',  # 프로젝트 이름 설정\n",
    "    name=run_name,         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': lamda,\n",
    "        \"cross_entropy_weight\": ce_weight,\n",
    "        'feature_size': feature_size,\n",
    "        'img_size': img_size,\n",
    "        'sampling_ratio': ratios_list,\n",
    "        'device': device.type,\n",
    "        \"checkpoint_dir\": str(checkpoint_dir),\n",
    "        \"class_weights\": class_weights.tolist() if class_weights is not None else None,\n",
    "        # \"use_checkpoint\": use_checkpoint,\n",
    "        \"drop_rate\": drop_rate,\n",
    "        # \"attn_drop_rate\": attn_drop_rate,\n",
    "        # \"use_v2\": use_v2,\n",
    "        \"accumulation_steps\": accumulation_steps,\n",
    "        \"num_repeat\": num_repeat,\n",
    "        # \"num_bottleneck\": num_bottleneck,\n",
    "        \n",
    "        # 필요한 하이퍼파라미터 추가\n",
    "    }\n",
    ")\n",
    "# 모델을 wandb에 연결\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/96 [00:00<?, ?it/s]C:\\Users\\pook0\\AppData\\Local\\Temp\\ipykernel_7604\\3379185286.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_weights = torch.tensor(self.ce.weight)  # CrossEntropy의 weight를 사용\n",
      "Training: 100%|██████████| 96/96 [01:54<00:00,  1.19s/it, loss=0.395]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9857, Class 1: 0.7139, Class 2: 0.1690, Class 3: 0.4226, Class 4: 0.6580, Class 5: 0.4272, Class 6: 0.8507, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9783, Class 1: 0.8727, Class 2: 0.2351, Class 3: 0.4607, Class 4: 0.8420, Class 5: 0.5677, Class 6: 0.9644, \n",
      "\n",
      "Overall Mean Dice Score: 0.6145\n",
      "Overall Mean F-beta Score: 0.7415\n",
      "\n",
      "Training Loss: 0.4120, Validation Loss: 0.3826, Validation F-beta: 0.7415\n",
      "Epoch 34/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:37<00:00,  1.02s/it, loss=0.426]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9904, Class 1: 0.7648, Class 2: 0.3382, Class 3: 0.3725, Class 4: 0.7841, Class 5: 0.4612, Class 6: 0.8681, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9864, Class 1: 0.8621, Class 2: 0.3455, Class 3: 0.6053, Class 4: 0.8890, Class 5: 0.5687, Class 6: 0.9356, \n",
      "\n",
      "Overall Mean Dice Score: 0.6501\n",
      "Overall Mean F-beta Score: 0.7722\n",
      "\n",
      "Training Loss: 0.4112, Validation Loss: 0.3614, Validation F-beta: 0.7722\n",
      "Epoch 35/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:32<00:00,  1.04it/s, loss=0.418]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9891, Class 1: 0.7562, Class 2: 0.3958, Class 3: 0.2927, Class 4: 0.6327, Class 5: 0.4559, Class 6: 0.8529, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9842, Class 1: 0.8637, Class 2: 0.4967, Class 3: 0.4051, Class 4: 0.7404, Class 5: 0.6271, Class 6: 0.9198, \n",
      "\n",
      "Overall Mean Dice Score: 0.5981\n",
      "Overall Mean F-beta Score: 0.7112\n",
      "\n",
      "Training Loss: 0.4077, Validation Loss: 0.3983, Validation F-beta: 0.7112\n",
      "Epoch 36/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:34<00:00,  1.02it/s, loss=0.414]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9873, Class 1: 0.6913, Class 2: 0.2903, Class 3: 0.4181, Class 4: 0.7145, Class 5: 0.4673, Class 6: 0.8062, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9818, Class 1: 0.7887, Class 2: 0.4280, Class 3: 0.5968, Class 4: 0.8364, Class 5: 0.5448, Class 6: 0.9685, \n",
      "\n",
      "Overall Mean Dice Score: 0.6195\n",
      "Overall Mean F-beta Score: 0.7470\n",
      "\n",
      "Training Loss: 0.4078, Validation Loss: 0.4014, Validation F-beta: 0.7470\n",
      "Epoch 37/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:36<00:00,  1.01s/it, loss=0.405]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9871, Class 1: 0.6945, Class 2: 0.1672, Class 3: 0.3453, Class 4: 0.7238, Class 5: 0.4055, Class 6: 0.8320, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9810, Class 1: 0.8157, Class 2: 0.2271, Class 3: 0.5649, Class 4: 0.8320, Class 5: 0.5494, Class 6: 0.8752, \n",
      "\n",
      "Overall Mean Dice Score: 0.6002\n",
      "Overall Mean F-beta Score: 0.7274\n",
      "\n",
      "Training Loss: 0.4057, Validation Loss: 0.4176, Validation F-beta: 0.7274\n",
      "Epoch 38/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:34<00:00,  1.02it/s, loss=0.394]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9891, Class 1: 0.7572, Class 2: 0.3091, Class 3: 0.3170, Class 4: 0.6832, Class 5: 0.5402, Class 6: 0.8935, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9862, Class 1: 0.8995, Class 2: 0.3444, Class 3: 0.4880, Class 4: 0.7546, Class 5: 0.5563, Class 6: 0.9786, \n",
      "\n",
      "Overall Mean Dice Score: 0.6382\n",
      "Overall Mean F-beta Score: 0.7354\n",
      "\n",
      "Training Loss: 0.4083, Validation Loss: 0.3727, Validation F-beta: 0.7354\n",
      "Epoch 39/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:37<00:00,  1.02s/it, loss=0.401]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9867, Class 1: 0.8091, Class 2: 0.3766, Class 3: 0.3582, Class 4: 0.6746, Class 5: 0.4507, Class 6: 0.9039, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9811, Class 1: 0.8677, Class 2: 0.3471, Class 3: 0.4447, Class 4: 0.8370, Class 5: 0.5283, Class 6: 0.9783, \n",
      "\n",
      "Overall Mean Dice Score: 0.6393\n",
      "Overall Mean F-beta Score: 0.7312\n",
      "\n",
      "Training Loss: 0.4073, Validation Loss: 0.3825, Validation F-beta: 0.7312\n",
      "Epoch 40/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:32<00:00,  1.04it/s, loss=0.425]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9893, Class 1: 0.8123, Class 2: 0.2731, Class 3: 0.3342, Class 4: 0.7342, Class 5: 0.4350, Class 6: 0.9146, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9842, Class 1: 0.8747, Class 2: 0.3927, Class 3: 0.3505, Class 4: 0.8245, Class 5: 0.6353, Class 6: 0.9515, \n",
      "\n",
      "Overall Mean Dice Score: 0.6461\n",
      "Overall Mean F-beta Score: 0.7273\n",
      "\n",
      "Training Loss: 0.4051, Validation Loss: 0.3777, Validation F-beta: 0.7273\n",
      "Epoch 41/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:37<00:00,  1.01s/it, loss=0.435]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9913, Class 1: 0.7506, Class 2: 0.2854, Class 3: 0.4010, Class 4: 0.7784, Class 5: 0.4440, Class 6: 0.9039, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9897, Class 1: 0.8773, Class 2: 0.3367, Class 3: 0.5358, Class 4: 0.7626, Class 5: 0.5207, Class 6: 0.9301, \n",
      "\n",
      "Overall Mean Dice Score: 0.6556\n",
      "Overall Mean F-beta Score: 0.7253\n",
      "\n",
      "Training Loss: 0.4048, Validation Loss: 0.3636, Validation F-beta: 0.7253\n",
      "Epoch 42/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:40<00:00,  1.04s/it, loss=0.396]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9905, Class 1: 0.7529, Class 2: 0.2421, Class 3: 0.4988, Class 4: 0.7629, Class 5: 0.4515, Class 6: 0.9051, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9866, Class 1: 0.9054, Class 2: 0.3614, Class 3: 0.6432, Class 4: 0.8053, Class 5: 0.6065, Class 6: 0.9823, \n",
      "\n",
      "Overall Mean Dice Score: 0.6743\n",
      "Overall Mean F-beta Score: 0.7885\n",
      "\n",
      "Training Loss: 0.4053, Validation Loss: 0.3739, Validation F-beta: 0.7885\n",
      "Epoch 43/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:36<00:00,  1.01s/it, loss=0.434]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9868, Class 1: 0.7617, Class 2: 0.2082, Class 3: 0.2798, Class 4: 0.7050, Class 5: 0.4248, Class 6: 0.7222, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9804, Class 1: 0.8408, Class 2: 0.2665, Class 3: 0.4767, Class 4: 0.8173, Class 5: 0.5870, Class 6: 0.9503, \n",
      "\n",
      "Overall Mean Dice Score: 0.5787\n",
      "Overall Mean F-beta Score: 0.7344\n",
      "\n",
      "Training Loss: 0.4025, Validation Loss: 0.4145, Validation F-beta: 0.7344\n",
      "Epoch 44/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:38<00:00,  1.03s/it, loss=0.402]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9863, Class 1: 0.8423, Class 2: 0.1528, Class 3: 0.3626, Class 4: 0.7292, Class 5: 0.4420, Class 6: 0.8825, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9820, Class 1: 0.8514, Class 2: 0.1948, Class 3: 0.5163, Class 4: 0.7811, Class 5: 0.5552, Class 6: 0.9520, \n",
      "\n",
      "Overall Mean Dice Score: 0.6517\n",
      "Overall Mean F-beta Score: 0.7312\n",
      "\n",
      "Training Loss: 0.4058, Validation Loss: 0.3870, Validation F-beta: 0.7312\n",
      "Epoch 45/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:34<00:00,  1.01it/s, loss=0.396]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9882, Class 1: 0.7096, Class 2: 0.0659, Class 3: 0.2038, Class 4: 0.7102, Class 5: 0.4589, Class 6: 0.9020, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9855, Class 1: 0.8325, Class 2: 0.0619, Class 3: 0.2529, Class 4: 0.7364, Class 5: 0.5412, Class 6: 0.9623, \n",
      "\n",
      "Overall Mean Dice Score: 0.5969\n",
      "Overall Mean F-beta Score: 0.6651\n",
      "\n",
      "Training Loss: 0.4042, Validation Loss: 0.3835, Validation F-beta: 0.6651\n",
      "Epoch 46/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:32<00:00,  1.04it/s, loss=0.378]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9891, Class 1: 0.6950, Class 2: 0.2638, Class 3: 0.2935, Class 4: 0.7378, Class 5: 0.3807, Class 6: 0.8212, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9860, Class 1: 0.8729, Class 2: 0.3001, Class 3: 0.4036, Class 4: 0.7749, Class 5: 0.4660, Class 6: 0.9500, \n",
      "\n",
      "Overall Mean Dice Score: 0.5856\n",
      "Overall Mean F-beta Score: 0.6935\n",
      "\n",
      "Training Loss: 0.4048, Validation Loss: 0.3853, Validation F-beta: 0.6935\n",
      "Epoch 47/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:36<00:00,  1.00s/it, loss=0.399]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s, loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9888, Class 1: 0.7048, Class 2: 0.2508, Class 3: 0.3333, Class 4: 0.6225, Class 5: 0.4549, Class 6: 0.8385, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9842, Class 1: 0.8855, Class 2: 0.3075, Class 3: 0.4471, Class 4: 0.7439, Class 5: 0.5595, Class 6: 0.9628, \n",
      "\n",
      "Overall Mean Dice Score: 0.5908\n",
      "Overall Mean F-beta Score: 0.7198\n",
      "\n",
      "Training Loss: 0.4027, Validation Loss: 0.3846, Validation F-beta: 0.7198\n",
      "Epoch 48/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:35<00:00,  1.00it/s, loss=0.406]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, loss=0.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9893, Class 1: 0.6620, Class 2: 0.1970, Class 3: 0.3344, Class 4: 0.7265, Class 5: 0.4415, Class 6: 0.8931, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9839, Class 1: 0.8978, Class 2: 0.3642, Class 3: 0.6021, Class 4: 0.8390, Class 5: 0.5727, Class 6: 0.9588, \n",
      "\n",
      "Overall Mean Dice Score: 0.6115\n",
      "Overall Mean F-beta Score: 0.7741\n",
      "\n",
      "Training Loss: 0.4043, Validation Loss: 0.4001, Validation F-beta: 0.7741\n",
      "Epoch 49/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:35<00:00,  1.01it/s, loss=0.41] \n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9885, Class 1: 0.7761, Class 2: 0.3023, Class 3: 0.4037, Class 4: 0.6422, Class 5: 0.5331, Class 6: 0.8986, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9833, Class 1: 0.8835, Class 2: 0.4381, Class 3: 0.5402, Class 4: 0.7844, Class 5: 0.6391, Class 6: 0.9613, \n",
      "\n",
      "Overall Mean Dice Score: 0.6508\n",
      "Overall Mean F-beta Score: 0.7617\n",
      "\n",
      "Training Loss: 0.4032, Validation Loss: 0.3772, Validation F-beta: 0.7617\n",
      "Epoch 50/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:31<00:00,  1.05it/s, loss=0.422]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s, loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9888, Class 1: 0.8455, Class 2: 0.0000, Class 3: 0.2499, Class 4: 0.7289, Class 5: 0.3679, Class 6: 0.9254, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9842, Class 1: 0.8598, Class 2: 0.0000, Class 3: 0.4717, Class 4: 0.7956, Class 5: 0.5073, Class 6: 0.9579, \n",
      "\n",
      "Overall Mean Dice Score: 0.6235\n",
      "Overall Mean F-beta Score: 0.7185\n",
      "\n",
      "Training Loss: 0.4017, Validation Loss: 0.3848, Validation F-beta: 0.7185\n",
      "Epoch 51/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:35<00:00,  1.01it/s, loss=0.404]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9918, Class 1: 0.7763, Class 2: 0.2794, Class 3: 0.4091, Class 4: 0.7446, Class 5: 0.3753, Class 6: 0.9112, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9885, Class 1: 0.8458, Class 2: 0.3009, Class 3: 0.5439, Class 4: 0.7968, Class 5: 0.5661, Class 6: 0.9671, \n",
      "\n",
      "Overall Mean Dice Score: 0.6433\n",
      "Overall Mean F-beta Score: 0.7439\n",
      "\n",
      "Training Loss: 0.4020, Validation Loss: 0.3971, Validation F-beta: 0.7439\n",
      "Epoch 52/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:36<00:00,  1.01s/it, loss=0.417]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, loss=0.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9894, Class 1: 0.7214, Class 2: 0.1491, Class 3: 0.2269, Class 4: 0.7308, Class 5: 0.5395, Class 6: 0.9212, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9857, Class 1: 0.8522, Class 2: 0.2283, Class 3: 0.3564, Class 4: 0.7771, Class 5: 0.6399, Class 6: 0.9749, \n",
      "\n",
      "Overall Mean Dice Score: 0.6279\n",
      "Overall Mean F-beta Score: 0.7201\n",
      "\n",
      "Training Loss: 0.4023, Validation Loss: 0.3803, Validation F-beta: 0.7201\n",
      "Epoch 53/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:36<00:00,  1.00s/it, loss=0.405]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, loss=0.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9894, Class 1: 0.7022, Class 2: 0.2367, Class 3: 0.3929, Class 4: 0.6822, Class 5: 0.4985, Class 6: 0.8711, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9851, Class 1: 0.8832, Class 2: 0.2998, Class 3: 0.4900, Class 4: 0.8155, Class 5: 0.6045, Class 6: 0.9688, \n",
      "\n",
      "Overall Mean Dice Score: 0.6294\n",
      "Overall Mean F-beta Score: 0.7524\n",
      "\n",
      "Training Loss: 0.4005, Validation Loss: 0.3702, Validation F-beta: 0.7524\n",
      "Epoch 54/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:32<00:00,  1.03it/s, loss=0.408]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9907, Class 1: 0.8719, Class 2: 0.2630, Class 3: 0.4590, Class 4: 0.7371, Class 5: 0.5135, Class 6: 0.8950, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9880, Class 1: 0.9234, Class 2: 0.3382, Class 3: 0.4423, Class 4: 0.8032, Class 5: 0.5835, Class 6: 0.9705, \n",
      "\n",
      "Overall Mean Dice Score: 0.6953\n",
      "Overall Mean F-beta Score: 0.7446\n",
      "\n",
      "Training Loss: 0.4000, Validation Loss: 0.3663, Validation F-beta: 0.7446\n",
      "Epoch 55/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:31<00:00,  1.05it/s, loss=0.394]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9895, Class 1: 0.7497, Class 2: 0.3504, Class 3: 0.4563, Class 4: 0.7537, Class 5: 0.3585, Class 6: 0.9213, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9866, Class 1: 0.8162, Class 2: 0.4192, Class 3: 0.6268, Class 4: 0.7873, Class 5: 0.4354, Class 6: 0.9709, \n",
      "\n",
      "Overall Mean Dice Score: 0.6479\n",
      "Overall Mean F-beta Score: 0.7273\n",
      "\n",
      "Training Loss: 0.4002, Validation Loss: 0.3822, Validation F-beta: 0.7273\n",
      "Epoch 56/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:31<00:00,  1.05it/s, loss=0.404]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9881, Class 1: 0.8026, Class 2: 0.3226, Class 3: 0.4632, Class 4: 0.7030, Class 5: 0.4158, Class 6: 0.8740, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9842, Class 1: 0.8787, Class 2: 0.3405, Class 3: 0.7033, Class 4: 0.7842, Class 5: 0.4682, Class 6: 0.9642, \n",
      "\n",
      "Overall Mean Dice Score: 0.6517\n",
      "Overall Mean F-beta Score: 0.7597\n",
      "\n",
      "Training Loss: 0.4024, Validation Loss: 0.3567, Validation F-beta: 0.7597\n",
      "Epoch 57/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:36<00:00,  1.01s/it, loss=0.379]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9886, Class 1: 0.7847, Class 2: 0.2856, Class 3: 0.4451, Class 4: 0.7133, Class 5: 0.4771, Class 6: 0.8455, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9850, Class 1: 0.8854, Class 2: 0.3368, Class 3: 0.4999, Class 4: 0.7746, Class 5: 0.5798, Class 6: 0.9575, \n",
      "\n",
      "Overall Mean Dice Score: 0.6531\n",
      "Overall Mean F-beta Score: 0.7394\n",
      "\n",
      "Training Loss: 0.4050, Validation Loss: 0.3433, Validation F-beta: 0.7394\n",
      "Epoch 58/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [01:36<00:00,  1.00s/it, loss=0.407]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, loss=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9895, Class 1: 0.7912, Class 2: 0.2461, Class 3: 0.3238, Class 4: 0.7846, Class 5: 0.4903, Class 6: 0.8826, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9856, Class 1: 0.8812, Class 2: 0.3583, Class 3: 0.5325, Class 4: 0.8302, Class 5: 0.5960, Class 6: 0.9808, \n",
      "\n",
      "Overall Mean Dice Score: 0.6545\n",
      "Overall Mean F-beta Score: 0.7641\n",
      "\n",
      "Training Loss: 0.3993, Validation Loss: 0.3940, Validation F-beta: 0.7641\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>class_0_dice_score</td><td>▁▆▅▃▃▅▂▅▇▇▂▂▄▅▅▅▄▅█▅▅▇▅▄▄▅</td></tr><tr><td>class_0_f_beta_score</td><td>▁▆▅▃▃▆▃▅█▆▂▃▅▆▅▄▄▅▇▆▅▇▆▅▅▆</td></tr><tr><td>class_1_dice_score</td><td>▃▄▄▂▂▄▆▆▄▄▄▇▃▂▂▁▅▇▅▃▂█▄▆▅▅</td></tr><tr><td>class_1_f_beta_score</td><td>▅▅▅▁▂▇▅▅▆▇▄▄▃▅▆▇▆▅▄▄▆█▂▆▆▆</td></tr><tr><td>class_2_dice_score</td><td>▄▇█▆▄▆█▆▆▅▅▄▂▆▅▄▆▁▆▄▅▆▇▇▆▅</td></tr><tr><td>class_2_f_beta_score</td><td>▄▆█▇▄▆▆▇▆▆▅▄▂▅▅▆▇▁▅▄▅▆▇▆▆▆</td></tr><tr><td>class_3_dice_score</td><td>▆▅▃▆▄▄▅▄▆█▃▅▁▃▄▄▆▂▆▂▅▇▇▇▇▄</td></tr><tr><td>class_3_f_beta_score</td><td>▄▆▃▆▆▅▄▃▅▇▄▅▁▃▄▆▅▄▆▃▅▄▇█▅▅</td></tr><tr><td>class_4_dice_score</td><td>▃█▁▅▅▄▃▆█▇▅▆▅▆▁▅▂▆▆▆▄▆▇▄▅█</td></tr><tr><td>class_4_f_beta_score</td><td>▆█▁▆▅▂▆▅▂▄▅▃▁▃▁▆▃▄▄▃▅▄▃▃▃▅</td></tr><tr><td>class_5_dice_score</td><td>▄▅▅▅▃█▅▄▄▅▄▄▅▂▅▄█▁▂█▆▇▁▃▆▆</td></tr><tr><td>class_5_f_beta_score</td><td>▆▆█▅▅▅▄█▄▇▆▅▅▂▅▆█▃▅█▇▆▁▂▆▆</td></tr><tr><td>class_6_dice_score</td><td>▅▆▆▄▅▇▇█▇▇▁▇▇▄▅▇▇███▆▇█▆▅▇</td></tr><tr><td>class_6_f_beta_score</td><td>▇▅▄▇▁██▆▅█▆▆▇▆▇▆▇▆▇█▇▇▇▇▆█</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>overall_mean_dice_score</td><td>▃▅▂▃▂▅▅▅▆▇▁▅▂▁▂▃▅▄▅▄▄█▅▅▅▆</td></tr><tr><td>overall_mean_f_beta_score</td><td>▅▇▄▆▅▅▅▅▄█▅▅▁▃▄▇▆▄▅▄▆▆▅▆▅▇</td></tr><tr><td>train_epoch_loss</td><td>██▆▆▅▆▅▄▄▄▃▅▄▄▃▄▃▂▂▃▂▁▁▃▄▁</td></tr><tr><td>val_epoch_loss</td><td>▅▃▆▆█▄▅▄▃▄█▅▅▅▅▆▄▅▆▄▄▃▅▂▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>class_0_dice_score</td><td>0.98947</td></tr><tr><td>class_0_f_beta_score</td><td>0.9856</td></tr><tr><td>class_1_dice_score</td><td>0.79117</td></tr><tr><td>class_1_f_beta_score</td><td>0.88115</td></tr><tr><td>class_2_dice_score</td><td>0.24611</td></tr><tr><td>class_2_f_beta_score</td><td>0.35833</td></tr><tr><td>class_3_dice_score</td><td>0.32377</td></tr><tr><td>class_3_f_beta_score</td><td>0.53247</td></tr><tr><td>class_4_dice_score</td><td>0.78458</td></tr><tr><td>class_4_f_beta_score</td><td>0.83022</td></tr><tr><td>class_5_dice_score</td><td>0.49035</td></tr><tr><td>class_5_f_beta_score</td><td>0.59597</td></tr><tr><td>class_6_dice_score</td><td>0.88264</td></tr><tr><td>class_6_f_beta_score</td><td>0.9808</td></tr><tr><td>epoch</td><td>58</td></tr><tr><td>overall_mean_dice_score</td><td>0.6545</td></tr><tr><td>overall_mean_f_beta_score</td><td>0.76412</td></tr><tr><td>train_epoch_loss</td><td>0.39931</td></tr><tr><td>val_epoch_loss</td><td>0.39401</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">UNET_randGaus_511_241_noclswt_f48_d96s96_numb2_lr1e-03_a0.52_b0.48_b16_r4_ce0.4_ac1</strong> at: <a href='https://wandb.ai/limbw/czii_UNet/runs/vhcnftut' target=\"_blank\">https://wandb.ai/limbw/czii_UNet/runs/vhcnftut</a><br> View project at: <a href='https://wandb.ai/limbw/czii_UNet' target=\"_blank\">https://wandb.ai/limbw/czii_UNet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250117_221155-vhcnftut\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    start_epoch=start_epoch,\n",
    "    best_val_loss=best_val_loss,\n",
    "    best_val_fbeta_score=best_val_fbeta_score,\n",
    "    calculate_dice_interval=1,\n",
    "    accumulation_steps = accumulation_steps,\n",
    "    pretrained=False,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (879943805.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    if:\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fs6utwyo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cc3ae761af43baae5aca60b5d2e7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>class_0_dice_score</td><td>▁</td></tr><tr><td>class_0_f_beta_score</td><td>▁</td></tr><tr><td>class_1_dice_score</td><td>▁</td></tr><tr><td>class_1_f_beta_score</td><td>▁</td></tr><tr><td>class_2_dice_score</td><td>▁</td></tr><tr><td>class_2_f_beta_score</td><td>▁</td></tr><tr><td>class_3_dice_score</td><td>▁</td></tr><tr><td>class_3_f_beta_score</td><td>▁</td></tr><tr><td>class_4_dice_score</td><td>▁</td></tr><tr><td>class_4_f_beta_score</td><td>▁</td></tr><tr><td>class_5_dice_score</td><td>▁</td></tr><tr><td>class_5_f_beta_score</td><td>▁</td></tr><tr><td>class_6_dice_score</td><td>▁</td></tr><tr><td>class_6_f_beta_score</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>overall_mean_dice_score</td><td>▁</td></tr><tr><td>overall_mean_f_beta_score</td><td>▁</td></tr><tr><td>val_epoch_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>class_0_dice_score</td><td>0.65703</td></tr><tr><td>class_0_f_beta_score</td><td>0.50748</td></tr><tr><td>class_1_dice_score</td><td>0.53332</td></tr><tr><td>class_1_f_beta_score</td><td>0.64703</td></tr><tr><td>class_2_dice_score</td><td>0.00286</td></tr><tr><td>class_2_f_beta_score</td><td>0.02334</td></tr><tr><td>class_3_dice_score</td><td>0.23703</td></tr><tr><td>class_3_f_beta_score</td><td>0.23033</td></tr><tr><td>class_4_dice_score</td><td>0.65487</td></tr><tr><td>class_4_f_beta_score</td><td>0.62525</td></tr><tr><td>class_5_dice_score</td><td>0.47899</td></tr><tr><td>class_5_f_beta_score</td><td>0.51448</td></tr><tr><td>class_6_dice_score</td><td>0.42545</td></tr><tr><td>class_6_f_beta_score</td><td>0.47197</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>overall_mean_dice_score</td><td>0.42708</td></tr><tr><td>overall_mean_f_beta_score</td><td>0.43141</td></tr><tr><td>val_epoch_loss</td><td>0.7152</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SwinUNETR96_96_lr0.001_lambda0.52_batch2</strong> at: <a href='https://wandb.ai/waooang/czii_SwinUnetR_val/runs/fs6utwyo' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR_val/runs/fs6utwyo</a><br/> View project at: <a href='https://wandb.ai/waooang/czii_SwinUnetR_val' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR_val</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241219_200219-fs6utwyo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fs6utwyo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Workspace\\czll\\wandb\\run-20241219_200454-121l7bn3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/czii_SwinUnetR_val/runs/121l7bn3' target=\"_blank\">SwinUNETR96_96_lr0.001_lambda0.52_batch2</a></strong> to <a href='https://wandb.ai/waooang/czii_SwinUnetR_val' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/czii_SwinUnetR_val' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR_val</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/czii_SwinUnetR_val/runs/121l7bn3' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR_val/runs/121l7bn3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n",
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_21000\\1177025787.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrain_path, map_location=device)\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.38it/s, loss=0.865]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.6570, Class 1: 0.5333, Class 2: 0.0029, Class 3: 0.2370, \n",
      "Class 4: 0.6549, Class 5: 0.4790, Class 6: 0.4255, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.5075, Class 1: 0.6470, Class 2: 0.0233, Class 3: 0.2303, \n",
      "Class 4: 0.6252, Class 5: 0.5145, Class 6: 0.4720, \n",
      "Overall Mean Dice Score: 0.4659\n",
      "Overall Mean F-beta Score: 0.4978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, CropForegroundd, GaussianSmoothd, ScaleIntensityd,\n",
    "    RandSpatialCropd, RandRotate90d, RandFlipd, RandGaussianNoised,\n",
    "    ToTensord, RandCropByLabelClassesd\n",
    ")\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR, SwinUNETR\n",
    "from monai.losses import TverskyLoss\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from src.dataset.dataset import make_val_dataloader\n",
    "\n",
    "val_img_dir = \"./datasets/val/images\"\n",
    "val_label_dir = \"./datasets/val/labels\"\n",
    "img_depth = 96\n",
    "img_size = 96  # Match your patch size\n",
    "n_classes = 7\n",
    "batch_size = 2 # 13.8GB GPU memory required for 128x128 img size\n",
    "num_samples = batch_size # 한 이미지에서 뽑을 샘플 수\n",
    "loader_batch = 1\n",
    "lamda = 0.52\n",
    "\n",
    "wandb.init(\n",
    "    project='czii_SwinUnetR_val',  # 프로젝트 이름 설정\n",
    "    name='SwinUNETR96_96_lr0.001_lambda0.52_batch2',         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': lamda,\n",
    "        'img_size': img_size,\n",
    "        'device': 'cuda',\n",
    "        \"checkpoint_dir\": \"./model_checkpoints/SwinUNETR96_96_lr0.001_lambda0.52_batch2\",\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    GaussianSmoothd(\n",
    "        keys=[\"image\"],      # 변환을 적용할 키\n",
    "        sigma=[1.0, 1.0, 1.0]  # 각 축(x, y, z)의 시그마 값\n",
    "        ),\n",
    "])\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "])\n",
    "\n",
    "val_loader = make_val_dataloader(\n",
    "    val_img_dir, \n",
    "    val_label_dir, \n",
    "    non_random_transforms = non_random_transforms, \n",
    "    random_transforms = random_transforms, \n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0\n",
    ")\n",
    "criterion = TverskyLoss(\n",
    "    alpha= 1 - lamda,  # FP에 대한 가중치\n",
    "    beta=lamda,       # FN에 대한 가중치\n",
    "    include_background=False,  # 배경 클래스 제외\n",
    "    softmax=True\n",
    ")\n",
    "    \n",
    "    \n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "img_size = 96\n",
    "img_depth = img_size\n",
    "n_classes = 7 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrain_path = \"./model_checkpoints/SwinUNETR96_96_lr0.001_lambda0.52_batch2/best_model.pt\"\n",
    "model = SwinUNETR(\n",
    "    img_size=(img_depth, img_size, img_size),\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n",
    "# Pretrained weights 불러오기\n",
    "checkpoint = torch.load(pretrain_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "val_loss, overall_mean_fbeta_score = validate_one_epoch(\n",
    "    model=model, \n",
    "    val_loader=val_loader, \n",
    "    criterion=criterion, \n",
    "    device=device, \n",
    "    epoch=0, \n",
    "    calculate_dice_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from src.dataset.preprocessing import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import Compose, EnsureChannelFirstd, NormalizeIntensityd, Orientationd, GaussianSmoothd\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copick\n",
    "\n",
    "import torch\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file written to ./kaggle/working/copick.config\n",
      "file length: 7\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "config_blob = \"\"\"{\n",
    "    \"name\": \"czii_cryoet_mlchallenge_2024\",\n",
    "    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n",
    "    \"version\": \"1.0.0\",\n",
    "\n",
    "    \"pickable_objects\": [\n",
    "        {\n",
    "            \"name\": \"apo-ferritin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"4V1W\",\n",
    "            \"label\": 1,\n",
    "            \"color\": [  0, 117, 220, 128],\n",
    "            \"radius\": 60,\n",
    "            \"map_threshold\": 0.0418\n",
    "        },\n",
    "        {\n",
    "          \"name\" : \"beta-amylase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"8ZRZ\",\n",
    "            \"label\": 2,\n",
    "            \"color\": [255, 255, 255, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578  \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"beta-galactosidase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6X1Q\",\n",
    "            \"label\": 3,\n",
    "            \"color\": [ 76,   0,  92, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ribosome\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6EK0\",\n",
    "            \"label\": 4,\n",
    "            \"color\": [  0,  92,  49, 128],\n",
    "            \"radius\": 150,\n",
    "            \"map_threshold\": 0.0374\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thyroglobulin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6SCJ\",\n",
    "            \"label\": 5,\n",
    "            \"color\": [ 43, 206,  72, 128],\n",
    "            \"radius\": 130,\n",
    "            \"map_threshold\": 0.0278\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"virus-like-particle\",\n",
    "            \"is_particle\": true,\n",
    "            \"label\": 6,\n",
    "            \"color\": [255, 204, 153, 128],\n",
    "            \"radius\": 135,\n",
    "            \"map_threshold\": 0.201\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"membrane\",\n",
    "            \"is_particle\": false,\n",
    "            \"label\": 8,\n",
    "            \"color\": [100, 100, 100, 128]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"background\",\n",
    "            \"is_particle\": false,\n",
    "            \"label\": 9,\n",
    "            \"color\": [10, 150, 200, 128]\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"overlay_root\": \"./kaggle/working/overlay\",\n",
    "\n",
    "    \"overlay_fs_args\": {\n",
    "        \"auto_mkdir\": true\n",
    "    },\n",
    "\n",
    "    \"static_root\": \"./kaggle/input/czii-cryo-et-object-identification/test/static\"\n",
    "}\"\"\"\n",
    "\n",
    "copick_config_path = \"./kaggle/working/copick.config\"\n",
    "preprocessor = Preprocessor(config_blob,copick_config_path=copick_config_path)\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    GaussianSmoothd(\n",
    "        keys=[\"image\"],      # 변환을 적용할 키\n",
    "        sigma=[1.0, 1.0, 1.0]  # 각 축(x, y, z)의 시그마 값\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\monai\\utils\\deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_6248\\2937359115.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrain_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "img_size = 96\n",
    "img_depth = img_size\n",
    "n_classes = 7 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrain_path = \"./model_checkpoints/SwinUNETR96_96_lr0.001_lambda0.52_batch2/best_model.pt\"\n",
    "model = SwinUNETR(\n",
    "    img_size=(img_depth, img_size, img_size),\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n",
    "# Pretrained weights 불러오기\n",
    "checkpoint = torch.load(pretrain_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/4 [00:03<?, ?it/s, loss=0.764]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcalculate_dice_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 64\u001b[0m, in \u001b[0;36mvalidate_one_epoch\u001b[1;34m(model, val_loader, criterion, device, epoch, calculate_dice_interval)\u001b[0m\n\u001b[0;32m     61\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 각 클래스별 Dice 점수 계산\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcalculate_dice_interval\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_classes):\n\u001b[0;32m     66\u001b[0m         pred_i \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m i)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: integer modulo by zero"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "val_loss = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=1, \n",
    "            calculate_dice_interval=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 2/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 3/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to: submission.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import Compose, NormalizeIntensity\n",
    "import cc3d\n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    all_coords = np.vstack(all_coords)\n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "    return df\n",
    "\n",
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              2: \"beta-amylase\",\n",
    "              3: \"beta-galactosidase\", \n",
    "              4: \"ribosome\", \n",
    "              5: \"thyroglobulin\", \n",
    "              6: \"virus-like-particle\"}\n",
    "BLOB_THRESHOLD = 200\n",
    "CERTAINTY_THRESHOLD = 0.05\n",
    "\n",
    "classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    location_dfs = []  # DataFrame 리스트로 초기화\n",
    "    \n",
    "    for vol_idx, run in enumerate(preprocessor.root.runs):\n",
    "        print(f\"Processing volume {vol_idx + 1}/{len(preprocessor.root.runs)}\")\n",
    "        tomogram = preprocessor.processing(run=run, task=\"task\")\n",
    "        task_files = [{\"image\": tomogram}]\n",
    "        task_ds = CacheDataset(data=task_files, transform=non_random_transforms)\n",
    "        task_loader = DataLoader(task_ds, batch_size=1, num_workers=0)\n",
    "        \n",
    "        for task_data in task_loader:\n",
    "            images = task_data['image'].to(\"cuda\")\n",
    "            outputs = sliding_window_inference(\n",
    "                inputs=images,\n",
    "                roi_size=(96, 96, 96),  # ROI 크기\n",
    "                sw_batch_size=4,\n",
    "                predictor=model.forward,\n",
    "                overlap=0.1,\n",
    "                sw_device=\"cuda\",\n",
    "                device=\"cpu\",\n",
    "                buffer_steps=1,\n",
    "                buffer_dim=-1\n",
    "            )\n",
    "            outputs = outputs.argmax(dim=1).squeeze(0).cpu().numpy()  # 클래스 채널 예측\n",
    "            location = {}  # 좌표 저장용 딕셔너리\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(outputs == c)  # cc3d 라벨링\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:] * 10.012444  # 스케일 변환\n",
    "                zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]  # 크기 필터링\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])  # 좌표 스왑 (z, y, x -> x, y, z)\n",
    "\n",
    "                location[id_to_name[c]] = xyz  # ID 이름 매칭 저장\n",
    "\n",
    "            # 데이터프레임 변환\n",
    "            df = dict_to_df(location, run.name)\n",
    "            location_dfs.append(df)  # 리스트에 추가\n",
    "        \n",
    "        # if vol_idx == 2:\n",
    "        #     break\n",
    "    \n",
    "    # DataFrame 병합\n",
    "    final_df = pd.concat(location_dfs, ignore_index=True)\n",
    "    \n",
    "    # ID 추가 및 CSV 저장\n",
    "    final_df.insert(loc=0, column='id', value=np.arange(len(final_df)))\n",
    "    final_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved to: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
