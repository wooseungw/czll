{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.4.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: c:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.14.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.19.1+cu121\n",
      "tqdm version: 4.66.5\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR, SwinUNETR\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_info = {\n",
    "#     0: {\"name\": \"background\", \"weight\": 10000},  # weight 없음\n",
    "#     1: {\"name\": \"apo-ferritin\", \"weight\": 300},\n",
    "#     2: {\"name\": \"beta-amylase\", \"weight\": 100}, # 4130\n",
    "#     3: {\"name\": \"beta-galactosidase\", \"weight\": 150}, #3080\n",
    "#     4: {\"name\": \"ribosome\", \"weight\": 6000},\n",
    "#     5: {\"name\": \"thyroglobulin\", \"weight\": 4000},\n",
    "#     6: {\"name\": \"virus-like-particle\", \"weight\": 2000},\n",
    "# }\n",
    "\n",
    "# # 가중치에 비례한 비율 계산\n",
    "# raw_ratios = {\n",
    "#     k: (v[\"weight\"] if v[\"weight\"] is not None else 0.01)  # 가중치 비례, None일 경우 기본값\n",
    "#     for k, v in class_info.items()\n",
    "# }\n",
    "# total = sum(raw_ratios.values())\n",
    "# ratios = {k: v / total for k, v in raw_ratios.items()}\n",
    "\n",
    "# # 최종 합계가 1인지 확인\n",
    "# final_total = sum(ratios.values())\n",
    "# print(\"클래스 비율:\", ratios)\n",
    "# print(\"최종 합계:\", final_total)\n",
    "\n",
    "# # 비율을 리스트로 변환\n",
    "# ratios_list = [ratios[k] for k in sorted(ratios.keys())]\n",
    "# print(\"클래스 비율 리스트:\", ratios_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반비례 클래스 비율: {0: 0.008995502248875561, 1: 0.29985007496251875, 2: 0.008995502248875561, 3: 0.5997001499250375, 4: 0.014992503748125935, 5: 0.022488755622188904, 6: 0.04497751124437781}\n",
      "최종 합계: 1.0\n",
      "반비례 클래스 비율 리스트: [0.008995502248875561, 0.29985007496251875, 0.008995502248875561, 0.5997001499250375, 0.014992503748125935, 0.022488755622188904, 0.04497751124437781]\n"
     ]
    }
   ],
   "source": [
    "class_info = {\n",
    "    0: {\"name\": \"background\", \"weight\": 10000},  # weight 없음\n",
    "    1: {\"name\": \"apo-ferritin\", \"weight\": 300},\n",
    "    2: {\"name\": \"beta-amylase\", \"weight\": 10000}, # 100\n",
    "    3: {\"name\": \"beta-galactosidase\", \"weight\": 150}, #3080\n",
    "    4: {\"name\": \"ribosome\", \"weight\": 6000},\n",
    "    5: {\"name\": \"thyroglobulin\", \"weight\": 4000},\n",
    "    6: {\"name\": \"virus-like-particle\", \"weight\": 2000},\n",
    "}\n",
    "\n",
    "# 가중치의 역수 계산 (0인 경우 작은 값을 대체)\n",
    "inverse_ratios = {\n",
    "    k: (1 / v[\"weight\"] if v[\"weight\"] > 0 else 1e-6)\n",
    "    for k, v in class_info.items()\n",
    "}\n",
    "\n",
    "# 정규화하여 비율 계산\n",
    "total_inverse = sum(inverse_ratios.values())\n",
    "ratios_inverse = {k: v / total_inverse for k, v in inverse_ratios.items()}\n",
    "\n",
    "# 최종 합계가 1인지 확인\n",
    "final_total_inverse = sum(ratios_inverse.values())\n",
    "print(\"반비례 클래스 비율:\", ratios_inverse)\n",
    "print(\"최종 합계:\", final_total_inverse)\n",
    "\n",
    "# 비율을 리스트로 변환\n",
    "ratios_list = [ratios_inverse[k] for k in sorted(ratios_inverse.keys())]\n",
    "print(\"반비례 클래스 비율 리스트:\", ratios_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.dataset import create_dataloaders\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, CropForegroundd, GaussianSmoothd, ScaleIntensityd,\n",
    "    RandSpatialCropd, RandRotate90d, RandFlipd, RandGaussianNoised,\n",
    "    ToTensord, RandCropByLabelClassesd\n",
    ")\n",
    "\n",
    "train_img_dir = \"./datasets/train/images\"\n",
    "train_label_dir = \"./datasets/train/labels\"\n",
    "val_img_dir = \"./datasets/val/images\"\n",
    "val_label_dir = \"./datasets/val/labels\"\n",
    "img_depth = 96\n",
    "img_size = 96  # Match your patch size\n",
    "n_classes = 7\n",
    "batch_size = 2 # 13.8GB GPU memory required for 128x128 img size\n",
    "num_samples = batch_size # 한 이미지에서 뽑을 샘플 수\n",
    "loader_batch = 1\n",
    "\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    GaussianSmoothd(\n",
    "        keys=[\"image\"],      # 변환을 적용할 키\n",
    "        sigma=[1.0, 1.0, 1.0]  # 각 축(x, y, z)의 시그마 값\n",
    "        ),\n",
    "])\n",
    "random_transforms = Compose([\n",
    "    \n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 24/24 [00:36<00:00,  1.53s/it]\n",
      "Loading dataset: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = None, None\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_img_dir, \n",
    "    train_label_dir, \n",
    "    val_img_dir, \n",
    "    val_label_dir, \n",
    "    non_random_transforms = non_random_transforms, \n",
    "    random_transforms = random_transforms, \n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://monai.io/model-zoo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import TverskyLoss\n",
    "import torch\n",
    "\n",
    "def loss_fn(loss, class_weights):\n",
    "    # 클래스 차원에 가중치 적용 (B, num_classes, ...)\n",
    "    class_weights = class_weights.view(1, n_classes, 1, 1, 1)  # [1, num_classes, 1, 1, 1]\n",
    "    weighted_loss = loss * class_weights\n",
    "\n",
    "    # 모든 차원을 평균 내어 스칼라 손실 반환\n",
    "    final_loss = torch.mean(weighted_loss)\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\monai\\utils\\deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from monai.metrics import DiceMetric\n",
    "# Model Configuration\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 4000\n",
    "lamda = 0.52\n",
    "lr = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=(img_depth, img_size, img_size),\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=False,\n",
    ").to(device)\n",
    "# Pretrained weights 불러오기\n",
    "# pretrain_path = \"./swin_unetr_btcv_segmentation/models/model.pt\"\n",
    "# weight = torch.load(pretrain_path, map_location=device)\n",
    "\n",
    "# # 출력 레이어의 키를 제외한 나머지 가중치만 로드\n",
    "# filtered_weights = {k: v for k, v in weight.items() if \"out.conv.conv\" not in k}\n",
    "\n",
    "# # strict=False로 로드하여 불일치하는 부분 무시\n",
    "# model.load_state_dict(filtered_weights, strict=False)\n",
    "# print(\"Filtered weights loaded successfully. Output layer will be trained from scratch.\")\n",
    "\n",
    "# Load pretrained weights\n",
    "# model.load_from(weights=np.load(config_vit.real_pretrained_path, allow_pickle=True))\n",
    "# TverskyLoss 설정\n",
    "lamda = 0.55\n",
    "n_classes = 7\n",
    "class_weights = torch.tensor([0.1, 1, 0.3, 1, 1, 1, 1], dtype=torch.float32)  # 클래스별 가중치\n",
    "# TverskyLoss 초기화\n",
    "criterion = TverskyLoss(\n",
    "    alpha=1 - lamda,  # FP에 대한 가중치\n",
    "    beta=lamda,       # FN에 대한 가중치\n",
    "    include_background=True,\n",
    "    reduction=\"none\",\n",
    ")\n",
    "\n",
    "# 체크포인트 디렉토리 및 파일 설정\n",
    "checkpoint_base_dir = Path(\"./model_checkpoints\")\n",
    "checkpoint_dir = checkpoint_base_dir / f\"SwinUNETR_no_pretrain_f48_newratio_bTrue_{img_depth}_{img_size}_lr{lr}_lambda{lamda}_batch{batch_size}\"\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# 체크포인트 디렉토리 생성\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    best_model_path = checkpoint_dir / 'best_model.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"기존 best model 발견: {best_model_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(best_model_path, map_location=device)\n",
    "            # 체크포인트 내부 키 검증\n",
    "            required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch', 'best_val_loss']\n",
    "            if all(k in checkpoint for k in required_keys):\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_val_loss = checkpoint['best_val_loss']\n",
    "                print(\"기존 학습된 가중치를 성공적으로 로드했습니다.\")\n",
    "            else:\n",
    "                raise ValueError(\"체크포인트 파일에 필요한 key가 없습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"체크포인트 파일을 로드하는 중 오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 96, 96, 96]) torch.Size([2, 1, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "images, labels = batch[\"image\"], batch[\"label\"]\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwoow070840\u001b[0m (\u001b[33mwaooang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Workspace\\czll\\wandb\\run-20241219_160616-yu7oyb67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/yu7oyb67' target=\"_blank\">SwinUNETR_lr0.001_bs2_lambda0.52_20241219_160616</a></strong> to <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/yu7oyb67' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR/runs/yu7oyb67</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f'SwinUNETR_lr{lr}_bs{batch_size}_lambda{lamda}_{current_time}'\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project='czii_SwinUnetR',  # 프로젝트 이름 설정\n",
    "    name=run_name,         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': lamda,\n",
    "        # 필요한 하이퍼파라미터 추가\n",
    "    }\n",
    ")\n",
    "# 모델을 wandb에 연결\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:59<00:00,  4.98s/it, loss=0.863]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.31it/s, loss=0.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9712, Class 1: 0.0033, Class 2: 0.0117, Class 3: 0.0131, \n",
      "Class 4: 0.1128, Class 5: 0.0248, Class 6: 0.0152, \n",
      "\n",
      "Overall Mean Dice Score: 0.1646\n",
      "\n",
      "Training Loss: 0.8961, Validation Loss: 0.8682\n",
      "========================================================\n",
      "Best model saved with validation loss: 0.8682\n",
      "========================================================\n",
      "Epoch 2/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:46<00:00,  4.43s/it, loss=0.863]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.38it/s, loss=0.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9787, Class 1: 0.0003, Class 2: 0.0087, Class 3: 0.0056, \n",
      "Class 4: 0.2366, Class 5: 0.0033, Class 6: 0.0007, \n",
      "\n",
      "Overall Mean Dice Score: 0.1763\n",
      "\n",
      "Training Loss: 0.8491, Validation Loss: 0.8371\n",
      "========================================================\n",
      "Best model saved with validation loss: 0.8371\n",
      "========================================================\n",
      "Epoch 3/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:45<00:00,  4.40s/it, loss=0.791]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.36it/s, loss=0.806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9903, Class 1: 0.0000, Class 2: 0.0022, Class 3: 0.0653, \n",
      "Class 4: 0.2027, Class 5: 0.0011, Class 6: 0.0099, \n",
      "\n",
      "Overall Mean Dice Score: 0.1816\n",
      "\n",
      "Training Loss: 0.8323, Validation Loss: 0.8279\n",
      "========================================================\n",
      "Best model saved with validation loss: 0.8279\n",
      "========================================================\n",
      "Epoch 4/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:46<00:00,  4.43s/it, loss=0.857]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.33it/s, loss=0.845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9830, Class 1: 0.0193, Class 2: 0.0000, Class 3: 0.1000, \n",
      "Class 4: 0.0974, Class 5: 0.0653, Class 6: 0.0174, \n",
      "\n",
      "Overall Mean Dice Score: 0.1832\n",
      "\n",
      "Training Loss: 0.8312, Validation Loss: 0.8333\n",
      "Epoch 5/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:45<00:00,  4.40s/it, loss=0.787]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.35it/s, loss=0.822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9891, Class 1: 0.0000, Class 2: 0.0000, Class 3: 0.0836, \n",
      "Class 4: 0.3100, Class 5: 0.0071, Class 6: 0.0000, \n",
      "\n",
      "Overall Mean Dice Score: 0.1985\n",
      "\n",
      "Training Loss: 0.8168, Validation Loss: 0.8163\n",
      "========================================================\n",
      "Best model saved with validation loss: 0.8163\n",
      "========================================================\n",
      "Epoch 6/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:46<00:00,  4.43s/it, loss=0.75] \n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.38it/s, loss=0.807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9891, Class 1: 0.0000, Class 2: 0.0000, Class 3: 0.2013, \n",
      "Class 4: 0.0799, Class 5: 0.0144, Class 6: 0.1413, \n",
      "\n",
      "Overall Mean Dice Score: 0.2037\n",
      "\n",
      "Training Loss: 0.7951, Validation Loss: 0.8187\n",
      "Epoch 7/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:46<00:00,  4.44s/it, loss=0.742]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.30it/s, loss=0.838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9869, Class 1: 0.0000, Class 2: 0.0000, Class 3: 0.1869, \n",
      "Class 4: 0.2029, Class 5: 0.0850, Class 6: 0.0888, \n",
      "\n",
      "Overall Mean Dice Score: 0.2215\n",
      "\n",
      "Training Loss: 0.8073, Validation Loss: 0.8003\n",
      "========================================================\n",
      "Best model saved with validation loss: 0.8003\n",
      "========================================================\n",
      "Epoch 8/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:46<00:00,  4.42s/it, loss=0.771]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.38it/s, loss=0.738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9877, Class 1: 0.0000, Class 2: 0.0000, Class 3: 0.2438, \n",
      "Class 4: 0.3417, Class 5: 0.1255, Class 6: 0.0279, \n",
      "\n",
      "Overall Mean Dice Score: 0.2467\n",
      "\n",
      "Training Loss: 0.7745, Validation Loss: 0.7649\n",
      "========================================================\n",
      "Best model saved with validation loss: 0.7649\n",
      "========================================================\n",
      "Epoch 9/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:46<00:00,  4.42s/it, loss=0.768]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.32it/s, loss=0.736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9863, Class 1: 0.0003, Class 2: 0.0000, Class 3: 0.4013, \n",
      "Class 4: 0.2687, Class 5: 0.0856, Class 6: 0.1699, \n",
      "\n",
      "Overall Mean Dice Score: 0.2732\n",
      "\n",
      "Training Loss: 0.7598, Validation Loss: 0.7583\n",
      "========================================================\n",
      "Best model saved with validation loss: 0.7583\n",
      "========================================================\n",
      "Epoch 10/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:46<00:00,  4.44s/it, loss=0.734]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s, loss=0.796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9857, Class 1: 0.0078, Class 2: 0.0000, Class 3: 0.2588, \n",
      "Class 4: 0.1776, Class 5: 0.1345, Class 6: 0.0000, \n",
      "\n",
      "Overall Mean Dice Score: 0.2235\n",
      "\n",
      "Training Loss: 0.7584, Validation Loss: 0.7886\n",
      "Epoch 11/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:47<00:00,  4.46s/it, loss=0.685]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.38it/s, loss=0.806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9822, Class 1: 0.0766, Class 2: 0.0000, Class 3: 0.0241, \n",
      "Class 4: 0.3636, Class 5: 0.0788, Class 6: 0.1830, \n",
      "\n",
      "Overall Mean Dice Score: 0.2440\n",
      "\n",
      "Training Loss: 0.7679, Validation Loss: 0.7793\n",
      "Epoch 12/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:45<00:00,  4.41s/it, loss=0.79] \n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s, loss=0.774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9838, Class 1: 0.0370, Class 2: 0.0000, Class 3: 0.0561, \n",
      "Class 4: 0.1563, Class 5: 0.1237, Class 6: 0.2144, \n",
      "\n",
      "Overall Mean Dice Score: 0.2245\n",
      "\n",
      "Training Loss: 0.7519, Validation Loss: 0.7875\n",
      "Epoch 13/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:45<00:00,  4.38s/it, loss=0.743]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.38it/s, loss=0.686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9858, Class 1: 0.0302, Class 2: 0.0000, Class 3: 0.3600, \n",
      "Class 4: 0.3503, Class 5: 0.1881, Class 6: 0.1710, \n",
      "\n",
      "Overall Mean Dice Score: 0.2979\n",
      "\n",
      "Training Loss: 0.7372, Validation Loss: 0.7274\n",
      "========================================================\n",
      "Best model saved with validation loss: 0.7274\n",
      "========================================================\n",
      "Epoch 14/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:45<00:00,  4.41s/it, loss=0.687]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.34it/s, loss=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9864, Class 1: 0.1398, Class 2: 0.0000, Class 3: 0.0050, \n",
      "Class 4: 0.1591, Class 5: 0.0399, Class 6: 0.0071, \n",
      "\n",
      "Overall Mean Dice Score: 0.1910\n",
      "\n",
      "Training Loss: 0.7598, Validation Loss: 0.8239\n",
      "Epoch 15/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:44<00:00,  4.34s/it, loss=0.783]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.33it/s, loss=0.848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9885, Class 1: 0.0308, Class 2: 0.0000, Class 3: 0.3278, \n",
      "Class 4: 0.2074, Class 5: 0.1206, Class 6: 0.0000, \n",
      "\n",
      "Overall Mean Dice Score: 0.2393\n",
      "\n",
      "Training Loss: 0.7377, Validation Loss: 0.7646\n",
      "Epoch 16/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:45<00:00,  4.38s/it, loss=0.757]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.35it/s, loss=0.738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9871, Class 1: 0.0851, Class 2: 0.0000, Class 3: 0.3174, \n",
      "Class 4: 0.2159, Class 5: 0.0800, Class 6: 0.1004, \n",
      "\n",
      "Overall Mean Dice Score: 0.2551\n",
      "\n",
      "Training Loss: 0.7203, Validation Loss: 0.7710\n",
      "Epoch 17/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:45<00:00,  4.40s/it, loss=0.664]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s, loss=0.845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9869, Class 1: 0.1139, Class 2: 0.0000, Class 3: 0.2645, \n",
      "Class 4: 0.1257, Class 5: 0.1303, Class 6: 0.0000, \n",
      "\n",
      "Overall Mean Dice Score: 0.2316\n",
      "\n",
      "Training Loss: 0.7343, Validation Loss: 0.7936\n",
      "Epoch 18/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24/24 [01:44<00:00,  4.36s/it, loss=0.797]\n",
      "Validation: 100%|██████████| 4/4 [00:01<00:00,  2.36it/s, loss=0.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9813, Class 1: 0.1036, Class 2: 0.0000, Class 3: 0.1761, \n",
      "Class 4: 0.1910, Class 5: 0.1302, Class 6: 0.0000, \n",
      "\n",
      "Overall Mean Dice Score: 0.2260\n",
      "\n",
      "Training Loss: 0.7463, Validation Loss: 0.7922\n",
      "Epoch 19/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 10/24 [00:43<01:01,  4.37s/it, loss=0.713]"
     ]
    }
   ],
   "source": [
    "from monai.metrics import DiceMetric\n",
    "\n",
    "def create_metric_dict(num_classes):\n",
    "    \"\"\"각 클래스별 DiceMetric 생성\"\"\"\n",
    "    metrics = {}\n",
    "    for i in range(num_classes):\n",
    "        metrics[f'dice_class_{i}'] = DiceMetric(\n",
    "            include_background=False if i == 0 else False,\n",
    "            reduction=\"mean\",\n",
    "            get_not_nans=False\n",
    "        )\n",
    "    return metrics\n",
    "    \n",
    "def processing(batch_data, model, criterion, device):\n",
    "    images = batch_data['image'].to(device)  # Input 이미지 (B, 1, 96, 96, 96)\n",
    "    labels = batch_data['label'].to(device)  # 라벨 (B, 96, 96, 96)\n",
    "\n",
    "    labels = labels.squeeze(1)  # (B, 1, 96, 96, 96) → (B, 96, 96, 96)\n",
    "    labels = labels.long()  # 라벨을 정수형으로 변환\n",
    "\n",
    "    # 원핫 인코딩 (B, H, W, D) → (B, num_classes, H, W, D)\n",
    "    \n",
    "    labels_onehot = torch.nn.functional.one_hot(labels, num_classes=n_classes)\n",
    "    labels_onehot = labels_onehot.permute(0, 4, 1, 2, 3).float()  # (B, num_classes, H, W, D)\n",
    "\n",
    "    # 모델 예측\n",
    "    outputs = model(images)  # outputs: (B, num_classes, H, W, D)\n",
    "\n",
    "    # Loss 계산\n",
    "    loss = loss_fn(criterion(outputs, labels_onehot), class_weights)\n",
    "    \n",
    "    return loss, outputs, labels, outputs.argmax(dim=1)\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    with tqdm(train_loader, desc='Training') as pbar:\n",
    "        for batch_data in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            loss, _, _, _ = processing(batch_data, model, criterion, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    wandb.log({'train_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device, epoch, calculate_dice_interval):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    metrics = create_metric_dict(n_classes)\n",
    "    class_dice_scores = {i: [] for i in range(n_classes)}\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc='Validation') as pbar:\n",
    "            for batch_data in pbar:\n",
    "                loss, _, labels, preds = processing(batch_data, model, criterion, device)\n",
    "                val_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "                # 각 클래스별 Dice 점수 계산\n",
    "                if epoch % calculate_dice_interval == 0:\n",
    "                    for i in range(n_classes):\n",
    "                        pred_i = (preds == i)\n",
    "                        label_i = (labels == i)\n",
    "                        dice_score = (2.0 * torch.sum(pred_i & label_i)) / (torch.sum(pred_i) + torch.sum(label_i) + 1e-8)\n",
    "                        class_dice_scores[i].append(dice_score.item())\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    # 에포크별 평균 손실 로깅\n",
    "    wandb.log({'val_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    \n",
    "    # 각 클래스별 평균 Dice 점수 출력\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation Dice Score\")\n",
    "        all_classes_dice_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_dice = np.mean(class_dice_scores[i])\n",
    "            wandb.log({f'class_{i}_dice_score': mean_dice, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_dice:.4f}\", end=\", \")\n",
    "            all_classes_dice_scores.append(mean_dice)\n",
    "            if i == 3:\n",
    "                print()\n",
    "        print()\n",
    "        overall_mean_dice = np.mean(all_classes_dice_scores)\n",
    "        wandb.log({'overall_mean_dice_score': overall_mean_dice, 'epoch': epoch + 1})\n",
    "        print(f\"\\nOverall Mean Dice Score: {overall_mean_dice:.4f}\\n\")\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, \n",
    "    device, start_epoch, best_val_loss, calculate_dice_interval=1\n",
    "):\n",
    "    \"\"\"\n",
    "    모델을 학습하고 검증하는 함수\n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        train_loader: 학습 데이터 로더\n",
    "        val_loader: 검증 데이터 로더\n",
    "        criterion: 손실 함수\n",
    "        optimizer: 최적화 알고리즘\n",
    "        num_epochs: 총 학습 epoch 수\n",
    "        patience: early stopping 기준\n",
    "        device: GPU/CPU 장치\n",
    "        start_epoch: 시작 epoch\n",
    "        best_val_loss: 이전 최적 validation loss\n",
    "        calculate_dice_interval: Dice 점수 계산 주기\n",
    "    \"\"\"\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train One Epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            criterion=criterion, \n",
    "            optimizer=optimizer, \n",
    "            device=device,\n",
    "            epoch=epoch\n",
    "        )\n",
    "\n",
    "        # Validate One Epoch\n",
    "        val_loss = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=epoch, \n",
    "            calculate_dice_interval=calculate_dice_interval\n",
    "        )\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping 및 모델 저장 로직\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"========================================================\")\n",
    "            print(f\"Best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "            print(f\"========================================================\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            \n",
    "            break\n",
    "    wandb.finish()\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=50,\n",
    "    device=device,\n",
    "    start_epoch=start_epoch,\n",
    "    best_val_loss=best_val_loss,\n",
    "    calculate_dice_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.preprocessing import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import Compose, EnsureChannelFirstd, NormalizeIntensityd, Orientationd, GaussianSmoothd\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copick\n",
    "\n",
    "import torch\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file written to ./kaggle/working/copick.config\n",
      "file length: 7\n"
     ]
    }
   ],
   "source": [
    "config_blob = \"\"\"{\n",
    "    \"name\": \"czii_cryoet_mlchallenge_2024\",\n",
    "    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n",
    "    \"version\": \"1.0.0\",\n",
    "\n",
    "    \"pickable_objects\": [\n",
    "        {\n",
    "            \"name\": \"apo-ferritin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"4V1W\",\n",
    "            \"label\": 1,\n",
    "            \"color\": [  0, 117, 220, 128],\n",
    "            \"radius\": 60,\n",
    "            \"map_threshold\": 0.0418\n",
    "        },\n",
    "        {\n",
    "          \"name\" : \"beta-amylase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"8ZRZ\",\n",
    "            \"label\": 2,\n",
    "            \"color\": [255, 255, 255, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578  \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"beta-galactosidase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6X1Q\",\n",
    "            \"label\": 3,\n",
    "            \"color\": [ 76,   0,  92, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ribosome\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6EK0\",\n",
    "            \"label\": 4,\n",
    "            \"color\": [  0,  92,  49, 128],\n",
    "            \"radius\": 150,\n",
    "            \"map_threshold\": 0.0374\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thyroglobulin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6SCJ\",\n",
    "            \"label\": 5,\n",
    "            \"color\": [ 43, 206,  72, 128],\n",
    "            \"radius\": 130,\n",
    "            \"map_threshold\": 0.0278\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"virus-like-particle\",\n",
    "            \"is_particle\": true,\n",
    "            \"label\": 6,\n",
    "            \"color\": [255, 204, 153, 128],\n",
    "            \"radius\": 135,\n",
    "            \"map_threshold\": 0.201\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"membrane\",\n",
    "            \"is_particle\": false,\n",
    "            \"label\": 8,\n",
    "            \"color\": [100, 100, 100, 128]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"background\",\n",
    "            \"is_particle\": false,\n",
    "            \"label\": 9,\n",
    "            \"color\": [10, 150, 200, 128]\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"overlay_root\": \"./kaggle/working/overlay\",\n",
    "\n",
    "    \"overlay_fs_args\": {\n",
    "        \"auto_mkdir\": true\n",
    "    },\n",
    "\n",
    "    \"static_root\": \"./kaggle/input/czii-cryo-et-object-identification/test/static\"\n",
    "}\"\"\"\n",
    "\n",
    "copick_config_path = \"./kaggle/working/copick.config\"\n",
    "preprocessor = Preprocessor(config_blob,copick_config_path=copick_config_path)\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    GaussianSmoothd(\n",
    "        keys=[\"image\"],      # 변환을 적용할 키\n",
    "        sigma=[1.0, 1.0, 1.0]  # 각 축(x, y, z)의 시그마 값\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ship\\Lib\\site-packages\\monai\\utils\\deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_38640\\2937359115.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrain_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = 96\n",
    "img_depth = img_size\n",
    "n_classes = 7 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrain_path = \"./model_checkpoints/SwinUNETR96_96_lr0.001_lambda0.52_batch2/best_model.pt\"\n",
    "model = SwinUNETR(\n",
    "    img_size=(img_depth, img_size, img_size),\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n",
    "# Pretrained weights 불러오기\n",
    "checkpoint = torch.load(pretrain_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=1, \n",
    "            calculate_dice_interval=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 2/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume 3/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to: submission.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import Compose, NormalizeIntensity\n",
    "import cc3d\n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    all_coords = np.vstack(all_coords)\n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "    return df\n",
    "\n",
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              2: \"beta-amylase\",\n",
    "              3: \"beta-galactosidase\", \n",
    "              4: \"ribosome\", \n",
    "              5: \"thyroglobulin\", \n",
    "              6: \"virus-like-particle\"}\n",
    "BLOB_THRESHOLD = 200\n",
    "CERTAINTY_THRESHOLD = 0.05\n",
    "\n",
    "classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    location_dfs = []  # DataFrame 리스트로 초기화\n",
    "    \n",
    "    for vol_idx, run in enumerate(preprocessor.root.runs):\n",
    "        print(f\"Processing volume {vol_idx + 1}/{len(preprocessor.root.runs)}\")\n",
    "        tomogram = preprocessor.processing(run=run, task=\"task\")\n",
    "        task_files = [{\"image\": tomogram}]\n",
    "        task_ds = CacheDataset(data=task_files, transform=non_random_transforms)\n",
    "        task_loader = DataLoader(task_ds, batch_size=1, num_workers=0)\n",
    "        \n",
    "        for task_data in task_loader:\n",
    "            images = task_data['image'].to(\"cuda\")\n",
    "            outputs = sliding_window_inference(\n",
    "                inputs=images,\n",
    "                roi_size=(96, 96, 96),  # ROI 크기\n",
    "                sw_batch_size=4,\n",
    "                predictor=model.forward,\n",
    "                overlap=0.1,\n",
    "                sw_device=\"cuda\",\n",
    "                device=\"cpu\",\n",
    "                buffer_steps=1,\n",
    "                buffer_dim=-1\n",
    "            )\n",
    "            outputs = outputs.argmax(dim=1).squeeze(0).cpu().numpy()  # 클래스 채널 예측\n",
    "            location = {}  # 좌표 저장용 딕셔너리\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(outputs == c)  # cc3d 라벨링\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:] * 10.012444  # 스케일 변환\n",
    "                zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]  # 크기 필터링\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])  # 좌표 스왑 (z, y, x -> x, y, z)\n",
    "\n",
    "                location[id_to_name[c]] = xyz  # ID 이름 매칭 저장\n",
    "\n",
    "            # 데이터프레임 변환\n",
    "            df = dict_to_df(location, run.name)\n",
    "            location_dfs.append(df)  # 리스트에 추가\n",
    "        \n",
    "        # if vol_idx == 2:\n",
    "        #     break\n",
    "    \n",
    "    # DataFrame 병합\n",
    "    final_df = pd.concat(location_dfs, ignore_index=True)\n",
    "    \n",
    "    # ID 추가 및 CSV 저장\n",
    "    final_df.insert(loc=0, column='id', value=np.arange(len(final_df)))\n",
    "    final_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved to: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
