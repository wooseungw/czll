{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.5.1+cu124\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.25.0\n",
      "scipy version: 1.15.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.20.1+cu124\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.1.1\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    ")\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "from src.models import UNet_CBAM\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "print_config()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import TverskyLoss\n",
    "\n",
    "# DynamicTverskyLoss 클래스 정의\n",
    "class DynamicTverskyLoss(TverskyLoss):\n",
    "    def __init__(self, lamda=0.5, **kwargs):\n",
    "        super().__init__(alpha=1 - lamda, beta=lamda, **kwargs)\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.lamda = lamda\n",
    "        self.alpha = 1 - lamda\n",
    "        self.beta = lamda\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombinedCETverskyLoss(nn.Module):\n",
    "    def __init__(self, lamda=0.5, ce_weight=0.5, ds_weights=[0.5, 0.3, 0.2], \n",
    "                 n_classes=7, class_weights=None, ignore_index=-1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.ds_weights = ds_weights  # Deep Supervision 가중치 리스트\n",
    "\n",
    "        # CrossEntropyLoss\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=self.ignore_index, reduction='mean', **kwargs)\n",
    "        \n",
    "        # TverskyLoss\n",
    "        self.tversky = DynamicTverskyLoss(lamda=lamda, reduction=\"mean\", softmax=True, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets, validation=False):\n",
    "        \"\"\"\n",
    "        inputs: Tensor 또는 리스트 (Deep Supervision인 경우 [ds1, ds2, ds3])\n",
    "        targets: 정수형 클래스 인덱스 (CrossEntropyLoss용)\n",
    "        validation: Validation 모드 여부\n",
    "        \"\"\"\n",
    "        # ✅ Validation 모드이거나 리스트가 아닐 경우 첫 번째 출력만 사용 (리스트 방지)\n",
    "        if validation or not isinstance(inputs, list):\n",
    "            return self._compute_loss(inputs[0] if isinstance(inputs, list) else inputs, targets)\n",
    "\n",
    "        # ✅ Deep Supervision 적용 (inputs이 리스트인 경우)\n",
    "        total_loss = 0.0\n",
    "        for i, ds_output in enumerate(inputs):\n",
    "            weight = self.ds_weights[i] if i < len(self.ds_weights) else 1.0\n",
    "            total_loss += weight * self._compute_loss(ds_output, targets)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def _compute_loss(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        CrossEntropyLoss와 TverskyLoss를 조합하여 최종 손실을 계산\n",
    "        \"\"\"\n",
    "        ce_loss = self.ce(inputs, targets)  # ✅ 여기서 inputs이 리스트이면 오류 발생 (방지됨)\n",
    "        tversky_loss = self.tversky(inputs, targets)\n",
    "        return self.ce_weight * ce_loss + (1 - self.ce_weight) * tversky_loss.mean()\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.tversky.set_lamda(lamda)\n",
    "\n",
    "    @property\n",
    "    def lamda(self):\n",
    "        return self.tversky.lamda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 비율: {0: 0.0, 1: 0.16393442622950818, 2: 0.01639344262295082, 3: 0.2459016393442623, 4: 0.16393442622950818, 5: 0.2459016393442623, 6: 0.16393442622950818}\n",
      "최종 합계: 1.0\n",
      "클래스 비율 리스트: [0.0, 0.16393442622950818, 0.01639344262295082, 0.2459016393442623, 0.16393442622950818, 0.2459016393442623, 0.16393442622950818]\n"
     ]
    }
   ],
   "source": [
    "class_info = {\n",
    "    0: {\"name\": \"background\", \"weight\": 0},  # weight 없음\n",
    "    1: {\"name\": \"apo-ferritin\", \"weight\": 1000},\n",
    "    2: {\"name\": \"beta-amylase\", \"weight\": 100}, # 4130\n",
    "    3: {\"name\": \"beta-galactosidase\", \"weight\": 1500}, #3080\n",
    "    4: {\"name\": \"ribosome\", \"weight\": 1000},\n",
    "    5: {\"name\": \"thyroglobulin\", \"weight\": 1500},\n",
    "    6: {\"name\": \"virus-like-particle\", \"weight\": 1000},\n",
    "}\n",
    "\n",
    "# 가중치에 비례한 비율 계산\n",
    "raw_ratios = {\n",
    "    k: (v[\"weight\"] if v[\"weight\"] is not None else 0.01)  # 가중치 비례, None일 경우 기본값a\n",
    "    for k, v in class_info.items()\n",
    "}\n",
    "total = sum(raw_ratios.values())\n",
    "ratios = {k: v / total for k, v in raw_ratios.items()}\n",
    "\n",
    "# 최종 합계가 1인지 확인\n",
    "final_total = sum(ratios.values())\n",
    "print(\"클래스 비율:\", ratios)\n",
    "print(\"최종 합계:\", final_total)\n",
    "\n",
    "# 비율을 리스트로 변환\n",
    "ratios_list = [ratios[k] for k in sorted(ratios.keys())]\n",
    "print(\"클래스 비율 리스트:\", ratios_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from collections.abc import Sequence\n",
    "\n",
    "# 불필요한 import 제거\n",
    "# from monai.networks.blocks.convolutions import Convolution, ResidualUnit\n",
    "# from monai.networks.layers.simplelayers import SkipConnection\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "\n",
    "from src.models.unet_block import Encoder, Decoder, get_conv_layer\n",
    "from src.models.cbam import CBAM3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm3d(nn.Module):\n",
    "    \"\"\"\n",
    "    3D 입력 (N, C, D, H, W)에 대해 layer normalization을 수행하는 모듈입니다.\n",
    "    \n",
    "    정규화는 각 배치 샘플의 모든 채널 및 공간 차원에 대해 진행되며,\n",
    "    공식은 다음과 같습니다.\n",
    "    \n",
    "        y = (x - μ) / sqrt(σ^2 + ε) * γ + β\n",
    "        \n",
    "    여기서,\n",
    "        - μ: 입력 x의 (C, D, H, W) 차원에 대한 평균\n",
    "        - σ^2: 입력 x의 (C, D, H, W) 차원에 대한 분산 (비편향 추정)\n",
    "        - ε: 수치 안정성을 위한 작은 상수 (default: 1e-5)\n",
    "        - γ, β: 학습 가능한 scale과 shift 파라미터 (elementwise_affine=True인 경우)\n",
    "        \n",
    "    Args:\n",
    "        num_channels (int): 입력 텐서의 채널 수. γ와 β의 크기를 결정합니다.\n",
    "        eps (float): 분산 계산 시 분모 안정화를 위한 상수. (default: 1e-5)\n",
    "        elementwise_affine (bool): True이면 γ와 β를 학습 가능한 파라미터로 사용합니다. (default: True)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-5, elementwise_affine=True):\n",
    "        super(LayerNorm3d, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        \n",
    "        if self.elementwise_affine:\n",
    "            # γ와 β는 채널마다 다르게 적용되도록 (1, C, 1, 1, 1) 크기로 초기화합니다.\n",
    "            self.weight = nn.Parameter(torch.ones(1, num_channels, 1, 1, 1))\n",
    "            self.bias = nn.Parameter(torch.zeros(1, num_channels, 1, 1, 1))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 입력 텐서, shape는 (N, C, D, H, W)로 가정합니다.\n",
    "        \"\"\"\n",
    "        # 배치 차원을 제외한 모든 차원(C, D, H, W)에 대해 평균과 분산 계산\n",
    "        mean = x.mean(dim=[1, 2, 3, 4], keepdim=True)\n",
    "        var = x.var(dim=[1, 2, 3, 4], keepdim=True, unbiased=False)\n",
    "        \n",
    "        # 정규화 수행\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # elementwise_affine가 True이면 γ와 β를 곱해줍니다.\n",
    "        if self.elementwise_affine:\n",
    "            x_norm = x_norm * self.weight + self.bias\n",
    "        \n",
    "        return x_norm\n",
    "\n",
    "\n",
    "    \n",
    "from collections.abc import Sequence\n",
    "\n",
    "def autopad(kernel_size: int | Sequence[int], padding: int | Sequence[int] | None = None) -> int | list[int]:\n",
    "    \"\"\"\n",
    "    padding이 None일 경우, kernel_size에 기반해 동일한 출력 shape을 만들기 위한 패딩 크기를 반환합니다.\n",
    "    보통 커널 사이즈가 홀수일 때 kernel_size//2로 설정합니다.\n",
    "    \n",
    "    Args:\n",
    "        kernel_size (int 또는 Sequence[int]): 커널 사이즈.\n",
    "        padding (int 또는 Sequence[int] 또는 None): 이미 지정된 패딩 값. None이면 자동으로 계산합니다.\n",
    "        \n",
    "    Returns:\n",
    "        계산된 padding 값.\n",
    "    \"\"\"\n",
    "    if padding is None:\n",
    "        if isinstance(kernel_size, int):\n",
    "            return kernel_size // 2\n",
    "        elif isinstance(kernel_size, Sequence):\n",
    "            return [k // 2 for k in kernel_size]\n",
    "        else:\n",
    "            raise TypeError(\"kernel_size must be int or sequence of ints\")\n",
    "    else:\n",
    "        return padding\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Sequence[int] | int,\n",
    "        stride: int,\n",
    "        dropout: tuple | str | float | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels = out_channels,\n",
    "            kernel_size= kernel_size,\n",
    "            stride = stride,\n",
    "            padding = autopad(kernel_size)\n",
    "        )\n",
    "        self.norm1 = nn.InstanceNorm3d(out_channels)\n",
    "        self.act1 = nn.PReLU(out_channels)\n",
    "        if dropout is not None:\n",
    "            self.dropout1 = nn.Dropout3d(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm1(x)\n",
    "        if hasattr(self, \"dropout1\"):\n",
    "            x = self.dropout1(x)\n",
    "        x = self.act1(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class c_Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Sequence[int] | int,\n",
    "        stride: int,\n",
    "        dropout: tuple | str | float | None = None,\n",
    "        conv_only = False\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.ConvTranspose3d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=autopad(kernel_size),\n",
    "            output_padding=1 if kernel_size // 2 == 1 else 0,\n",
    "            bias=False,\n",
    "        )\n",
    "        if not conv_only:\n",
    "            self.norm1 = nn.InstanceNorm3d(out_channels)\n",
    "            self.act1 = nn.PReLU(out_channels)\n",
    "            if dropout is not None:\n",
    "                self.dropout1 = nn.Dropout3d(dropout)\n",
    "        self.cbam = CBAM3D(channels=out_channels, reduction=8, spatial_kernel_size=3)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        if not hasattr(self, \"norm1\"):  # conv_only인 경우 norm1, act1, dropout1이 없을 것임\n",
    "            pass\n",
    "        else:\n",
    "            x = self.norm1(x)\n",
    "            x = self.act1(x)\n",
    "            if hasattr(self, \"dropout1\"):\n",
    "                x = self.dropout1(x)\n",
    "\n",
    "        return self.cbam(x)  # norm, act 후 CBAM 적용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# from collections.abc import Sequence\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "\n",
    "# from monai.networks.blocks.convolutions import Convolution\n",
    "# from monai.networks.layers.factories import Act, Norm\n",
    "\n",
    "# # ---------------------------\n",
    "# # 1) LayerNorm3D (옵션)\n",
    "# # ---------------------------\n",
    "# class LayerNorm3D(nn.Module):\n",
    "#     def __init__(self, num_channels: int):\n",
    "#         super().__init__()\n",
    "#         self.layer_norm = nn.LayerNorm(num_channels)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         N, C, *spatial_dims = x.shape\n",
    "#         x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
    "#         x = self.layer_norm(x)\n",
    "#         x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 2) 헬퍼 함수: padding 계산\n",
    "# # ---------------------------\n",
    "# def get_padding(kernel_size: Sequence[int] | int, stride: Sequence[int] | int):\n",
    "#     kernel_size_np = np.atleast_1d(kernel_size)\n",
    "#     stride_np = np.atleast_1d(stride)\n",
    "#     padding_np = (kernel_size_np - stride_np + 1) / 2\n",
    "#     if np.min(padding_np) < 0:\n",
    "#         raise AssertionError(\"padding must not be negative.\")\n",
    "#     padding = tuple(int(p) for p in padding_np)\n",
    "#     return padding if len(padding) > 1 else padding[0]\n",
    "\n",
    "# def get_output_padding(kernel_size, stride, padding):\n",
    "#     kernel_size_np = np.atleast_1d(kernel_size)\n",
    "#     stride_np = np.atleast_1d(stride)\n",
    "#     padding_np = np.atleast_1d(padding)\n",
    "#     out_padding_np = 2 * padding_np + stride_np - kernel_size_np\n",
    "#     if np.min(out_padding_np) < 0:\n",
    "#         raise AssertionError(\"out_padding must not be negative.\")\n",
    "#     out_padding = tuple(int(p) for p in out_padding_np)\n",
    "#     return out_padding if len(out_padding) > 1 else out_padding[0]\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 3) get_conv_layer\n",
    "# # ---------------------------\n",
    "# def get_conv_layer(\n",
    "#     spatial_dims: int,\n",
    "#     in_channels: int,\n",
    "#     out_channels: int,\n",
    "#     kernel_size: int | Sequence[int] = 3,\n",
    "#     stride: int | Sequence[int] = 1,\n",
    "#     act: tuple | str | None = Act.PRELU,\n",
    "#     norm: tuple | str | None = Norm.INSTANCE,\n",
    "#     dropout: float | None = 0.0,\n",
    "#     bias: bool = True,\n",
    "#     conv_only: bool = False,\n",
    "#     is_transposed: bool = False,\n",
    "# ):\n",
    "#     padding = get_padding(kernel_size, stride)\n",
    "#     output_padding = None\n",
    "#     if is_transposed:\n",
    "#         output_padding = get_output_padding(kernel_size, stride, padding)\n",
    "\n",
    "#     return Convolution(\n",
    "#         spatial_dims=spatial_dims,\n",
    "#         in_channels=in_channels,\n",
    "#         out_channels=out_channels,\n",
    "#         strides=stride,\n",
    "#         kernel_size=kernel_size,\n",
    "#         act=act,\n",
    "#         norm=norm,\n",
    "#         dropout=dropout,\n",
    "#         bias=bias,\n",
    "#         conv_only=conv_only,\n",
    "#         is_transposed=is_transposed,\n",
    "#         padding=padding,\n",
    "#         output_padding=output_padding,\n",
    "#     )\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 4) ResizeLayer(nn.Upsample)\n",
    "# # ---------------------------\n",
    "# class ResizeLayer(nn.Module):\n",
    "#     def __init__(self, mode=\"trilinear\", align_corners=False):\n",
    "#         super().__init__()\n",
    "#         self.mode = mode\n",
    "#         self.align_corners = align_corners\n",
    "\n",
    "#     def forward(self, x: torch.Tensor, size: tuple[int, ...]) -> torch.Tensor:\n",
    "#         up = nn.Upsample(size=size, mode=self.mode, align_corners=self.align_corners)\n",
    "#         return up(x)\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 5) SkipAlign\n",
    "# # ---------------------------\n",
    "# class SkipAlign(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         skip_in_channels: int,\n",
    "#         out_channels: int,\n",
    "#         spatial_dims: int = 3,\n",
    "#         channel_match: bool = True,\n",
    "#         mode: str = \"trilinear\",\n",
    "#         align_corners: bool = False,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.spatial_dims = spatial_dims\n",
    "#         self.channel_match = channel_match\n",
    "\n",
    "#         self.upsample = ResizeLayer(mode=mode, align_corners=align_corners)\n",
    "\n",
    "#         if channel_match and (skip_in_channels != out_channels):\n",
    "#             if spatial_dims == 3:\n",
    "#                 self.conv1x1 = nn.Conv3d(skip_in_channels, out_channels, kernel_size=1, bias=True)\n",
    "#             else:\n",
    "#                 self.conv1x1 = nn.Conv2d(skip_in_channels, out_channels, kernel_size=1, bias=True)\n",
    "#         else:\n",
    "#             self.conv1x1 = None\n",
    "\n",
    "#     def forward(self, skip_tensor: torch.Tensor, target_tensor: torch.Tensor) -> torch.Tensor:\n",
    "#         device = target_tensor.device\n",
    "#         dtype = target_tensor.dtype\n",
    "#         skip_tensor = skip_tensor.to(device=device, dtype=dtype)\n",
    "\n",
    "#         # 업샘플\n",
    "#         D_out, H_out, W_out = target_tensor.shape[2:]\n",
    "#         if skip_tensor.shape[2:] != (D_out, H_out, W_out):\n",
    "#             skip_tensor = self.upsample(skip_tensor, (D_out, H_out, W_out))\n",
    "\n",
    "#         # 1×1 Conv\n",
    "#         if self.conv1x1 is not None:\n",
    "#             skip_tensor = self.conv1x1(skip_tensor)\n",
    "\n",
    "#         return skip_tensor\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 6) build_conv_stack\n",
    "# # ---------------------------\n",
    "# def build_conv_stack(\n",
    "#     spatial_dims: int,\n",
    "#     in_channels: int,\n",
    "#     out_channels: int,\n",
    "#     num_layers: int,\n",
    "#     kernel_size: int | Sequence[int],\n",
    "#     stride: int | Sequence[int],\n",
    "#     act: tuple | str | None,\n",
    "#     norm: tuple | str | None,\n",
    "#     dropout: float,\n",
    "#     bias: bool,\n",
    "#     is_transposed: bool = False,\n",
    "#     use_cbam: bool = False,\n",
    "# ):\n",
    "#     layers = []\n",
    "#     for i in range(num_layers):\n",
    "#         if i == 0:\n",
    "#             layers.append(\n",
    "#                 get_conv_layer(\n",
    "#                     spatial_dims=spatial_dims,\n",
    "#                     in_channels=in_channels,\n",
    "#                     out_channels=out_channels,\n",
    "#                     kernel_size=kernel_size,\n",
    "#                     stride=stride,\n",
    "#                     act=act,\n",
    "#                     norm=norm,\n",
    "#                     dropout=dropout,\n",
    "#                     bias=bias,\n",
    "#                     conv_only=False,\n",
    "#                     is_transposed=is_transposed,\n",
    "#                 )\n",
    "#             )\n",
    "#         else:\n",
    "#             layers.append(\n",
    "#                 get_conv_layer(\n",
    "#                     spatial_dims=spatial_dims,\n",
    "#                     in_channels=out_channels,\n",
    "#                     out_channels=out_channels,\n",
    "#                     kernel_size=kernel_size,\n",
    "#                     stride=1,\n",
    "#                     act=act,\n",
    "#                     norm=norm,\n",
    "#                     dropout=dropout,\n",
    "#                     bias=bias,\n",
    "#                     conv_only=False,\n",
    "#                     is_transposed=False,\n",
    "#                 )\n",
    "#             )\n",
    "#         if use_cbam:\n",
    "#             layers.append(CBAM3D(channels=out_channels, reduction=8, spatial_kernel_size=3))\n",
    "#     return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 7) SingleEncoderBlock\n",
    "# # ---------------------------\n",
    "# class SingleEncoderBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         spatial_dims: int,\n",
    "#         in_channels: int,\n",
    "#         out_channels: int,\n",
    "#         num_layers: int,\n",
    "#         kernel_size: int | Sequence[int],\n",
    "#         stride: int | Sequence[int],\n",
    "#         act: tuple | str | None,\n",
    "#         norm: tuple | str | None,\n",
    "#         dropout: float,\n",
    "#         bias: bool = True,\n",
    "#         use_cbam: bool = False,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.stack = build_conv_stack(\n",
    "#             spatial_dims=spatial_dims,\n",
    "#             in_channels=in_channels,\n",
    "#             out_channels=out_channels,\n",
    "#             num_layers=num_layers,\n",
    "#             kernel_size=kernel_size,\n",
    "#             stride=stride,\n",
    "#             act=act,\n",
    "#             norm=norm,\n",
    "#             dropout=dropout,\n",
    "#             bias=bias,\n",
    "#             is_transposed=False,\n",
    "#             use_cbam=use_cbam,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.stack(x)\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 8) SingleDecoderBlock\n",
    "# # ---------------------------\n",
    "# class SingleDecoderBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         spatial_dims: int,\n",
    "#         main_in_channels: int,\n",
    "#         core_channels: int,\n",
    "#         out_channels: int,\n",
    "#         skip_in_channels_list: list[int],\n",
    "#         num_layers: int,\n",
    "#         kernel_size: int | Sequence[int],\n",
    "#         stride: int | Sequence[int],\n",
    "#         act: tuple | str | None,\n",
    "#         norm: tuple | str | None,\n",
    "#         dropout: float,\n",
    "#         bias: bool,\n",
    "#         mode: str = \"trilinear\",\n",
    "#         align_corners: bool = False,\n",
    "#         use_cbam: bool = True,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.spatial_dims = spatial_dims\n",
    "#         self.out_channels = out_channels\n",
    "#         self.skip_count = len(skip_in_channels_list)\n",
    "        \n",
    "#         # (1) Main input channel aligner (1x1 conv로 core_channels로 맞춤)\n",
    "#         if spatial_dims == 3:\n",
    "#             self.main_aligner = nn.Conv3d(main_in_channels, core_channels, kernel_size=1, bias=True)\n",
    "#         else:\n",
    "#             self.main_aligner = nn.Conv2d(main_in_channels, core_channels, kernel_size=1, bias=True)\n",
    "        \n",
    "#         # (2) Skip aligners (채널 매칭을 위한)\n",
    "#         self.skip_aligners = nn.ModuleList()\n",
    "#         for s_in_ch in skip_in_channels_list:\n",
    "#             aligner = SkipAlign(\n",
    "#                 skip_in_channels=s_in_ch,\n",
    "#                 out_channels=core_channels,\n",
    "#                 spatial_dims=spatial_dims,\n",
    "#                 channel_match=True,\n",
    "#                 mode=mode,\n",
    "#                 align_corners=align_corners,\n",
    "#             )\n",
    "#             self.skip_aligners.append(aligner)\n",
    "\n",
    "#         # (3) Main conv stack with transposed conv\n",
    "#         total_in_channels = core_channels * (1 + self.skip_count)  # main(core_ch) + skips(core_ch each)\n",
    "        \n",
    "        \n",
    "#         self.conv_stack = build_conv_stack(\n",
    "#             spatial_dims=spatial_dims,\n",
    "#             in_channels=total_in_channels,\n",
    "#             out_channels=out_channels,\n",
    "#             num_layers=num_layers,  # Already used one layer for upsampling\n",
    "#             kernel_size=kernel_size,\n",
    "#             stride=stride,\n",
    "#             act=act,\n",
    "#             norm=norm,\n",
    "#             dropout=dropout,\n",
    "#             bias=bias,\n",
    "#             is_transposed=True,\n",
    "#             use_cbam=use_cbam,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_main: torch.Tensor, skip_tensors: list[torch.Tensor]) -> torch.Tensor:\n",
    "#         # (1) Main input을 core_channels로 변환\n",
    "#         x_main = self.main_aligner(x_main)  # (N, core_channels, ...)\n",
    "        \n",
    "#         # (2) 모든 skip connection을 현재 입력 크기로 맞춤\n",
    "#         aligned_skips = []\n",
    "        \n",
    "#         for i, s in enumerate(skip_tensors):\n",
    "#             aligned_s = self.skip_aligners[i](s, x_main)  # (N, core_channels, ...)\n",
    "#             aligned_skips.append(aligned_s)\n",
    "            \n",
    "        \n",
    "#         # (3) Concatenate main input with aligned skips\n",
    "#         cat_list = [x_main] + aligned_skips\n",
    "#         cat_input = torch.cat(cat_list, dim=1)  # (N, core_channels * (1 + skip_count), ...)\n",
    "        \n",
    "#         # (4) Apply transposed conv for upsampling\n",
    "#         out = self.conv_stack(cat_input)\n",
    "    \n",
    "#         return out\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 9) FlexibleUNet\n",
    "# # ---------------------------\n",
    "# class FlexibleUNet(nn.Module):\n",
    "#     \"\"\"\n",
    "#     디코더 간 스킵 연결:\n",
    "#       skip_connections = {\n",
    "#          dec_idx: [\n",
    "#            (\"enc\", enc_i),  # 인코더 레벨 enc_i\n",
    "#            (\"dec\", dec_j),  # 디코더 레벨 dec_j\n",
    "#          ],\n",
    "#          ...\n",
    "#       }\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         spatial_dims: int = 3,\n",
    "#         in_channels: int = 1,\n",
    "#         out_channels: int = 2,\n",
    "#         encoder_channels: Sequence[int] = (32, 64, 128, 256),\n",
    "#         encoder_strides: Sequence[int] = (2, 2, 2),\n",
    "#         core_channels: int = 64,\n",
    "#         decoder_channels: Sequence[int] = (128, 64, 32),\n",
    "#         decoder_strides: Sequence[int] = (2, 2, 2),\n",
    "#         num_layers_encoder: Sequence[int] = (1, 1, 1, 1),\n",
    "#         num_layers_decoder: Sequence[int] = (1, 1, 1),\n",
    "#         skip_connections: dict[int, list[tuple[str, int]]] | None = None,\n",
    "#         kernel_size: int | Sequence[int] = 3,\n",
    "#         up_kernel_size: int | Sequence[int] = 3,\n",
    "#         act: tuple | str = Act.PRELU,\n",
    "#         norm: tuple | str = Norm.INSTANCE,\n",
    "#         dropout: float = 0.0,\n",
    "#         bias: bool = True,\n",
    "#         mode: str = \"trilinear\",\n",
    "#         align_corners: bool = False,\n",
    "#         encoder_use_cbam: bool = True,\n",
    "#         decoder_use_cbam: bool = True\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         skip_connections: {\n",
    "#           decoder_index: [(\"enc\", i), (\"dec\", j), ...],\n",
    "#           ...\n",
    "#         }\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         if len(encoder_channels) != len(num_layers_encoder):\n",
    "#             raise ValueError(\"encoder_channels와 num_layers_encoder 길이가 맞지 않습니다.\")\n",
    "#         if len(encoder_strides) != len(encoder_channels) - 1:\n",
    "#             raise ValueError(\"encoder_strides 길이는 (len(encoder_channels) - 1)이어야 합니다.\")\n",
    "#         if len(decoder_channels) != len(num_layers_decoder):\n",
    "#             raise ValueError(\"decoder_channels와 num_layers_decoder 길이가 맞지 않습니다.\")\n",
    "#         if len(decoder_strides) != len(decoder_channels):\n",
    "#             raise ValueError(\"decoder_strides 길이는 len(decoder_channels)와 같아야 합니다.\")\n",
    "\n",
    "#         self.spatial_dims = spatial_dims\n",
    "#         self.skip_connections = skip_connections if skip_connections else {}\n",
    "\n",
    "#         # ---------------------- 인코더 구성 ----------------------\n",
    "#         self.encoder_blocks = nn.ModuleList()\n",
    "#         prev_ch = in_channels\n",
    "#         for i, out_ch in enumerate(encoder_channels):\n",
    "#             stride = encoder_strides[i] if i < len(encoder_strides) else 1\n",
    "#             block = SingleEncoderBlock(\n",
    "#                 spatial_dims=spatial_dims,\n",
    "#                 in_channels=prev_ch,\n",
    "#                 out_channels=out_ch,\n",
    "#                 num_layers=num_layers_encoder[i],\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride,\n",
    "#                 act=act,\n",
    "#                 norm=norm,\n",
    "#                 dropout=dropout,\n",
    "#                 bias=bias,\n",
    "#                 use_cbam=encoder_use_cbam,\n",
    "#             )\n",
    "#             self.encoder_blocks.append(block)\n",
    "#             prev_ch = out_ch\n",
    "\n",
    "#         # ---------------------- 디코더 구성 ----------------------\n",
    "#         self.decoder_blocks = nn.ModuleList()\n",
    "#         main_in_ch = encoder_channels[-1]  # bottleneck\n",
    "#         for dec_i in range(len(decoder_channels)):\n",
    "#             out_ch = decoder_channels[dec_i]\n",
    "#             stride = decoder_strides[dec_i]\n",
    "#             nlayer = num_layers_decoder[dec_i]\n",
    "\n",
    "#             # skip 인덱스 -> skip_in_channels\n",
    "#             # \"enc\" -> encoder_channels[idx], \"dec\" -> decoder_channels[idx]\n",
    "#             skip_info_list = skip_connections.get(dec_i, [])\n",
    "#             skip_in_channels_list = []\n",
    "#             for (typ, idx) in skip_info_list:\n",
    "#                 if typ == \"enc\":\n",
    "#                     skip_in_channels_list.append(encoder_channels[idx])\n",
    "#                 elif typ == \"dec\":\n",
    "#                     # 디코더 레벨 idx의 출력 채널\n",
    "#                     skip_in_channels_list.append(decoder_channels[idx])\n",
    "#                 else:\n",
    "#                     raise ValueError(f\"Invalid skip type: {typ}, must be 'enc' or 'dec'.\")\n",
    "            \n",
    "#             block = SingleDecoderBlock(\n",
    "#                 spatial_dims=spatial_dims,\n",
    "#                 main_in_channels=main_in_ch,\n",
    "#                 out_channels=out_ch,\n",
    "#                 core_channels=core_channels,\n",
    "#                 skip_in_channels_list=skip_in_channels_list,\n",
    "#                 num_layers=nlayer,\n",
    "#                 kernel_size=up_kernel_size,\n",
    "#                 stride=stride,\n",
    "#                 act=act,\n",
    "#                 norm=norm,\n",
    "#                 dropout=dropout,\n",
    "#                 bias=bias,\n",
    "#                 mode=mode,\n",
    "#                 align_corners=align_corners,\n",
    "#                 use_cbam=decoder_use_cbam,\n",
    "#             )\n",
    "#             self.decoder_blocks.append(block)\n",
    "#             main_in_ch = out_ch\n",
    "\n",
    "#         # 최종 Conv\n",
    "#         self.final_conv = get_conv_layer(\n",
    "#             spatial_dims=spatial_dims,\n",
    "#             in_channels=decoder_channels[-1],\n",
    "#             out_channels=out_channels,\n",
    "#             kernel_size=1,\n",
    "#             stride=1,\n",
    "#             norm=None,\n",
    "#             act=None,\n",
    "#             dropout=0.0,\n",
    "#             bias=True,\n",
    "#             conv_only=True,\n",
    "#             is_transposed=False,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # device/dtype\n",
    "#         device = next(self.parameters()).device\n",
    "#         dtype = next(self.parameters()).dtype\n",
    "#         x = x.to(device=device, dtype=dtype)\n",
    "\n",
    "#         # 1) 인코더\n",
    "#         encoder_outputs = []\n",
    "#         out = x\n",
    "#         for enc in self.encoder_blocks:\n",
    "#             out = enc(out)\n",
    "#             encoder_outputs.append(out)\n",
    "\n",
    "#         # 2) 디코더\n",
    "#         decoder_outputs = []\n",
    "#         # out = encoder_outputs[-1]  # bottleneck\n",
    "#         # decoder_outputs.append(out)  # 0번 디코더가 시작하기 전(bottleneck)에 쓸 수도 있지만, 여기선 i=0부터 맞춰줄 수도 있음.\n",
    "\n",
    "#         for dec_i, dec_block in enumerate(self.decoder_blocks):\n",
    "#             # skip에 \"enc\" => encoder_outputs, \"dec\" => decoder_outputs\n",
    "#             skip_info_list = self.skip_connections.get(dec_i, [])\n",
    "#             skip_list = []\n",
    "#             for (typ, idx) in skip_info_list:\n",
    "#                 if typ == \"enc\":\n",
    "#                     skip_list.append(encoder_outputs[idx])\n",
    "#                 elif typ == \"dec\":\n",
    "#                     # 디코더 idx는 0.. dec_i-1 범위여야함\n",
    "#                     skip_list.append(decoder_outputs[idx])\n",
    "#                 else:\n",
    "#                     raise ValueError(f\"Invalid skip type: {typ}.\")\n",
    "\n",
    "#             out = dec_block(out, skip_list)\n",
    "#             # 디코더 i번 블록 결과를 decoder_outputs에 저장\n",
    "#             decoder_outputs.append(out)\n",
    "\n",
    "#         # 3) 최종 Conv\n",
    "#         out = self.final_conv(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 10) 테스트\n",
    "# # ---------------------------\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# #     enc_channels = (32, 64, 128, 256)\n",
    "# #     enc_strides = (2, 2, 2)\n",
    "# #     num_layers_enc = (1, 1, 1, 1)\n",
    "\n",
    "# #     core_channels = 64\n",
    "# #     dec_channels = (256, 256, 256)\n",
    "# #     dec_strides = (2, 2, 2)\n",
    "# #     num_layers_dec = (1, 1, 1)\n",
    "\n",
    "# #     # 디코더간 스킵 예시:\n",
    "# #     #   디코더0: (\"enc\",2), (\"dec\", ?) -- 가능하지만 보통 dec_? < dec_0\n",
    "# #     #   디코더1: (\"enc\",1), (\"dec\",0)\n",
    "# #     #   디코더2: (\"enc\",0), (\"dec\",1)\n",
    "# #     # 반드시 \"dec\", j => j < i 여야함\n",
    "# #     skip_map = {\n",
    "# #         0: [(\"enc\", 2), (\"enc\", 0), (\"enc\", 1)],       # 디코더0 => 인코더2\n",
    "# #         1: [(\"enc\", 3), (\"enc\", 0), (\"enc\", 1)],  # 디코더1 => 인코더1 + 디코더0\n",
    "# #         2: [(\"enc\", 3), (\"dec\", 0), (\"enc\", 0)]   # 디코더2 => 인코더0 + 디코더1\n",
    "# #     }\n",
    "\n",
    "# #     # net = FlexibleUNet(\n",
    "# #     #     spatial_dims=3,\n",
    "# #     #     in_channels=1,\n",
    "# #     #     out_channels=2,\n",
    "# #     #     encoder_channels=enc_channels,\n",
    "# #     #     encoder_strides=enc_strides,\n",
    "# #     #     core_channels=core_channels,\n",
    "# #     #     decoder_channels=dec_channels,\n",
    "# #     #     decoder_strides=dec_strides,\n",
    "# #     #     num_layers_encoder=num_layers_enc,\n",
    "# #     #     num_layers_decoder=num_layers_dec,\n",
    "# #     #     skip_connections=skip_map,\n",
    "# #     #     kernel_size=3,\n",
    "# #     #     up_kernel_size=3,\n",
    "# #     #     dropout=0.0,\n",
    "# #     #     bias=True,\n",
    "# #     #     mode=\"trilinear\",\n",
    "# #     #     align_corners=False,\n",
    "# #     # ).to(device)\n",
    "\n",
    "# #     # x = torch.randn((1, 1, 64, 64, 32), device=device)\n",
    "# #     # with torch.no_grad():\n",
    "# #     #     out = net(x)\n",
    "# #     #     print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm3d 사용 예제\n",
      "입력 텐서 shape: torch.Size([2, 1, 32, 96, 96])\n",
      "출력 텐서 shape: torch.Size([2, 1, 32, 96, 96])\n",
      "\n",
      "Encoder, Decoder, Unet_CBAM_LAYERNORM 사용 예제\n",
      "입력 텐서 shape: torch.Size([2, 1, 32, 96, 96])\n",
      "출력 텐서 shape: torch.Size([2, 4, 32, 96, 96])\n",
      "출력 텐서 shape: torch.Size([2, 4, 32, 96, 96])\n",
      "출력 텐서 shape: torch.Size([2, 4, 32, 96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_25956\\1409511436.py:26: UserWarning: `len(strides) > len(channels) - 1`, the last 1 values of strides will not be used.\n",
      "  warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n",
      "c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "class Unet_CBAM_ds(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        channels: Sequence[int],\n",
    "        strides: Sequence[int],\n",
    "        kernel_size: Sequence[int] | int = 3,\n",
    "        up_kernel_size: Sequence[int] | int = 3,\n",
    "\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "        use_supervision: bool = False,\n",
    "\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if len(channels) < 2:\n",
    "            raise ValueError(\"the length of `channels` should be no less than 2.\")\n",
    "        \n",
    "        # 기존 코드와 동일한 검사\n",
    "        delta = len(strides) - (len(channels) - 1)\n",
    "        if delta < 0:\n",
    "            raise ValueError(\"the length of `strides` should equal `len(channels) - 1`.\")\n",
    "        if delta > 0:\n",
    "            warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_kernel_size = up_kernel_size\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        # Encoder\n",
    "        # ---------------------\n",
    "        self.encoder1 = Encoder(\n",
    "            in_channels,\n",
    "            channels[0],\n",
    "            kernel_size,\n",
    "            strides[0],\n",
    "            dropout,\n",
    "        )\n",
    "        self.encoder2 = Encoder(\n",
    "            channels[0],\n",
    "            channels[1],\n",
    "            kernel_size,\n",
    "            strides[1],\n",
    "            dropout,\n",
    "        )\n",
    "        self.encoder3 = Encoder(\n",
    "            channels[1],\n",
    "            channels[2],\n",
    "            kernel_size,\n",
    "            strides[2],\n",
    "            dropout,\n",
    "        )\n",
    "        # encoder4는 strides[3]를 사용할 수 있도록 strides에 4개 값을 넣어주거나, 아래처럼 stride=1로 따로 설정 가능\n",
    "        # 여기서는 strides에 4개 값을 넣어준다고 가정함\n",
    "        self.bottleneck = Encoder(\n",
    "            channels[2],\n",
    "            channels[3],\n",
    "            kernel_size,\n",
    "            1, \n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "        # self.cbam = CBAM3D(channels=channels[3], reduction=8, spatial_kernel_size=3)\n",
    "        # ---------------------\n",
    "        # Decoder\n",
    "        # ---------------------\n",
    "        self.decoder3 = c_Decoder(\n",
    "            channels[3] + channels[2],\n",
    "            channels[1],\n",
    "            up_kernel_size,\n",
    "            strides[2],\n",
    "            dropout,\n",
    "        )\n",
    "        self.decoder2 = c_Decoder(\n",
    "            channels[1] + channels[1],\n",
    "            channels[0],\n",
    "            up_kernel_size,\n",
    "            strides[1],\n",
    "            dropout,\n",
    "        )\n",
    "        self.decoder1 = c_Decoder(\n",
    "            channels[0] + channels[0],\n",
    "            out_channels,\n",
    "            up_kernel_size,\n",
    "            strides[0],\n",
    "            conv_only=True,\n",
    "        )\n",
    "        if use_supervision:\n",
    "            self.supervision2 = nn.Conv3d(channels[0], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "            self.supervision3 = nn.Conv3d(channels[1], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.encoder3(x2)\n",
    "        \n",
    "        x = self.bottleneck(x3)\n",
    "\n",
    "        # Decoder: 이전 decoder 출력과 skip connection을 제대로 연결\n",
    "        x = self.decoder3(x, x3)   # bottleneck + encoder3\n",
    "        x = self.decoder2(x, x2)    # decoder3 출력 + encoder2\n",
    "        x = self.decoder1(x, x1)    # decoder2 출력 + encoder1\n",
    "        if hasattr(self, \"supervision2\"):\n",
    "            out2 = self.supervision2(x1)\n",
    "            out2 = F.interpolate(out2, x.shape[2:], mode=\"trilinear\", align_corners=False)\n",
    "            out3 = self.supervision3(x2)\n",
    "            out3 = F.interpolate(out3, x.shape[2:], mode=\"trilinear\", align_corners=False)\n",
    "            return [x, out2, out3]\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "# 예제 사용법:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 임의의 3D 데이터: 배치 크기 2, 채널 4, 깊이 8, 높이 16, 너비 16\n",
    "#     print(\"LayerNorm3d 사용 예제\")\n",
    "#     input_tensor = torch.randn(2, 1, 32, 96, 96)\n",
    "#     layer_norm3d = LayerNorm3d(num_channels=1)\n",
    "#     output = layer_norm3d(input_tensor)\n",
    "#     print(\"입력 텐서 shape:\", input_tensor.shape)\n",
    "#     print(\"출력 텐서 shape:\", output.shape)\n",
    "    \n",
    "#     # Encoder, Decoder, Unet_CBAM_LAYERNORM 사용 예제\n",
    "#     print(\"\\nEncoder, Decoder, Unet_CBAM_LAYERNORM 사용 예제\")\n",
    "#     model = Unet_CBAM_ds(\n",
    "#         in_channels=1,\n",
    "#         out_channels=4,\n",
    "#         channels=[16, 32, 64, 128],\n",
    "#         strides=[2, 2, 2, 2],\n",
    "#         kernel_size=3,\n",
    "#         up_kernel_size=3,\n",
    "#         dropout=0.1,\n",
    "#         use_supervision=True,\n",
    "#     )\n",
    "    \n",
    "#     output,ds2,ds3 = model(input_tensor)\n",
    "#     print(\"입력 텐서 shape:\", input_tensor.shape)\n",
    "#     print(\"출력 텐서 shape:\", output.shape)\n",
    "#     print(\"출력 텐서 shape:\", ds2.shape)\n",
    "#     print(\"출력 텐서 shape:\", ds3.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "import torch\n",
    "from torch.profiler import ProfilerActivity\n",
    "from torch.profiler import profile as profilee\n",
    "    \n",
    "\n",
    "def print_model_summary(model, input_size):\n",
    "    input_tensor = torch.randn(input_size)\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    flops, params = profile(model, inputs=(input_tensor,))\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"FLOPs: {flops:,}, GFLOPs: {flops / 1e9:.2f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def profile_model(model, input_size, log_dir='./log'):\n",
    "\n",
    "    input_tensor = torch.randn(input_size)\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    # 프로파일링\n",
    "    with profilee(\n",
    "        activities=[\n",
    "            ProfilerActivity.CPU, \n",
    "            ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(log_dir),  # TensorBoard 연동\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        model(input_tensor)\n",
    "\n",
    "    # 프로파일링 결과 출력\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.dataset import create_dataloaders\n",
    "from src.dataset.dataset_csv import create_dataloaders_from_csv\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, CropForegroundd, GaussianSmoothd, ScaleIntensityd,\n",
    "    RandSpatialCropd, RandRotate90d, RandFlipd, RandGaussianNoised,\n",
    "    ToTensord, RandCropByLabelClassesd, RandCropd,RandCropByPosNegLabeld, RandGaussianSmoothd, RandCoarseDropoutd\n",
    ")\n",
    "from monai.transforms import CastToTyped\n",
    "import numpy as np\n",
    "\n",
    "train_csv = \"./datasets/train_647.csv\"\n",
    "val_csv = \"./datasets/val2.csv\"\n",
    "# DATA CONFIG\n",
    "img_size =  96 # Match your patch size\n",
    "img_depth = 32\n",
    "n_classes = 7\n",
    "batch_size = 32 # 13.8GB GPU memory required for 128x128 img size\n",
    "loader_batch = 1\n",
    "num_samples = batch_size // loader_batch # 한 이미지에서 뽑을 샘플 수\n",
    "num_repeat = 32\n",
    "val_num_repeat = 20\n",
    "# MODEL CONFIG\n",
    "num_epochs = 4000\n",
    "lamda = 0.5\n",
    "ce_weight = 0.4\n",
    "lr = 0.001\n",
    "feature_size = [64, 128, 256, 512]\n",
    "dropout= 0.25\n",
    "use_ds = False\n",
    "# CLASS_WEIGHTS\n",
    "class_weights = None\n",
    "# class_weights = torch.tensor([0.0001, 1, 0.001, 1.1, 1, 1.1, 1], dtype=torch.float32)  # 클래스별 가중치\n",
    "sigma = 1.5\n",
    "\n",
    "ds = [0.6,0.3,0.1]\n",
    "\n",
    "accumulation_steps = 1\n",
    "# INIT\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "best_val_fbeta_score = 0\n",
    "\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    # GaussianSmoothd(\n",
    "    #     keys=[\"image\"],      # 변환을 적용할 키\n",
    "    #     sigma=[sigma, sigma, sigma]  # 각 축(x, y, z)의 시그마 값\n",
    "    #     ),\n",
    "])\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandCoarseDropoutd(\n",
    "        keys=[\"image\"],\n",
    "        holes=3,\n",
    "        spatial_size=(8,8,8),\n",
    "        prob=0.5,\n",
    "        fill_value=255,\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    # RandGaussianSmoothd(\n",
    "    # keys=[\"image\"],      # 변환을 적용할 키\n",
    "    # sigma_x = (0.5, sigma), # 각 축(x, y, z)의 시그마 값\n",
    "    # sigma_y = (0.5, sigma),\n",
    "    # sigma_z = (0.5, sigma),\n",
    "    # prob=0.5,\n",
    "    # ),\n",
    "])\n",
    "val_random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    # RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    # RandGaussianSmoothd(\n",
    "    # keys=[\"image\"],      # 변환을 적용할 키\n",
    "    # sigma_x = (0.0, sigma), # 각 축(x, y, z)의 시그마 값\n",
    "    # sigma_y = (0.0, sigma),\n",
    "    # sigma_z = (0.0, sigma),\n",
    "    # prob=1.0,\n",
    "    # ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 9/9 [00:00<00:00, 12.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = None, None\n",
    "train_loader, val_loader = create_dataloaders_from_csv(\n",
    "    train_csv,\n",
    "    val_csv, \n",
    "    train_non_random_transforms = non_random_transforms, \n",
    "    val_non_random_transforms=non_random_transforms,\n",
    "    train_random_transforms=random_transforms,\n",
    "    val_random_transforms=val_random_transforms,\n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0,train_num_repeat=num_repeat, val_num_repeat=val_num_repeat\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://monai.io/model-zoo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_25956\\1409511436.py:26: UserWarning: `len(strides) > len(channels) - 1`, the last 1 values of strides will not be used.\n",
      "  warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n",
      "c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Unet_CBAM_ds(\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    channels=feature_size,\n",
    "    strides=[2, 2, 2, 2],\n",
    "    kernel_size=3,\n",
    "    up_kernel_size=3,\n",
    "    dropout=dropout,\n",
    "    use_supervision=use_ds,\n",
    "    ).to(device)\n",
    "# x = (batch_size, 1, img_depth, img_size, img_size)\n",
    "# print_model_summary(model, x)\n",
    "# profile_model(model, x, './log')\n",
    "\n",
    "criterion = CombinedCETverskyLoss(\n",
    "    lamda=lamda,\n",
    "    ce_weight=ce_weight,\n",
    "    n_classes=n_classes,\n",
    "    ds_weights=ds,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "weight_str = \"weighted\" if class_weights is not None else \"\"\n",
    "\n",
    "# 체크포인트 디렉토리 및 파일 설정\n",
    "checkpoint_base_dir = Path(\"./model_checkpoints\")\n",
    "folder_name = f\"UNetCBAM_whitehole_denoiisoonly_{img_depth}x{img_size}x{img_size}_e{num_epochs}_lr{lr}_lamda{lamda}_ce{ce_weight}_{weight_str}\"\n",
    "checkpoint_dir = checkpoint_base_dir / folder_name\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "# 체크포인트 디렉토리 생성\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    best_model_path = checkpoint_dir / 'best_model.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"기존 best model 발견: {best_model_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(best_model_path, map_location=device)\n",
    "            # 체크포인트 내부 키 검증\n",
    "            required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch', 'best_val_loss']\n",
    "            if all(k in checkpoint for k in required_keys):\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_val_loss = checkpoint['best_val_loss']\n",
    "                print(\"기존 학습된 가중치를 성공적으로 로드했습니다.\")\n",
    "                checkpoint= None\n",
    "            else:\n",
    "                raise ValueError(\"체크포인트 파일에 필요한 key가 없습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"체크포인트 파일을 로드하는 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(val_loader))\n",
    "# images, labels = batch[\"image\"], batch[\"label\"]\n",
    "# print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwoow070840\u001b[0m (\u001b[33mwaooang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Workspace\\czll\\wandb\\run-20250203_122446-1d8jy5wt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/1d8jy5wt' target=\"_blank\">UNetCBAM_ds_denoiisoonly_32x96x96_e4000_lr0.001_lamda0.5_ce0.4_</a></strong> to <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/1d8jy5wt' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR/runs/1d8jy5wt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = folder_name\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project='czii_SwinUnetR',  # 프로젝트 이름 설정\n",
    "    name=run_name,         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': lamda,\n",
    "        \"cross_entropy_weight\": ce_weight,\n",
    "        'img_depth': img_depth,\n",
    "        'img_size': img_size,\n",
    "        'sampling_ratio': ratios_list,\n",
    "        'device': device.type,\n",
    "        \"checkpoint_dir\": str(checkpoint_dir),\n",
    "        \"class_weights\": class_weights.tolist() if class_weights is not None else None,\n",
    "        \"feature_size\": feature_size,\n",
    "        \"deep_supervision\": use_ds,\n",
    "        \"dropout\": dropout,        \n",
    "        \"accumulation_steps\": accumulation_steps,\n",
    "        \"num_repeat\": num_repeat,\n",
    "        \"ds_weight\": ds,\n",
    "        \n",
    "        # 필요한 하이퍼파라미터 추가\n",
    "    }\n",
    ")\n",
    "# 모델을 wandb에 연결\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric\n",
    "\n",
    "def processing(batch_data, model, criterion,device, val = False):\n",
    "    \n",
    "        \n",
    "    images = batch_data['image'].to(device)  # Input 이미지 (B, 1, 96, 96, 96)\n",
    "    labels = batch_data['label'].to(device)  # 라벨 (B, 96, 96, 96)\n",
    "\n",
    "    labels = labels.squeeze(1)  # (B, 1, 96, 96, 96) → (B, 96, 96, 96)\n",
    "    labels = labels.long()  # 라벨을 정수형으로 변환\n",
    "\n",
    "    # 원핫 인코딩 (B, H, W, D) → (B, num_classes, H, W, D)\n",
    "    \n",
    "    labels_onehot = torch.nn.functional.one_hot(labels, num_classes=n_classes)\n",
    "    labels_onehot = labels_onehot.permute(0, 4, 1, 2, 3).float()  # (B, num_classes, H, W, D)\n",
    "\n",
    "    \n",
    "    outputs = model(images)  # outputs: (B, num_classes, H, W, D)\n",
    "\n",
    "    # Loss 계산\n",
    "    loss = criterion(outputs, labels_onehot,validation = val)\n",
    "        \n",
    "    \n",
    "    # loss = loss_fn(criterion(outputs, labels_onehot),class_weights=class_weights, device=device)\n",
    "    return loss, outputs, labels, outputs[0].argmax(dim=1) if type(outputs) == list else outputs.argmax(dim=1)\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, accumulation_steps=4):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()  # 그래디언트 초기화\n",
    "    with tqdm(train_loader, desc='Training') as pbar:\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "            # 손실 계산\n",
    "            loss, _, _, _ = processing(batch_data, model, criterion, device)\n",
    "\n",
    "            # 그래디언트를 계산하고 누적\n",
    "            loss = loss / accumulation_steps  # 그래디언트 누적을 위한 스케일링\n",
    "            loss.backward()  # 그래디언트 계산 및 누적\n",
    "            \n",
    "            # 그래디언트 업데이트 (accumulation_steps마다 한 번)\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                optimizer.step()  # 파라미터 업데이트\n",
    "                optimizer.zero_grad()  # 누적된 그래디언트 초기화\n",
    "            \n",
    "            # 손실값 누적 (스케일링 복구)\n",
    "            epoch_loss += loss.item() * accumulation_steps  # 실제 손실값 반영\n",
    "            pbar.set_postfix(loss=loss.item() * accumulation_steps)  # 실제 손실값 출력\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    wandb.log({'train_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device, epoch, calculate_dice_interval, ce_weight):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    class_dice_scores = {i: [] for i in range(n_classes)}\n",
    "    class_f_beta_scores = {i: [] for i in range(n_classes)}\n",
    "    class_mIoU_scores = {i: [] for i in range(n_classes)}\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc='Validation') as pbar:\n",
    "            for batch_data in pbar:\n",
    "                loss, _, labels, preds = processing(batch_data, model, criterion, device, val=True)\n",
    "                val_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "                # 각 클래스별 Dice 점수 계산\n",
    "                if epoch % calculate_dice_interval == 0:\n",
    "                    for i in range(n_classes):\n",
    "                        pred_i = (preds == i)\n",
    "                        label_i = (labels == i)\n",
    "                        dice_score = (2.0 * torch.sum(pred_i & label_i)) / (torch.sum(pred_i) + torch.sum(label_i) + 1e-8)\n",
    "                        class_dice_scores[i].append(dice_score.item())\n",
    "                        precision = (torch.sum(pred_i & label_i) + 1e-8) / (torch.sum(pred_i) + 1e-8)\n",
    "                        recall = (torch.sum(pred_i & label_i) + 1e-8) / (torch.sum(label_i) + 1e-8)\n",
    "                        f_beta_score = (1 + 4**2) * (precision * recall) / (4**2 * precision + recall + 1e-8)\n",
    "                        class_f_beta_scores[i].append(f_beta_score.item())\n",
    "                        intersection = torch.sum(pred_i & label_i).float()\n",
    "                        union = torch.sum(pred_i | label_i).float()\n",
    "                        iou = (intersection + 1e-8) / (union + 1e-8)\n",
    "                        class_mIoU_scores[i].append(iou.item())\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    # 에포크별 평균 손실 로깅\n",
    "    wandb.log({'val_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    \n",
    "    # 각 클래스별 평균 Dice 점수 출력\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation Dice Score\")\n",
    "        all_classes_dice_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_dice = np.mean(class_dice_scores[i])\n",
    "            wandb.log({f'class_{i}_dice_score': mean_dice, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_dice:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_dice_scores.append(mean_dice)\n",
    "            \n",
    "        print()\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation F-beta Score\")\n",
    "        all_classes_fbeta_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_fbeta = np.mean(class_f_beta_scores[i])\n",
    "            wandb.log({f'class_{i}_f_beta_score': mean_fbeta, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_fbeta:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_fbeta_scores.append(mean_fbeta)\n",
    "               \n",
    "        print() \n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation mIoU Score\")\n",
    "        all_classes_mIoU_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_IoU = np.mean(class_mIoU_scores[i])\n",
    "            wandb.log({f'class_{i}_IoU_score': mean_fbeta, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_IoU:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_mIoU_scores.append(mean_IoU)\n",
    "                \n",
    "        print()\n",
    "        overall_mean_dice = np.mean(all_classes_dice_scores)\n",
    "        overall_mean_fbeta = np.mean(all_classes_fbeta_scores)\n",
    "        overall_mean_IoU = np.mean(all_classes_mIoU_scores)\n",
    "        wandb.log({'overall_mean_f_beta_score': overall_mean_fbeta, 'overall_mean_dice_score': overall_mean_dice, 'epoch': epoch + 1, 'overall_mean_IoU_score': overall_mean_IoU})\n",
    "        print(f\"\\nOverall Mean Dice Score: {overall_mean_dice:.4f}\\nOverall Mean F-beta Score: {overall_mean_fbeta:.4f}\\nOverall Mean IoU Score: {overall_mean_IoU:.4f}\")\n",
    "\n",
    "    if overall_mean_fbeta is None:\n",
    "        overall_mean_fbeta = 0\n",
    "\n",
    "    final_score = overall_mean_fbeta * (1 - ce_weight) + overall_mean_IoU * ce_weight\n",
    "    return val_loss / len(val_loader), final_score \n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, \n",
    "    device, start_epoch, best_val_loss, best_val_fbeta_score, calculate_dice_interval=1,\n",
    "    accumulation_steps=4, pretrained=False\n",
    "):\n",
    "    \"\"\"\n",
    "    모델을 학습하고 검증하는 함수\n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        train_loader: 학습 데이터 로더\n",
    "        val_loader: 검증 데이터 로더\n",
    "        criterion: 손실 함수\n",
    "        optimizer: 최적화 알고리즘\n",
    "        num_epochs: 총 학습 epoch 수\n",
    "        patience: early stopping 기준\n",
    "        device: GPU/CPU 장치\n",
    "        start_epoch: 시작 epoch\n",
    "        best_val_loss: 이전 최적 validation loss\n",
    "        best_val_fbeta_score: 이전 최적 validation f-beta score\n",
    "        calculate_dice_interval: Dice 점수 계산 주기\n",
    "    \"\"\"\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train One Epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            criterion=criterion, \n",
    "            optimizer=optimizer, \n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            accumulation_steps= accumulation_steps\n",
    "        )\n",
    "        \n",
    "        scheduler.step(train_loss)\n",
    "        # Validate One Epoch\n",
    "        val_loss, overall_mean_fbeta_score = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=epoch, \n",
    "            calculate_dice_interval=calculate_dice_interval,\n",
    "            ce_weight=ce_weight\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F-beta: {overall_mean_fbeta_score:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss and overall_mean_fbeta_score > best_val_fbeta_score:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_fbeta_score = overall_mean_fbeta_score\n",
    "            epochs_no_improve = 0\n",
    "            if pretrained:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model_pretrained.pt')\n",
    "            else:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            print(f\"========================================================\")\n",
    "            print(f\"SUPER Best model saved. Loss:{best_val_loss:.4f}, Score:{best_val_fbeta_score:.4f}\")\n",
    "            print(f\"========================================================\")\n",
    "\n",
    "        # Early stopping 조건 체크\n",
    "        if val_loss >= best_val_loss and overall_mean_fbeta_score <= best_val_fbeta_score:\n",
    "            epochs_no_improve += 1\n",
    "        else:\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'last.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            break\n",
    "        # if epochs_no_improve % 6 == 0 & epochs_no_improve != 0:\n",
    "        #     # 손실이 개선되지 않았으므로 lambda 감소\n",
    "        #     new_lamda = max(criterion.lamda - 0.01, 0.35)  # 최소값은 0.1로 설정\n",
    "        #     criterion.set_lamda(new_lamda)\n",
    "        #     print(f\"Validation loss did not improve. Reducing lambda to {new_lamda:.4f}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [05:10<00:00,  1.24it/s, loss=0.542]\n",
      "Validation: 100%|██████████| 20/20 [00:12<00:00,  1.61it/s, loss=0.561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9779, Class 1: 0.0023, Class 2: 0.0000, Class 3: 0.1173, Class 4: 0.0248, Class 5: 0.0462, Class 6: 0.2367, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9968, Class 1: 0.0012, Class 2: 0.0000, Class 3: 0.0770, Class 4: 0.0134, Class 5: 0.0321, Class 6: 0.1661, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9568, Class 1: 0.0012, Class 2: 0.0000, Class 3: 0.0633, Class 4: 0.0126, Class 5: 0.0238, Class 6: 0.1351, \n",
      "\n",
      "Overall Mean Dice Score: 0.0855\n",
      "Overall Mean F-beta Score: 0.0580\n",
      "Overall Mean IoU Score: 0.0472\n",
      "Training Loss: 0.6189, Validation Loss: 0.5867, Validation F-beta: 0.0536\n",
      "========================================================\n",
      "SUPER Best model saved. Loss:0.5867, Score:0.0536\n",
      "========================================================\n",
      "Epoch 2/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:54<00:00,  1.31it/s, loss=0.502]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.86it/s, loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9819, Class 1: 0.0577, Class 2: 0.0012, Class 3: 0.2045, Class 4: 0.4011, Class 5: 0.2640, Class 6: 0.3913, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9953, Class 1: 0.0325, Class 2: 0.0007, Class 3: 0.1326, Class 4: 0.2662, Class 5: 0.2791, Class 6: 0.2588, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9644, Class 1: 0.0299, Class 2: 0.0006, Class 3: 0.1151, Class 4: 0.2514, Class 5: 0.1539, Class 6: 0.2457, \n",
      "\n",
      "Overall Mean Dice Score: 0.2637\n",
      "Overall Mean F-beta Score: 0.1939\n",
      "Overall Mean IoU Score: 0.1592\n",
      "Training Loss: 0.5205, Validation Loss: 0.5393, Validation F-beta: 0.1800\n",
      "========================================================\n",
      "SUPER Best model saved. Loss:0.5393, Score:0.1800\n",
      "========================================================\n",
      "Epoch 3/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:49<00:00,  1.33it/s, loss=0.499]\n",
      "Validation: 100%|██████████| 20/20 [00:11<00:00,  1.77it/s, loss=0.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9846, Class 1: 0.1021, Class 2: 0.0035, Class 3: 0.2174, Class 4: 0.6113, Class 5: 0.3659, Class 6: 0.5415, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9952, Class 1: 0.0577, Class 2: 0.0019, Class 3: 0.1400, Class 4: 0.4819, Class 5: 0.3245, Class 6: 0.3908, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9696, Class 1: 0.0542, Class 2: 0.0018, Class 3: 0.1240, Class 4: 0.4413, Class 5: 0.2289, Class 6: 0.3731, \n",
      "\n",
      "Overall Mean Dice Score: 0.3676\n",
      "Overall Mean F-beta Score: 0.2790\n",
      "Overall Mean IoU Score: 0.2443\n",
      "Training Loss: 0.5046, Validation Loss: 0.5114, Validation F-beta: 0.2651\n",
      "========================================================\n",
      "SUPER Best model saved. Loss:0.5114, Score:0.2651\n",
      "========================================================\n",
      "Epoch 4/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:49<00:00,  1.33it/s, loss=0.501]\n",
      "Validation: 100%|██████████| 20/20 [00:11<00:00,  1.79it/s, loss=0.507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9851, Class 1: 0.1745, Class 2: 0.0255, Class 3: 0.3038, Class 4: 0.6060, Class 5: 0.3328, Class 6: 0.5507, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9958, Class 1: 0.1023, Class 2: 0.0146, Class 3: 0.2127, Class 4: 0.4654, Class 5: 0.2800, Class 6: 0.4018, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9705, Class 1: 0.0958, Class 2: 0.0133, Class 3: 0.1824, Class 4: 0.4355, Class 5: 0.2016, Class 6: 0.3842, \n",
      "\n",
      "Overall Mean Dice Score: 0.3936\n",
      "Overall Mean F-beta Score: 0.2924\n",
      "Overall Mean IoU Score: 0.2599\n",
      "Training Loss: 0.4979, Validation Loss: 0.5104, Validation F-beta: 0.2794\n",
      "========================================================\n",
      "SUPER Best model saved. Loss:0.5104, Score:0.2794\n",
      "========================================================\n",
      "Epoch 5/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.495]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9849, Class 1: 0.2441, Class 2: 0.0220, Class 3: 0.3375, Class 4: 0.4802, Class 5: 0.3440, Class 6: 0.5825, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9947, Class 1: 0.1508, Class 2: 0.0133, Class 3: 0.2468, Class 4: 0.3340, Class 5: 0.3797, Class 6: 0.4335, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9702, Class 1: 0.1395, Class 2: 0.0113, Class 3: 0.2062, Class 4: 0.3177, Class 5: 0.2098, Class 6: 0.4141, \n",
      "\n",
      "Overall Mean Dice Score: 0.3976\n",
      "Overall Mean F-beta Score: 0.3089\n",
      "Overall Mean IoU Score: 0.2575\n",
      "Training Loss: 0.4928, Validation Loss: 0.5152, Validation F-beta: 0.2883\n",
      "Epoch 6/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.485]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s, loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9863, Class 1: 0.2167, Class 2: 0.0132, Class 3: 0.2798, Class 4: 0.7181, Class 5: 0.3636, Class 6: 0.6106, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9949, Class 1: 0.1316, Class 2: 0.0073, Class 3: 0.1861, Class 4: 0.6296, Class 5: 0.3027, Class 6: 0.4638, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9729, Class 1: 0.1220, Class 2: 0.0068, Class 3: 0.1663, Class 4: 0.5605, Class 5: 0.2271, Class 6: 0.4423, \n",
      "\n",
      "Overall Mean Dice Score: 0.4378\n",
      "Overall Mean F-beta Score: 0.3428\n",
      "Overall Mean IoU Score: 0.3036\n",
      "Training Loss: 0.4898, Validation Loss: 0.4941, Validation F-beta: 0.3271\n",
      "========================================================\n",
      "SUPER Best model saved. Loss:0.4941, Score:0.3271\n",
      "========================================================\n",
      "Epoch 7/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:49<00:00,  1.33it/s, loss=0.486]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s, loss=0.512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9839, Class 1: 0.2641, Class 2: 0.0236, Class 3: 0.3475, Class 4: 0.5041, Class 5: 0.3285, Class 6: 0.5639, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9954, Class 1: 0.1659, Class 2: 0.0135, Class 3: 0.2611, Class 4: 0.3571, Class 5: 0.3126, Class 6: 0.4132, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9683, Class 1: 0.1533, Class 2: 0.0123, Class 3: 0.2152, Class 4: 0.3386, Class 5: 0.1987, Class 6: 0.3939, \n",
      "\n",
      "Overall Mean Dice Score: 0.4016\n",
      "Overall Mean F-beta Score: 0.3020\n",
      "Overall Mean IoU Score: 0.2599\n",
      "Training Loss: 0.4875, Validation Loss: 0.5181, Validation F-beta: 0.2852\n",
      "Epoch 8/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.479]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.93it/s, loss=0.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9862, Class 1: 0.2975, Class 2: 0.0221, Class 3: 0.3063, Class 4: 0.7003, Class 5: 0.3611, Class 6: 0.5701, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9957, Class 1: 0.1900, Class 2: 0.0138, Class 3: 0.2065, Class 4: 0.5790, Class 5: 0.3113, Class 6: 0.4192, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9728, Class 1: 0.1751, Class 2: 0.0116, Class 3: 0.1835, Class 4: 0.5403, Class 5: 0.2233, Class 6: 0.4004, \n",
      "\n",
      "Overall Mean Dice Score: 0.4470\n",
      "Overall Mean F-beta Score: 0.3412\n",
      "Overall Mean IoU Score: 0.3045\n",
      "Training Loss: 0.4847, Validation Loss: 0.5003, Validation F-beta: 0.3265\n",
      "Epoch 9/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.496]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.94it/s, loss=0.512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9856, Class 1: 0.2529, Class 2: 0.0166, Class 3: 0.2860, Class 4: 0.6616, Class 5: 0.3496, Class 6: 0.6087, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9958, Class 1: 0.1577, Class 2: 0.0097, Class 3: 0.2035, Class 4: 0.5330, Class 5: 0.2855, Class 6: 0.4609, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9717, Class 1: 0.1453, Class 2: 0.0087, Class 3: 0.1707, Class 4: 0.4959, Class 5: 0.2144, Class 6: 0.4404, \n",
      "\n",
      "Overall Mean Dice Score: 0.4318\n",
      "Overall Mean F-beta Score: 0.3281\n",
      "Overall Mean IoU Score: 0.2933\n",
      "Training Loss: 0.4831, Validation Loss: 0.5042, Validation F-beta: 0.3142\n",
      "Epoch 10/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.478]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9846, Class 1: 0.2983, Class 2: 0.0091, Class 3: 0.1918, Class 4: 0.6211, Class 5: 0.3242, Class 6: 0.5547, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9968, Class 1: 0.1921, Class 2: 0.0053, Class 3: 0.1213, Class 4: 0.4758, Class 5: 0.2398, Class 6: 0.4058, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9697, Class 1: 0.1760, Class 2: 0.0047, Class 3: 0.1080, Class 4: 0.4511, Class 5: 0.1976, Class 6: 0.3853, \n",
      "\n",
      "Overall Mean Dice Score: 0.3980\n",
      "Overall Mean F-beta Score: 0.2870\n",
      "Overall Mean IoU Score: 0.2636\n",
      "Training Loss: 0.4812, Validation Loss: 0.5129, Validation F-beta: 0.2776\n",
      "Epoch 11/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:49<00:00,  1.33it/s, loss=0.465]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.94it/s, loss=0.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9848, Class 1: 0.2391, Class 2: 0.0259, Class 3: 0.2132, Class 4: 0.6259, Class 5: 0.3205, Class 6: 0.5977, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9953, Class 1: 0.1465, Class 2: 0.0158, Class 3: 0.1365, Class 4: 0.4910, Class 5: 0.2849, Class 6: 0.4514, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9700, Class 1: 0.1362, Class 2: 0.0135, Class 3: 0.1213, Class 4: 0.4568, Class 5: 0.1930, Class 6: 0.4286, \n",
      "\n",
      "Overall Mean Dice Score: 0.3993\n",
      "Overall Mean F-beta Score: 0.3021\n",
      "Overall Mean IoU Score: 0.2672\n",
      "Training Loss: 0.4807, Validation Loss: 0.5125, Validation F-beta: 0.2881\n",
      "Epoch 12/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.477]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.93it/s, loss=0.51] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9855, Class 1: 0.3026, Class 2: 0.0096, Class 3: 0.2240, Class 4: 0.6457, Class 5: 0.3496, Class 6: 0.5554, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9969, Class 1: 0.1941, Class 2: 0.0055, Class 3: 0.1453, Class 4: 0.5022, Class 5: 0.2688, Class 6: 0.4044, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9715, Class 1: 0.1790, Class 2: 0.0050, Class 3: 0.1296, Class 4: 0.4777, Class 5: 0.2162, Class 6: 0.3869, \n",
      "\n",
      "Overall Mean Dice Score: 0.4154\n",
      "Overall Mean F-beta Score: 0.3030\n",
      "Overall Mean IoU Score: 0.2779\n",
      "Training Loss: 0.4793, Validation Loss: 0.5083, Validation F-beta: 0.2929\n",
      "Epoch 13/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.474]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.90it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9836, Class 1: 0.3064, Class 2: 0.0179, Class 3: 0.2555, Class 4: 0.5019, Class 5: 0.2940, Class 6: 0.5353, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9967, Class 1: 0.1985, Class 2: 0.0103, Class 3: 0.1695, Class 4: 0.3527, Class 5: 0.2236, Class 6: 0.3922, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9678, Class 1: 0.1822, Class 2: 0.0092, Class 3: 0.1482, Class 4: 0.3363, Class 5: 0.1743, Class 6: 0.3745, \n",
      "\n",
      "Overall Mean Dice Score: 0.3786\n",
      "Overall Mean F-beta Score: 0.2673\n",
      "Overall Mean IoU Score: 0.2431\n",
      "Training Loss: 0.4780, Validation Loss: 0.5242, Validation F-beta: 0.2576\n",
      "Epoch 14/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.474]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s, loss=0.524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9852, Class 1: 0.3451, Class 2: 0.0516, Class 3: 0.2484, Class 4: 0.6376, Class 5: 0.3752, Class 6: 0.5997, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9963, Class 1: 0.2292, Class 2: 0.0354, Class 3: 0.1621, Class 4: 0.4953, Class 5: 0.3081, Class 6: 0.4527, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9708, Class 1: 0.2093, Class 2: 0.0277, Class 3: 0.1436, Class 4: 0.4693, Class 5: 0.2334, Class 6: 0.4311, \n",
      "\n",
      "Overall Mean Dice Score: 0.4412\n",
      "Overall Mean F-beta Score: 0.3295\n",
      "Overall Mean IoU Score: 0.2973\n",
      "Training Loss: 0.4778, Validation Loss: 0.5072, Validation F-beta: 0.3166\n",
      "Epoch 15/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.469]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s, loss=0.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9858, Class 1: 0.3396, Class 2: 0.0368, Class 3: 0.2601, Class 4: 0.6627, Class 5: 0.3613, Class 6: 0.6155, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9954, Class 1: 0.2241, Class 2: 0.0217, Class 3: 0.1731, Class 4: 0.5270, Class 5: 0.3236, Class 6: 0.4720, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9720, Class 1: 0.2053, Class 2: 0.0192, Class 3: 0.1515, Class 4: 0.4965, Class 5: 0.2229, Class 6: 0.4466, \n",
      "\n",
      "Overall Mean Dice Score: 0.4478\n",
      "Overall Mean F-beta Score: 0.3440\n",
      "Overall Mean IoU Score: 0.3046\n",
      "Training Loss: 0.4759, Validation Loss: 0.5041, Validation F-beta: 0.3282\n",
      "Epoch 16/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:49<00:00,  1.33it/s, loss=0.48] \n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.94it/s, loss=0.503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9864, Class 1: 0.3145, Class 2: 0.0154, Class 3: 0.1829, Class 4: 0.7174, Class 5: 0.3890, Class 6: 0.6473, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9951, Class 1: 0.2051, Class 2: 0.0085, Class 3: 0.1162, Class 4: 0.6131, Class 5: 0.3453, Class 6: 0.5085, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9732, Class 1: 0.1874, Class 2: 0.0079, Class 3: 0.1023, Class 4: 0.5605, Class 5: 0.2441, Class 6: 0.4796, \n",
      "\n",
      "Overall Mean Dice Score: 0.4502\n",
      "Overall Mean F-beta Score: 0.3576\n",
      "Overall Mean IoU Score: 0.3148\n",
      "Training Loss: 0.4750, Validation Loss: 0.4967, Validation F-beta: 0.3405\n",
      "Epoch 17/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:50<00:00,  1.32it/s, loss=0.473]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.93it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9843, Class 1: 0.2827, Class 2: 0.0300, Class 3: 0.1820, Class 4: 0.5524, Class 5: 0.3735, Class 6: 0.5782, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9960, Class 1: 0.1791, Class 2: 0.0187, Class 3: 0.1145, Class 4: 0.4013, Class 5: 0.3353, Class 6: 0.4313, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9690, Class 1: 0.1656, Class 2: 0.0170, Class 3: 0.1030, Class 4: 0.3826, Class 5: 0.2327, Class 6: 0.4081, \n",
      "\n",
      "Overall Mean Dice Score: 0.3938\n",
      "Overall Mean F-beta Score: 0.2923\n",
      "Overall Mean IoU Score: 0.2584\n",
      "Training Loss: 0.4742, Validation Loss: 0.5206, Validation F-beta: 0.2787\n",
      "Epoch 18/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:51<00:00,  1.32it/s, loss=0.479]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9846, Class 1: 0.3584, Class 2: 0.0121, Class 3: 0.2364, Class 4: 0.5916, Class 5: 0.3853, Class 6: 0.5820, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9963, Class 1: 0.2390, Class 2: 0.0068, Class 3: 0.1569, Class 4: 0.4446, Class 5: 0.3166, Class 6: 0.4348, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9697, Class 1: 0.2189, Class 2: 0.0063, Class 3: 0.1389, Class 4: 0.4206, Class 5: 0.2417, Class 6: 0.4120, \n",
      "\n",
      "Overall Mean Dice Score: 0.4307\n",
      "Overall Mean F-beta Score: 0.3184\n",
      "Overall Mean IoU Score: 0.2864\n",
      "Training Loss: 0.4737, Validation Loss: 0.5116, Validation F-beta: 0.3056\n",
      "Epoch 19/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:57<00:00,  1.29it/s, loss=0.474]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.94it/s, loss=0.507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9851, Class 1: 0.3496, Class 2: 0.0266, Class 3: 0.2483, Class 4: 0.6485, Class 5: 0.3265, Class 6: 0.6550, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9958, Class 1: 0.2340, Class 2: 0.0157, Class 3: 0.1638, Class 4: 0.5107, Class 5: 0.2643, Class 6: 0.5184, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9706, Class 1: 0.2128, Class 2: 0.0141, Class 3: 0.1450, Class 4: 0.4805, Class 5: 0.1985, Class 6: 0.4887, \n",
      "\n",
      "Overall Mean Dice Score: 0.4456\n",
      "Overall Mean F-beta Score: 0.3383\n",
      "Overall Mean IoU Score: 0.3051\n",
      "Training Loss: 0.4738, Validation Loss: 0.5053, Validation F-beta: 0.3250\n",
      "Epoch 20/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:53<00:00,  1.31it/s, loss=0.477]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.89it/s, loss=0.496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9869, Class 1: 0.3414, Class 2: 0.0132, Class 3: 0.3267, Class 4: 0.7057, Class 5: 0.3595, Class 6: 0.6588, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9957, Class 1: 0.2260, Class 2: 0.0077, Class 3: 0.2273, Class 4: 0.5958, Class 5: 0.2893, Class 6: 0.5173, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9742, Class 1: 0.2065, Class 2: 0.0069, Class 3: 0.1985, Class 4: 0.5466, Class 5: 0.2215, Class 6: 0.4934, \n",
      "\n",
      "Overall Mean Dice Score: 0.4784\n",
      "Overall Mean F-beta Score: 0.3712\n",
      "Overall Mean IoU Score: 0.3333\n",
      "Training Loss: 0.4723, Validation Loss: 0.4943, Validation F-beta: 0.3560\n",
      "Epoch 21/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:52<00:00,  1.31it/s, loss=0.475]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.90it/s, loss=0.519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9842, Class 1: 0.3421, Class 2: 0.0163, Class 3: 0.2355, Class 4: 0.5092, Class 5: 0.3927, Class 6: 0.5930, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9963, Class 1: 0.2252, Class 2: 0.0111, Class 3: 0.1514, Class 4: 0.3600, Class 5: 0.3444, Class 6: 0.4478, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9689, Class 1: 0.2068, Class 2: 0.0086, Class 3: 0.1360, Class 4: 0.3430, Class 5: 0.2459, Class 6: 0.4253, \n",
      "\n",
      "Overall Mean Dice Score: 0.4145\n",
      "Overall Mean F-beta Score: 0.3057\n",
      "Overall Mean IoU Score: 0.2714\n",
      "Training Loss: 0.4721, Validation Loss: 0.5242, Validation F-beta: 0.2920\n",
      "Epoch 22/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:43<00:00,  1.36it/s, loss=0.475]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.97it/s, loss=0.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9847, Class 1: 0.3706, Class 2: 0.0239, Class 3: 0.2457, Class 4: 0.5712, Class 5: 0.3560, Class 6: 0.6328, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9968, Class 1: 0.2532, Class 2: 0.0155, Class 3: 0.1575, Class 4: 0.4208, Class 5: 0.2678, Class 6: 0.4878, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9699, Class 1: 0.2299, Class 2: 0.0129, Class 3: 0.1423, Class 4: 0.4008, Class 5: 0.2193, Class 6: 0.4641, \n",
      "\n",
      "Overall Mean Dice Score: 0.4353\n",
      "Overall Mean F-beta Score: 0.3174\n",
      "Overall Mean IoU Score: 0.2913\n",
      "Training Loss: 0.4724, Validation Loss: 0.5149, Validation F-beta: 0.3070\n",
      "Epoch 23/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:39<00:00,  1.37it/s, loss=0.48] \n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.96it/s, loss=0.519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9845, Class 1: 0.3841, Class 2: 0.0150, Class 3: 0.2170, Class 4: 0.5045, Class 5: 0.3746, Class 6: 0.6176, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9966, Class 1: 0.2645, Class 2: 0.0089, Class 3: 0.1385, Class 4: 0.3547, Class 5: 0.3032, Class 6: 0.4716, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9695, Class 1: 0.2388, Class 2: 0.0079, Class 3: 0.1231, Class 4: 0.3384, Class 5: 0.2337, Class 6: 0.4492, \n",
      "\n",
      "Overall Mean Dice Score: 0.4196\n",
      "Overall Mean F-beta Score: 0.3065\n",
      "Overall Mean IoU Score: 0.2766\n",
      "Training Loss: 0.4710, Validation Loss: 0.5225, Validation F-beta: 0.2946\n",
      "Epoch 24/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:39<00:00,  1.38it/s, loss=0.468]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9843, Class 1: 0.3626, Class 2: 0.0181, Class 3: 0.2336, Class 4: 0.5225, Class 5: 0.3843, Class 6: 0.6429, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9957, Class 1: 0.2446, Class 2: 0.0105, Class 3: 0.1538, Class 4: 0.3734, Class 5: 0.3507, Class 6: 0.5009, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9691, Class 1: 0.2219, Class 2: 0.0094, Class 3: 0.1344, Class 4: 0.3545, Class 5: 0.2402, Class 6: 0.4759, \n",
      "\n",
      "Overall Mean Dice Score: 0.4292\n",
      "Overall Mean F-beta Score: 0.3247\n",
      "Overall Mean IoU Score: 0.2854\n",
      "Training Loss: 0.4711, Validation Loss: 0.5186, Validation F-beta: 0.3090\n",
      "Epoch 25/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:40<00:00,  1.37it/s, loss=0.477]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9854, Class 1: 0.3709, Class 2: 0.0054, Class 3: 0.2314, Class 4: 0.6003, Class 5: 0.3067, Class 6: 0.6356, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9968, Class 1: 0.2517, Class 2: 0.0030, Class 3: 0.1492, Class 4: 0.4533, Class 5: 0.2229, Class 6: 0.4982, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9712, Class 1: 0.2285, Class 2: 0.0027, Class 3: 0.1352, Class 4: 0.4295, Class 5: 0.1828, Class 6: 0.4679, \n",
      "\n",
      "Overall Mean Dice Score: 0.4290\n",
      "Overall Mean F-beta Score: 0.3151\n",
      "Overall Mean IoU Score: 0.2888\n",
      "Training Loss: 0.4707, Validation Loss: 0.5121, Validation F-beta: 0.3045\n",
      "Epoch 26/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:39<00:00,  1.37it/s, loss=0.469]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.94it/s, loss=0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9856, Class 1: 0.3888, Class 2: 0.0381, Class 3: 0.2440, Class 4: 0.6175, Class 5: 0.3802, Class 6: 0.6445, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9962, Class 1: 0.2648, Class 2: 0.0243, Class 3: 0.1593, Class 4: 0.4762, Class 5: 0.3085, Class 6: 0.5033, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9717, Class 1: 0.2422, Class 2: 0.0203, Class 3: 0.1413, Class 4: 0.4471, Class 5: 0.2386, Class 6: 0.4769, \n",
      "\n",
      "Overall Mean Dice Score: 0.4550\n",
      "Overall Mean F-beta Score: 0.3424\n",
      "Overall Mean IoU Score: 0.3092\n",
      "Training Loss: 0.4700, Validation Loss: 0.5077, Validation F-beta: 0.3292\n",
      "Epoch 27/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:38<00:00,  1.38it/s, loss=0.473]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s, loss=0.519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9850, Class 1: 0.4116, Class 2: 0.0366, Class 3: 0.2012, Class 4: 0.5752, Class 5: 0.3740, Class 6: 0.6226, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9966, Class 1: 0.2883, Class 2: 0.0236, Class 3: 0.1261, Class 4: 0.4238, Class 5: 0.3083, Class 6: 0.4754, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9705, Class 1: 0.2607, Class 2: 0.0195, Class 3: 0.1136, Class 4: 0.4048, Class 5: 0.2335, Class 6: 0.4542, \n",
      "\n",
      "Overall Mean Dice Score: 0.4369\n",
      "Overall Mean F-beta Score: 0.3244\n",
      "Overall Mean IoU Score: 0.2933\n",
      "Training Loss: 0.4693, Validation Loss: 0.5170, Validation F-beta: 0.3120\n",
      "Epoch 28/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:38<00:00,  1.38it/s, loss=0.473]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.97it/s, loss=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9861, Class 1: 0.4141, Class 2: 0.0045, Class 3: 0.2035, Class 4: 0.6699, Class 5: 0.4007, Class 6: 0.7115, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9953, Class 1: 0.2921, Class 2: 0.0025, Class 3: 0.1263, Class 4: 0.5361, Class 5: 0.3664, Class 6: 0.5956, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9725, Class 1: 0.2620, Class 2: 0.0023, Class 3: 0.1149, Class 4: 0.5045, Class 5: 0.2542, Class 6: 0.5537, \n",
      "\n",
      "Overall Mean Dice Score: 0.4799\n",
      "Overall Mean F-beta Score: 0.3833\n",
      "Overall Mean IoU Score: 0.3379\n",
      "Training Loss: 0.4690, Validation Loss: 0.4989, Validation F-beta: 0.3651\n",
      "Epoch 29/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:40<00:00,  1.37it/s, loss=0.47] \n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.96it/s, loss=0.505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9851, Class 1: 0.3732, Class 2: 0.0065, Class 3: 0.2255, Class 4: 0.6071, Class 5: 0.3808, Class 6: 0.6013, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9963, Class 1: 0.2518, Class 2: 0.0036, Class 3: 0.1459, Class 4: 0.4579, Class 5: 0.3211, Class 6: 0.4535, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9706, Class 1: 0.2301, Class 2: 0.0033, Class 3: 0.1283, Class 4: 0.4363, Class 5: 0.2393, Class 6: 0.4320, \n",
      "\n",
      "Overall Mean Dice Score: 0.4376\n",
      "Overall Mean F-beta Score: 0.3260\n",
      "Overall Mean IoU Score: 0.2932\n",
      "Training Loss: 0.4687, Validation Loss: 0.5133, Validation F-beta: 0.3129\n",
      "Epoch 30/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:39<00:00,  1.38it/s, loss=0.481]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  2.00it/s, loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9861, Class 1: 0.3959, Class 2: 0.0445, Class 3: 0.2337, Class 4: 0.6707, Class 5: 0.3694, Class 6: 0.6685, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9957, Class 1: 0.2725, Class 2: 0.0292, Class 3: 0.1554, Class 4: 0.5391, Class 5: 0.3177, Class 6: 0.5375, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9727, Class 1: 0.2474, Class 2: 0.0236, Class 3: 0.1341, Class 4: 0.5054, Class 5: 0.2296, Class 6: 0.5042, \n",
      "\n",
      "Overall Mean Dice Score: 0.4677\n",
      "Overall Mean F-beta Score: 0.3644\n",
      "Overall Mean IoU Score: 0.3241\n",
      "Training Loss: 0.4687, Validation Loss: 0.5012, Validation F-beta: 0.3483\n",
      "Epoch 31/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:39<00:00,  1.37it/s, loss=0.469]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.99it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9841, Class 1: 0.3886, Class 2: 0.0264, Class 3: 0.2065, Class 4: 0.5073, Class 5: 0.3651, Class 6: 0.6125, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9961, Class 1: 0.2649, Class 2: 0.0166, Class 3: 0.1307, Class 4: 0.3575, Class 5: 0.3139, Class 6: 0.4662, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9686, Class 1: 0.2415, Class 2: 0.0138, Class 3: 0.1185, Class 4: 0.3407, Class 5: 0.2274, Class 6: 0.4440, \n",
      "\n",
      "Overall Mean Dice Score: 0.4160\n",
      "Overall Mean F-beta Score: 0.3066\n",
      "Overall Mean IoU Score: 0.2744\n",
      "Training Loss: 0.4685, Validation Loss: 0.5248, Validation F-beta: 0.2937\n",
      "Epoch 32/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:40<00:00,  1.37it/s, loss=0.468]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.97it/s, loss=0.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9847, Class 1: 0.3842, Class 2: 0.0209, Class 3: 0.1961, Class 4: 0.5860, Class 5: 0.3347, Class 6: 0.6207, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9967, Class 1: 0.2652, Class 2: 0.0121, Class 3: 0.1240, Class 4: 0.4374, Class 5: 0.2549, Class 6: 0.4740, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9698, Class 1: 0.2387, Class 2: 0.0110, Class 3: 0.1103, Class 4: 0.4155, Class 5: 0.2028, Class 6: 0.4513, \n",
      "\n",
      "Overall Mean Dice Score: 0.4243\n",
      "Overall Mean F-beta Score: 0.3111\n",
      "Overall Mean IoU Score: 0.2837\n",
      "Training Loss: 0.4679, Validation Loss: 0.5189, Validation F-beta: 0.3001\n",
      "Epoch 33/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:39<00:00,  1.38it/s, loss=0.467]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s, loss=0.51] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9858, Class 1: 0.3573, Class 2: 0.0167, Class 3: 0.1681, Class 4: 0.6610, Class 5: 0.3554, Class 6: 0.6323, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9962, Class 1: 0.2400, Class 2: 0.0108, Class 3: 0.1044, Class 4: 0.5265, Class 5: 0.2815, Class 6: 0.4885, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9720, Class 1: 0.2189, Class 2: 0.0087, Class 3: 0.0940, Class 4: 0.4944, Class 5: 0.2194, Class 6: 0.4643, \n",
      "\n",
      "Overall Mean Dice Score: 0.4348\n",
      "Overall Mean F-beta Score: 0.3282\n",
      "Overall Mean IoU Score: 0.2982\n",
      "Training Loss: 0.4670, Validation Loss: 0.5118, Validation F-beta: 0.3162\n",
      "Epoch 34/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 384/384 [04:39<00:00,  1.38it/s, loss=0.474]\n",
      "Validation: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dice Score\n",
      "Class 0: 0.9856, Class 1: 0.3400, Class 2: 0.0026, Class 3: 0.1814, Class 4: 0.6374, Class 5: 0.3267, Class 6: 0.6232, \n",
      "Validation F-beta Score\n",
      "Class 0: 0.9966, Class 1: 0.2238, Class 2: 0.0015, Class 3: 0.1110, Class 4: 0.5000, Class 5: 0.2490, Class 6: 0.4752, \n",
      "Validation mIoU Score\n",
      "Class 0: 0.9717, Class 1: 0.2055, Class 2: 0.0013, Class 3: 0.1015, Class 4: 0.4688, Class 5: 0.1994, Class 6: 0.4543, \n",
      "\n",
      "Overall Mean Dice Score: 0.4217\n",
      "Overall Mean F-beta Score: 0.3118\n",
      "Overall Mean IoU Score: 0.2859\n",
      "Training Loss: 0.4667, Validation Loss: 0.5169, Validation F-beta: 0.3014\n",
      "Epoch 35/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▎    | 206/384 [02:30<02:19,  1.27it/s, loss=0.459]"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    start_epoch=start_epoch,\n",
    "    best_val_loss=best_val_loss,\n",
    "    best_val_fbeta_score=best_val_fbeta_score,\n",
    "    calculate_dice_interval=1,\n",
    "    accumulation_steps = accumulation_steps\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "czii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
