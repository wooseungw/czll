{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.5.1+cu124\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.25.0\n",
      "scipy version: 1.15.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.20.1+cu124\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.1.1\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    ")\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "from src.models import UNet_CBAM\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "from src.models import UNet_CBAM\n",
    "print_config()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import TverskyLoss\n",
    "\n",
    "# DynamicTverskyLoss 클래스 정의\n",
    "class DynamicTverskyLoss(TverskyLoss):\n",
    "    def __init__(self, lamda=0.5, **kwargs):\n",
    "        super().__init__(alpha=1 - lamda, beta=lamda, **kwargs)\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.lamda = lamda\n",
    "        self.alpha = 1 - lamda\n",
    "        self.beta = lamda\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombinedCETverskyLoss(nn.Module):\n",
    "    def __init__(self, lamda=0.5, ce_weight=0.5, n_classes=7, class_weights=None, ignore_index=-1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "        # CrossEntropyLoss에서 클래스별 가중치를 적용\n",
    "        self.ce = nn.CrossEntropyLoss(weight=class_weights, ignore_index=self.ignore_index, reduction='mean', **kwargs)\n",
    "        \n",
    "        # TverskyLoss\n",
    "        self.tversky = DynamicTverskyLoss(lamda=lamda, reduction=\"mean\",softmax=True, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        # CrossEntropyLoss는 정수형 클래스 인덱스를 사용\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "\n",
    "        # TverskyLoss 계산 (원핫 인코딩된 라벨을 사용)\n",
    "        \n",
    "        tversky_loss = self.tversky(inputs, targets)\n",
    "\n",
    "        # 최종 손실 계산\n",
    "        final_loss = self.ce_weight * ce_loss + (1 - self.ce_weight) * tversky_loss\n",
    "        return final_loss\n",
    "\n",
    "    def set_lamda(self, lamda):\n",
    "        self.tversky.set_lamda(lamda)\n",
    "\n",
    "    @property\n",
    "    def lamda(self):\n",
    "        return self.tversky.lamda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 비율: {0: 0.0, 1: 0.16393442622950818, 2: 0.01639344262295082, 3: 0.2459016393442623, 4: 0.16393442622950818, 5: 0.2459016393442623, 6: 0.16393442622950818}\n",
      "최종 합계: 1.0\n",
      "클래스 비율 리스트: [0.0, 0.16393442622950818, 0.01639344262295082, 0.2459016393442623, 0.16393442622950818, 0.2459016393442623, 0.16393442622950818]\n"
     ]
    }
   ],
   "source": [
    "class_info = {\n",
    "    0: {\"name\": \"background\", \"weight\": 0},  # weight 없음\n",
    "    1: {\"name\": \"apo-ferritin\", \"weight\": 1000},\n",
    "    2: {\"name\": \"beta-amylase\", \"weight\": 100}, # 4130\n",
    "    3: {\"name\": \"beta-galactosidase\", \"weight\": 1500}, #3080\n",
    "    4: {\"name\": \"ribosome\", \"weight\": 1000},\n",
    "    5: {\"name\": \"thyroglobulin\", \"weight\": 1500},\n",
    "    6: {\"name\": \"virus-like-particle\", \"weight\": 1000},\n",
    "}\n",
    "\n",
    "# 가중치에 비례한 비율 계산\n",
    "raw_ratios = {\n",
    "    k: (v[\"weight\"] if v[\"weight\"] is not None else 0.01)  # 가중치 비례, None일 경우 기본값a\n",
    "    for k, v in class_info.items()\n",
    "}\n",
    "total = sum(raw_ratios.values())\n",
    "ratios = {k: v / total for k, v in raw_ratios.items()}\n",
    "\n",
    "# 최종 합계가 1인지 확인\n",
    "final_total = sum(ratios.values())\n",
    "print(\"클래스 비율:\", ratios)\n",
    "print(\"최종 합계:\", final_total)\n",
    "\n",
    "# 비율을 리스트로 변환\n",
    "ratios_list = [ratios[k] for k in sorted(ratios.keys())]\n",
    "print(\"클래스 비율 리스트:\", ratios_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from collections.abc import Sequence\n",
    "\n",
    "# 불필요한 import 제거\n",
    "# from monai.networks.blocks.convolutions import Convolution, ResidualUnit\n",
    "# from monai.networks.layers.simplelayers import SkipConnection\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "\n",
    "from src.models.unet_block import Encoder, Decoder, get_conv_layer\n",
    "from src.models.cbam import CBAM3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm3d(nn.Module):\n",
    "    \"\"\"\n",
    "    3D 입력 (N, C, D, H, W)에 대해 layer normalization을 수행하는 모듈입니다.\n",
    "    \n",
    "    정규화는 각 배치 샘플의 모든 채널 및 공간 차원에 대해 진행되며,\n",
    "    공식은 다음과 같습니다.\n",
    "    \n",
    "        y = (x - μ) / sqrt(σ^2 + ε) * γ + β\n",
    "        \n",
    "    여기서,\n",
    "        - μ: 입력 x의 (C, D, H, W) 차원에 대한 평균\n",
    "        - σ^2: 입력 x의 (C, D, H, W) 차원에 대한 분산 (비편향 추정)\n",
    "        - ε: 수치 안정성을 위한 작은 상수 (default: 1e-5)\n",
    "        - γ, β: 학습 가능한 scale과 shift 파라미터 (elementwise_affine=True인 경우)\n",
    "        \n",
    "    Args:\n",
    "        num_channels (int): 입력 텐서의 채널 수. γ와 β의 크기를 결정합니다.\n",
    "        eps (float): 분산 계산 시 분모 안정화를 위한 상수. (default: 1e-5)\n",
    "        elementwise_affine (bool): True이면 γ와 β를 학습 가능한 파라미터로 사용합니다. (default: True)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-5, elementwise_affine=True):\n",
    "        super(LayerNorm3d, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        \n",
    "        if self.elementwise_affine:\n",
    "            # γ와 β는 채널마다 다르게 적용되도록 (1, C, 1, 1, 1) 크기로 초기화합니다.\n",
    "            self.weight = nn.Parameter(torch.ones(1, num_channels, 1, 1, 1))\n",
    "            self.bias = nn.Parameter(torch.zeros(1, num_channels, 1, 1, 1))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 입력 텐서, shape는 (N, C, D, H, W)로 가정합니다.\n",
    "        \"\"\"\n",
    "        # 배치 차원을 제외한 모든 차원(C, D, H, W)에 대해 평균과 분산 계산\n",
    "        mean = x.mean(dim=[1, 2, 3, 4], keepdim=True)\n",
    "        var = x.var(dim=[1, 2, 3, 4], keepdim=True, unbiased=False)\n",
    "        \n",
    "        # 정규화 수행\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # elementwise_affine가 True이면 γ와 β를 곱해줍니다.\n",
    "        if self.elementwise_affine:\n",
    "            x_norm = x_norm * self.weight + self.bias\n",
    "        \n",
    "        return x_norm\n",
    "\n",
    "\n",
    "    \n",
    "from collections.abc import Sequence\n",
    "\n",
    "def autopad(kernel_size: int | Sequence[int], padding: int | Sequence[int] | None = None) -> int | list[int]:\n",
    "    \"\"\"\n",
    "    padding이 None일 경우, kernel_size에 기반해 동일한 출력 shape을 만들기 위한 패딩 크기를 반환합니다.\n",
    "    보통 커널 사이즈가 홀수일 때 kernel_size//2로 설정합니다.\n",
    "    \n",
    "    Args:\n",
    "        kernel_size (int 또는 Sequence[int]): 커널 사이즈.\n",
    "        padding (int 또는 Sequence[int] 또는 None): 이미 지정된 패딩 값. None이면 자동으로 계산합니다.\n",
    "        \n",
    "    Returns:\n",
    "        계산된 padding 값.\n",
    "    \"\"\"\n",
    "    if padding is None:\n",
    "        if isinstance(kernel_size, int):\n",
    "            return kernel_size // 2\n",
    "        elif isinstance(kernel_size, Sequence):\n",
    "            return [k // 2 for k in kernel_size]\n",
    "        else:\n",
    "            raise TypeError(\"kernel_size must be int or sequence of ints\")\n",
    "    else:\n",
    "        return padding\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Sequence[int] | int,\n",
    "        stride: int,\n",
    "        dropout: tuple | str | float | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels = out_channels,\n",
    "            kernel_size= kernel_size,\n",
    "            stride = stride,\n",
    "            padding = autopad(kernel_size)\n",
    "        )\n",
    "        self.norm1 = nn.InstanceNorm3d(out_channels)\n",
    "        self.act1 = nn.PReLU(out_channels)\n",
    "        if dropout is not None:\n",
    "            self.dropout1 = nn.Dropout3d(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm1(x)\n",
    "        if hasattr(self, \"dropout1\"):\n",
    "            x = self.dropout1(x)\n",
    "        x = self.act1(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class c_Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Sequence[int] | int,\n",
    "        stride: int,\n",
    "        dropout: tuple | str | float | None = None,\n",
    "        conv_only = False\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.ConvTranspose3d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=autopad(kernel_size),\n",
    "            output_padding=1 if kernel_size // 2 == 1 else 0,\n",
    "            bias=False,\n",
    "        )\n",
    "        if not conv_only:\n",
    "            self.norm1 = nn.InstanceNorm3d(out_channels)\n",
    "            self.act1 = nn.PReLU(out_channels)\n",
    "            if dropout is not None:\n",
    "                self.dropout1 = nn.Dropout3d(dropout)\n",
    "        self.cbam = CBAM3D(channels=out_channels, reduction=8, spatial_kernel_size=3)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        if not hasattr(self, \"norm1\"):  # conv_only인 경우 norm1, act1, dropout1이 없을 것임\n",
    "            pass\n",
    "        else:\n",
    "            x = self.norm1(x)\n",
    "            x = self.act1(x)\n",
    "            if hasattr(self, \"dropout1\"):\n",
    "                x = self.dropout1(x)\n",
    "\n",
    "        return self.cbam(x)  # norm, act 후 CBAM 적용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# from collections.abc import Sequence\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "\n",
    "# from monai.networks.blocks.convolutions import Convolution\n",
    "# from monai.networks.layers.factories import Act, Norm\n",
    "\n",
    "# # ---------------------------\n",
    "# # 1) LayerNorm3D (옵션)\n",
    "# # ---------------------------\n",
    "# class LayerNorm3D(nn.Module):\n",
    "#     def __init__(self, num_channels: int):\n",
    "#         super().__init__()\n",
    "#         self.layer_norm = nn.LayerNorm(num_channels)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         N, C, *spatial_dims = x.shape\n",
    "#         x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
    "#         x = self.layer_norm(x)\n",
    "#         x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 2) 헬퍼 함수: padding 계산\n",
    "# # ---------------------------\n",
    "# def get_padding(kernel_size: Sequence[int] | int, stride: Sequence[int] | int):\n",
    "#     kernel_size_np = np.atleast_1d(kernel_size)\n",
    "#     stride_np = np.atleast_1d(stride)\n",
    "#     padding_np = (kernel_size_np - stride_np + 1) / 2\n",
    "#     if np.min(padding_np) < 0:\n",
    "#         raise AssertionError(\"padding must not be negative.\")\n",
    "#     padding = tuple(int(p) for p in padding_np)\n",
    "#     return padding if len(padding) > 1 else padding[0]\n",
    "\n",
    "# def get_output_padding(kernel_size, stride, padding):\n",
    "#     kernel_size_np = np.atleast_1d(kernel_size)\n",
    "#     stride_np = np.atleast_1d(stride)\n",
    "#     padding_np = np.atleast_1d(padding)\n",
    "#     out_padding_np = 2 * padding_np + stride_np - kernel_size_np\n",
    "#     if np.min(out_padding_np) < 0:\n",
    "#         raise AssertionError(\"out_padding must not be negative.\")\n",
    "#     out_padding = tuple(int(p) for p in out_padding_np)\n",
    "#     return out_padding if len(out_padding) > 1 else out_padding[0]\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 3) get_conv_layer\n",
    "# # ---------------------------\n",
    "# def get_conv_layer(\n",
    "#     spatial_dims: int,\n",
    "#     in_channels: int,\n",
    "#     out_channels: int,\n",
    "#     kernel_size: int | Sequence[int] = 3,\n",
    "#     stride: int | Sequence[int] = 1,\n",
    "#     act: tuple | str | None = Act.PRELU,\n",
    "#     norm: tuple | str | None = Norm.INSTANCE,\n",
    "#     dropout: float | None = 0.0,\n",
    "#     bias: bool = True,\n",
    "#     conv_only: bool = False,\n",
    "#     is_transposed: bool = False,\n",
    "# ):\n",
    "#     padding = get_padding(kernel_size, stride)\n",
    "#     output_padding = None\n",
    "#     if is_transposed:\n",
    "#         output_padding = get_output_padding(kernel_size, stride, padding)\n",
    "\n",
    "#     return Convolution(\n",
    "#         spatial_dims=spatial_dims,\n",
    "#         in_channels=in_channels,\n",
    "#         out_channels=out_channels,\n",
    "#         strides=stride,\n",
    "#         kernel_size=kernel_size,\n",
    "#         act=act,\n",
    "#         norm=norm,\n",
    "#         dropout=dropout,\n",
    "#         bias=bias,\n",
    "#         conv_only=conv_only,\n",
    "#         is_transposed=is_transposed,\n",
    "#         padding=padding,\n",
    "#         output_padding=output_padding,\n",
    "#     )\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 4) ResizeLayer(nn.Upsample)\n",
    "# # ---------------------------\n",
    "# class ResizeLayer(nn.Module):\n",
    "#     def __init__(self, mode=\"trilinear\", align_corners=False):\n",
    "#         super().__init__()\n",
    "#         self.mode = mode\n",
    "#         self.align_corners = align_corners\n",
    "\n",
    "#     def forward(self, x: torch.Tensor, size: tuple[int, ...]) -> torch.Tensor:\n",
    "#         up = nn.Upsample(size=size, mode=self.mode, align_corners=self.align_corners)\n",
    "#         return up(x)\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 5) SkipAlign\n",
    "# # ---------------------------\n",
    "# class SkipAlign(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         skip_in_channels: int,\n",
    "#         out_channels: int,\n",
    "#         spatial_dims: int = 3,\n",
    "#         channel_match: bool = True,\n",
    "#         mode: str = \"trilinear\",\n",
    "#         align_corners: bool = False,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.spatial_dims = spatial_dims\n",
    "#         self.channel_match = channel_match\n",
    "\n",
    "#         self.upsample = ResizeLayer(mode=mode, align_corners=align_corners)\n",
    "\n",
    "#         if channel_match and (skip_in_channels != out_channels):\n",
    "#             if spatial_dims == 3:\n",
    "#                 self.conv1x1 = nn.Conv3d(skip_in_channels, out_channels, kernel_size=1, bias=True)\n",
    "#             else:\n",
    "#                 self.conv1x1 = nn.Conv2d(skip_in_channels, out_channels, kernel_size=1, bias=True)\n",
    "#         else:\n",
    "#             self.conv1x1 = None\n",
    "\n",
    "#     def forward(self, skip_tensor: torch.Tensor, target_tensor: torch.Tensor) -> torch.Tensor:\n",
    "#         device = target_tensor.device\n",
    "#         dtype = target_tensor.dtype\n",
    "#         skip_tensor = skip_tensor.to(device=device, dtype=dtype)\n",
    "\n",
    "#         # 업샘플\n",
    "#         D_out, H_out, W_out = target_tensor.shape[2:]\n",
    "#         if skip_tensor.shape[2:] != (D_out, H_out, W_out):\n",
    "#             skip_tensor = self.upsample(skip_tensor, (D_out, H_out, W_out))\n",
    "\n",
    "#         # 1×1 Conv\n",
    "#         if self.conv1x1 is not None:\n",
    "#             skip_tensor = self.conv1x1(skip_tensor)\n",
    "\n",
    "#         return skip_tensor\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 6) build_conv_stack\n",
    "# # ---------------------------\n",
    "# def build_conv_stack(\n",
    "#     spatial_dims: int,\n",
    "#     in_channels: int,\n",
    "#     out_channels: int,\n",
    "#     num_layers: int,\n",
    "#     kernel_size: int | Sequence[int],\n",
    "#     stride: int | Sequence[int],\n",
    "#     act: tuple | str | None,\n",
    "#     norm: tuple | str | None,\n",
    "#     dropout: float,\n",
    "#     bias: bool,\n",
    "#     is_transposed: bool = False,\n",
    "#     use_cbam: bool = False,\n",
    "# ):\n",
    "#     layers = []\n",
    "#     for i in range(num_layers):\n",
    "#         if i == 0:\n",
    "#             layers.append(\n",
    "#                 get_conv_layer(\n",
    "#                     spatial_dims=spatial_dims,\n",
    "#                     in_channels=in_channels,\n",
    "#                     out_channels=out_channels,\n",
    "#                     kernel_size=kernel_size,\n",
    "#                     stride=stride,\n",
    "#                     act=act,\n",
    "#                     norm=norm,\n",
    "#                     dropout=dropout,\n",
    "#                     bias=bias,\n",
    "#                     conv_only=False,\n",
    "#                     is_transposed=is_transposed,\n",
    "#                 )\n",
    "#             )\n",
    "#         else:\n",
    "#             layers.append(\n",
    "#                 get_conv_layer(\n",
    "#                     spatial_dims=spatial_dims,\n",
    "#                     in_channels=out_channels,\n",
    "#                     out_channels=out_channels,\n",
    "#                     kernel_size=kernel_size,\n",
    "#                     stride=1,\n",
    "#                     act=act,\n",
    "#                     norm=norm,\n",
    "#                     dropout=dropout,\n",
    "#                     bias=bias,\n",
    "#                     conv_only=False,\n",
    "#                     is_transposed=False,\n",
    "#                 )\n",
    "#             )\n",
    "#         if use_cbam:\n",
    "#             layers.append(CBAM3D(channels=out_channels, reduction=8, spatial_kernel_size=3))\n",
    "#     return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 7) SingleEncoderBlock\n",
    "# # ---------------------------\n",
    "# class SingleEncoderBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         spatial_dims: int,\n",
    "#         in_channels: int,\n",
    "#         out_channels: int,\n",
    "#         num_layers: int,\n",
    "#         kernel_size: int | Sequence[int],\n",
    "#         stride: int | Sequence[int],\n",
    "#         act: tuple | str | None,\n",
    "#         norm: tuple | str | None,\n",
    "#         dropout: float,\n",
    "#         bias: bool = True,\n",
    "#         use_cbam: bool = False,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.stack = build_conv_stack(\n",
    "#             spatial_dims=spatial_dims,\n",
    "#             in_channels=in_channels,\n",
    "#             out_channels=out_channels,\n",
    "#             num_layers=num_layers,\n",
    "#             kernel_size=kernel_size,\n",
    "#             stride=stride,\n",
    "#             act=act,\n",
    "#             norm=norm,\n",
    "#             dropout=dropout,\n",
    "#             bias=bias,\n",
    "#             is_transposed=False,\n",
    "#             use_cbam=use_cbam,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.stack(x)\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 8) SingleDecoderBlock\n",
    "# # ---------------------------\n",
    "# class SingleDecoderBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         spatial_dims: int,\n",
    "#         main_in_channels: int,\n",
    "#         core_channels: int,\n",
    "#         out_channels: int,\n",
    "#         skip_in_channels_list: list[int],\n",
    "#         num_layers: int,\n",
    "#         kernel_size: int | Sequence[int],\n",
    "#         stride: int | Sequence[int],\n",
    "#         act: tuple | str | None,\n",
    "#         norm: tuple | str | None,\n",
    "#         dropout: float,\n",
    "#         bias: bool,\n",
    "#         mode: str = \"trilinear\",\n",
    "#         align_corners: bool = False,\n",
    "#         use_cbam: bool = True,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.spatial_dims = spatial_dims\n",
    "#         self.out_channels = out_channels\n",
    "#         self.skip_count = len(skip_in_channels_list)\n",
    "        \n",
    "#         # (1) Main input channel aligner (1x1 conv로 core_channels로 맞춤)\n",
    "#         if spatial_dims == 3:\n",
    "#             self.main_aligner = nn.Conv3d(main_in_channels, core_channels, kernel_size=1, bias=True)\n",
    "#         else:\n",
    "#             self.main_aligner = nn.Conv2d(main_in_channels, core_channels, kernel_size=1, bias=True)\n",
    "        \n",
    "#         # (2) Skip aligners (채널 매칭을 위한)\n",
    "#         self.skip_aligners = nn.ModuleList()\n",
    "#         for s_in_ch in skip_in_channels_list:\n",
    "#             aligner = SkipAlign(\n",
    "#                 skip_in_channels=s_in_ch,\n",
    "#                 out_channels=core_channels,\n",
    "#                 spatial_dims=spatial_dims,\n",
    "#                 channel_match=True,\n",
    "#                 mode=mode,\n",
    "#                 align_corners=align_corners,\n",
    "#             )\n",
    "#             self.skip_aligners.append(aligner)\n",
    "\n",
    "#         # (3) Main conv stack with transposed conv\n",
    "#         total_in_channels = core_channels * (1 + self.skip_count)  # main(core_ch) + skips(core_ch each)\n",
    "        \n",
    "        \n",
    "#         self.conv_stack = build_conv_stack(\n",
    "#             spatial_dims=spatial_dims,\n",
    "#             in_channels=total_in_channels,\n",
    "#             out_channels=out_channels,\n",
    "#             num_layers=num_layers,  # Already used one layer for upsampling\n",
    "#             kernel_size=kernel_size,\n",
    "#             stride=stride,\n",
    "#             act=act,\n",
    "#             norm=norm,\n",
    "#             dropout=dropout,\n",
    "#             bias=bias,\n",
    "#             is_transposed=True,\n",
    "#             use_cbam=use_cbam,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_main: torch.Tensor, skip_tensors: list[torch.Tensor]) -> torch.Tensor:\n",
    "#         # (1) Main input을 core_channels로 변환\n",
    "#         x_main = self.main_aligner(x_main)  # (N, core_channels, ...)\n",
    "        \n",
    "#         # (2) 모든 skip connection을 현재 입력 크기로 맞춤\n",
    "#         aligned_skips = []\n",
    "        \n",
    "#         for i, s in enumerate(skip_tensors):\n",
    "#             aligned_s = self.skip_aligners[i](s, x_main)  # (N, core_channels, ...)\n",
    "#             aligned_skips.append(aligned_s)\n",
    "            \n",
    "        \n",
    "#         # (3) Concatenate main input with aligned skips\n",
    "#         cat_list = [x_main] + aligned_skips\n",
    "#         cat_input = torch.cat(cat_list, dim=1)  # (N, core_channels * (1 + skip_count), ...)\n",
    "        \n",
    "#         # (4) Apply transposed conv for upsampling\n",
    "#         out = self.conv_stack(cat_input)\n",
    "    \n",
    "#         return out\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 9) FlexibleUNet\n",
    "# # ---------------------------\n",
    "# class FlexibleUNet(nn.Module):\n",
    "#     \"\"\"\n",
    "#     디코더 간 스킵 연결:\n",
    "#       skip_connections = {\n",
    "#          dec_idx: [\n",
    "#            (\"enc\", enc_i),  # 인코더 레벨 enc_i\n",
    "#            (\"dec\", dec_j),  # 디코더 레벨 dec_j\n",
    "#          ],\n",
    "#          ...\n",
    "#       }\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         spatial_dims: int = 3,\n",
    "#         in_channels: int = 1,\n",
    "#         out_channels: int = 2,\n",
    "#         encoder_channels: Sequence[int] = (32, 64, 128, 256),\n",
    "#         encoder_strides: Sequence[int] = (2, 2, 2),\n",
    "#         core_channels: int = 64,\n",
    "#         decoder_channels: Sequence[int] = (128, 64, 32),\n",
    "#         decoder_strides: Sequence[int] = (2, 2, 2),\n",
    "#         num_layers_encoder: Sequence[int] = (1, 1, 1, 1),\n",
    "#         num_layers_decoder: Sequence[int] = (1, 1, 1),\n",
    "#         skip_connections: dict[int, list[tuple[str, int]]] | None = None,\n",
    "#         kernel_size: int | Sequence[int] = 3,\n",
    "#         up_kernel_size: int | Sequence[int] = 3,\n",
    "#         act: tuple | str = Act.PRELU,\n",
    "#         norm: tuple | str = Norm.INSTANCE,\n",
    "#         dropout: float = 0.0,\n",
    "#         bias: bool = True,\n",
    "#         mode: str = \"trilinear\",\n",
    "#         align_corners: bool = False,\n",
    "#         encoder_use_cbam: bool = True,\n",
    "#         decoder_use_cbam: bool = True\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         skip_connections: {\n",
    "#           decoder_index: [(\"enc\", i), (\"dec\", j), ...],\n",
    "#           ...\n",
    "#         }\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         if len(encoder_channels) != len(num_layers_encoder):\n",
    "#             raise ValueError(\"encoder_channels와 num_layers_encoder 길이가 맞지 않습니다.\")\n",
    "#         if len(encoder_strides) != len(encoder_channels) - 1:\n",
    "#             raise ValueError(\"encoder_strides 길이는 (len(encoder_channels) - 1)이어야 합니다.\")\n",
    "#         if len(decoder_channels) != len(num_layers_decoder):\n",
    "#             raise ValueError(\"decoder_channels와 num_layers_decoder 길이가 맞지 않습니다.\")\n",
    "#         if len(decoder_strides) != len(decoder_channels):\n",
    "#             raise ValueError(\"decoder_strides 길이는 len(decoder_channels)와 같아야 합니다.\")\n",
    "\n",
    "#         self.spatial_dims = spatial_dims\n",
    "#         self.skip_connections = skip_connections if skip_connections else {}\n",
    "\n",
    "#         # ---------------------- 인코더 구성 ----------------------\n",
    "#         self.encoder_blocks = nn.ModuleList()\n",
    "#         prev_ch = in_channels\n",
    "#         for i, out_ch in enumerate(encoder_channels):\n",
    "#             stride = encoder_strides[i] if i < len(encoder_strides) else 1\n",
    "#             block = SingleEncoderBlock(\n",
    "#                 spatial_dims=spatial_dims,\n",
    "#                 in_channels=prev_ch,\n",
    "#                 out_channels=out_ch,\n",
    "#                 num_layers=num_layers_encoder[i],\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride,\n",
    "#                 act=act,\n",
    "#                 norm=norm,\n",
    "#                 dropout=dropout,\n",
    "#                 bias=bias,\n",
    "#                 use_cbam=encoder_use_cbam,\n",
    "#             )\n",
    "#             self.encoder_blocks.append(block)\n",
    "#             prev_ch = out_ch\n",
    "\n",
    "#         # ---------------------- 디코더 구성 ----------------------\n",
    "#         self.decoder_blocks = nn.ModuleList()\n",
    "#         main_in_ch = encoder_channels[-1]  # bottleneck\n",
    "#         for dec_i in range(len(decoder_channels)):\n",
    "#             out_ch = decoder_channels[dec_i]\n",
    "#             stride = decoder_strides[dec_i]\n",
    "#             nlayer = num_layers_decoder[dec_i]\n",
    "\n",
    "#             # skip 인덱스 -> skip_in_channels\n",
    "#             # \"enc\" -> encoder_channels[idx], \"dec\" -> decoder_channels[idx]\n",
    "#             skip_info_list = skip_connections.get(dec_i, [])\n",
    "#             skip_in_channels_list = []\n",
    "#             for (typ, idx) in skip_info_list:\n",
    "#                 if typ == \"enc\":\n",
    "#                     skip_in_channels_list.append(encoder_channels[idx])\n",
    "#                 elif typ == \"dec\":\n",
    "#                     # 디코더 레벨 idx의 출력 채널\n",
    "#                     skip_in_channels_list.append(decoder_channels[idx])\n",
    "#                 else:\n",
    "#                     raise ValueError(f\"Invalid skip type: {typ}, must be 'enc' or 'dec'.\")\n",
    "            \n",
    "#             block = SingleDecoderBlock(\n",
    "#                 spatial_dims=spatial_dims,\n",
    "#                 main_in_channels=main_in_ch,\n",
    "#                 out_channels=out_ch,\n",
    "#                 core_channels=core_channels,\n",
    "#                 skip_in_channels_list=skip_in_channels_list,\n",
    "#                 num_layers=nlayer,\n",
    "#                 kernel_size=up_kernel_size,\n",
    "#                 stride=stride,\n",
    "#                 act=act,\n",
    "#                 norm=norm,\n",
    "#                 dropout=dropout,\n",
    "#                 bias=bias,\n",
    "#                 mode=mode,\n",
    "#                 align_corners=align_corners,\n",
    "#                 use_cbam=decoder_use_cbam,\n",
    "#             )\n",
    "#             self.decoder_blocks.append(block)\n",
    "#             main_in_ch = out_ch\n",
    "\n",
    "#         # 최종 Conv\n",
    "#         self.final_conv = get_conv_layer(\n",
    "#             spatial_dims=spatial_dims,\n",
    "#             in_channels=decoder_channels[-1],\n",
    "#             out_channels=out_channels,\n",
    "#             kernel_size=1,\n",
    "#             stride=1,\n",
    "#             norm=None,\n",
    "#             act=None,\n",
    "#             dropout=0.0,\n",
    "#             bias=True,\n",
    "#             conv_only=True,\n",
    "#             is_transposed=False,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # device/dtype\n",
    "#         device = next(self.parameters()).device\n",
    "#         dtype = next(self.parameters()).dtype\n",
    "#         x = x.to(device=device, dtype=dtype)\n",
    "\n",
    "#         # 1) 인코더\n",
    "#         encoder_outputs = []\n",
    "#         out = x\n",
    "#         for enc in self.encoder_blocks:\n",
    "#             out = enc(out)\n",
    "#             encoder_outputs.append(out)\n",
    "\n",
    "#         # 2) 디코더\n",
    "#         decoder_outputs = []\n",
    "#         # out = encoder_outputs[-1]  # bottleneck\n",
    "#         # decoder_outputs.append(out)  # 0번 디코더가 시작하기 전(bottleneck)에 쓸 수도 있지만, 여기선 i=0부터 맞춰줄 수도 있음.\n",
    "\n",
    "#         for dec_i, dec_block in enumerate(self.decoder_blocks):\n",
    "#             # skip에 \"enc\" => encoder_outputs, \"dec\" => decoder_outputs\n",
    "#             skip_info_list = self.skip_connections.get(dec_i, [])\n",
    "#             skip_list = []\n",
    "#             for (typ, idx) in skip_info_list:\n",
    "#                 if typ == \"enc\":\n",
    "#                     skip_list.append(encoder_outputs[idx])\n",
    "#                 elif typ == \"dec\":\n",
    "#                     # 디코더 idx는 0.. dec_i-1 범위여야함\n",
    "#                     skip_list.append(decoder_outputs[idx])\n",
    "#                 else:\n",
    "#                     raise ValueError(f\"Invalid skip type: {typ}.\")\n",
    "\n",
    "#             out = dec_block(out, skip_list)\n",
    "#             # 디코더 i번 블록 결과를 decoder_outputs에 저장\n",
    "#             decoder_outputs.append(out)\n",
    "\n",
    "#         # 3) 최종 Conv\n",
    "#         out = self.final_conv(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # 10) 테스트\n",
    "# # ---------------------------\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# #     enc_channels = (32, 64, 128, 256)\n",
    "# #     enc_strides = (2, 2, 2)\n",
    "# #     num_layers_enc = (1, 1, 1, 1)\n",
    "\n",
    "# #     core_channels = 64\n",
    "# #     dec_channels = (256, 256, 256)\n",
    "# #     dec_strides = (2, 2, 2)\n",
    "# #     num_layers_dec = (1, 1, 1)\n",
    "\n",
    "# #     # 디코더간 스킵 예시:\n",
    "# #     #   디코더0: (\"enc\",2), (\"dec\", ?) -- 가능하지만 보통 dec_? < dec_0\n",
    "# #     #   디코더1: (\"enc\",1), (\"dec\",0)\n",
    "# #     #   디코더2: (\"enc\",0), (\"dec\",1)\n",
    "# #     # 반드시 \"dec\", j => j < i 여야함\n",
    "# #     skip_map = {\n",
    "# #         0: [(\"enc\", 2), (\"enc\", 0), (\"enc\", 1)],       # 디코더0 => 인코더2\n",
    "# #         1: [(\"enc\", 3), (\"enc\", 0), (\"enc\", 1)],  # 디코더1 => 인코더1 + 디코더0\n",
    "# #         2: [(\"enc\", 3), (\"dec\", 0), (\"enc\", 0)]   # 디코더2 => 인코더0 + 디코더1\n",
    "# #     }\n",
    "\n",
    "# #     # net = FlexibleUNet(\n",
    "# #     #     spatial_dims=3,\n",
    "# #     #     in_channels=1,\n",
    "# #     #     out_channels=2,\n",
    "# #     #     encoder_channels=enc_channels,\n",
    "# #     #     encoder_strides=enc_strides,\n",
    "# #     #     core_channels=core_channels,\n",
    "# #     #     decoder_channels=dec_channels,\n",
    "# #     #     decoder_strides=dec_strides,\n",
    "# #     #     num_layers_encoder=num_layers_enc,\n",
    "# #     #     num_layers_decoder=num_layers_dec,\n",
    "# #     #     skip_connections=skip_map,\n",
    "# #     #     kernel_size=3,\n",
    "# #     #     up_kernel_size=3,\n",
    "# #     #     dropout=0.0,\n",
    "# #     #     bias=True,\n",
    "# #     #     mode=\"trilinear\",\n",
    "# #     #     align_corners=False,\n",
    "# #     # ).to(device)\n",
    "\n",
    "# #     # x = torch.randn((1, 1, 64, 64, 32), device=device)\n",
    "# #     # with torch.no_grad():\n",
    "# #     #     out = net(x)\n",
    "# #     #     print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet_CBAM_ds(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        channels: Sequence[int],\n",
    "        strides: Sequence[int],\n",
    "        kernel_size: Sequence[int] | int = 3,\n",
    "        up_kernel_size: Sequence[int] | int = 3,\n",
    "\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "        use_supervision: bool = False,\n",
    "\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if len(channels) < 2:\n",
    "            raise ValueError(\"the length of `channels` should be no less than 2.\")\n",
    "        \n",
    "        # 기존 코드와 동일한 검사\n",
    "        delta = len(strides) - (len(channels) - 1)\n",
    "        if delta < 0:\n",
    "            raise ValueError(\"the length of `strides` should equal `len(channels) - 1`.\")\n",
    "        if delta > 0:\n",
    "            warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_kernel_size = up_kernel_size\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        # Encoder\n",
    "        # ---------------------\n",
    "        self.encoder1 = Encoder(\n",
    "            in_channels,\n",
    "            channels[0],\n",
    "            kernel_size,\n",
    "            strides[0],\n",
    "            dropout,\n",
    "        )\n",
    "        self.encoder2 = Encoder(\n",
    "            channels[0],\n",
    "            channels[1],\n",
    "            kernel_size,\n",
    "            strides[1],\n",
    "            dropout,\n",
    "        )\n",
    "        self.encoder3 = Encoder(\n",
    "            channels[1],\n",
    "            channels[2],\n",
    "            kernel_size,\n",
    "            strides[2],\n",
    "            dropout,\n",
    "        )\n",
    "        # encoder4는 strides[3]를 사용할 수 있도록 strides에 4개 값을 넣어주거나, 아래처럼 stride=1로 따로 설정 가능\n",
    "        # 여기서는 strides에 4개 값을 넣어준다고 가정함\n",
    "        self.bottleneck = Encoder(\n",
    "            channels[2],\n",
    "            channels[3],\n",
    "            kernel_size,\n",
    "            1, \n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "        # self.cbam = CBAM3D(channels=channels[3], reduction=8, spatial_kernel_size=3)\n",
    "        # ---------------------\n",
    "        # Decoder\n",
    "        # ---------------------\n",
    "        self.decoder3 = c_Decoder(\n",
    "            channels[3] + channels[2],\n",
    "            channels[1],\n",
    "            up_kernel_size,\n",
    "            strides[2],\n",
    "            dropout,\n",
    "        )\n",
    "        self.decoder2 = c_Decoder(\n",
    "            channels[1] + channels[1],\n",
    "            channels[0],\n",
    "            up_kernel_size,\n",
    "            strides[1],\n",
    "            dropout,\n",
    "        )\n",
    "        self.decoder1 = c_Decoder(\n",
    "            channels[0] + channels[0],\n",
    "            out_channels,\n",
    "            up_kernel_size,\n",
    "            strides[0],\n",
    "            conv_only=True,\n",
    "        )\n",
    "        if use_supervision:\n",
    "            self.supervision2 = nn.Conv3d(channels[0], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "            self.supervision3 = nn.Conv3d(channels[1], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Encoder\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.encoder3(x2)\n",
    "        \n",
    "        x = self.bottleneck(x3)\n",
    "\n",
    "        # Decoder: 이전 decoder 출력과 skip connection을 제대로 연결\n",
    "        x = self.decoder3(x, x3)   # bottleneck + encoder3\n",
    "        x = self.decoder2(x, x2)    # decoder3 출력 + encoder2\n",
    "        x = self.decoder1(x, x1)    # decoder2 출력 + encoder1\n",
    "        if hasattr(self, \"supervision2\"):\n",
    "            out2 = self.supervision2(x1)\n",
    "            out2 = F.interpolate(out2, x.shape[2:], mode=\"trilinear\", align_corners=False)\n",
    "            out3 = self.supervision3(x2)\n",
    "            out3 = F.interpolate(out3, x.shape[2:], mode=\"trilinear\", align_corners=False)\n",
    "            return [x, out2, out3]\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "# 예제 사용법:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 임의의 3D 데이터: 배치 크기 2, 채널 4, 깊이 8, 높이 16, 너비 16\n",
    "#     print(\"LayerNorm3d 사용 예제\")\n",
    "#     input_tensor = torch.randn(2, 1, 32, 96, 96)\n",
    "#     layer_norm3d = LayerNorm3d(num_channels=1)\n",
    "#     output = layer_norm3d(input_tensor)\n",
    "#     print(\"입력 텐서 shape:\", input_tensor.shape)\n",
    "#     print(\"출력 텐서 shape:\", output.shape)\n",
    "    \n",
    "#     # Encoder, Decoder, Unet_CBAM_LAYERNORM 사용 예제\n",
    "#     print(\"\\nEncoder, Decoder, Unet_CBAM_LAYERNORM 사용 예제\")\n",
    "#     model = Unet_CBAM_ds(\n",
    "#         in_channels=1,\n",
    "#         out_channels=4,\n",
    "#         channels=[16, 32, 64, 128],\n",
    "#         strides=[2, 2, 2, 2],\n",
    "#         kernel_size=3,\n",
    "#         up_kernel_size=3,\n",
    "#         dropout=0.1,\n",
    "#         use_supervision=True,\n",
    "#     )\n",
    "    \n",
    "#     output,ds2,ds3 = model(input_tensor)\n",
    "#     print(\"입력 텐서 shape:\", input_tensor.shape)\n",
    "#     print(\"출력 텐서 shape:\", output.shape)\n",
    "#     print(\"출력 텐서 shape:\", ds2.shape)\n",
    "#     print(\"출력 텐서 shape:\", ds3.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "import torch\n",
    "from torch.profiler import ProfilerActivity\n",
    "from torch.profiler import profile as profilee\n",
    "    \n",
    "\n",
    "def print_model_summary(model, input_size):\n",
    "    input_tensor = torch.randn(input_size)\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    flops, params = profile(model, inputs=(input_tensor,))\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"FLOPs: {flops:,}, GFLOPs: {flops / 1e9:.2f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def profile_model(model, input_size, log_dir='./log'):\n",
    "\n",
    "    input_tensor = torch.randn(input_size)\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    # 프로파일링\n",
    "    with profilee(\n",
    "        activities=[\n",
    "            ProfilerActivity.CPU, \n",
    "            ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(log_dir),  # TensorBoard 연동\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        model(input_tensor)\n",
    "\n",
    "    # 프로파일링 결과 출력\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.dataset import create_dataloaders\n",
    "from src.dataset.dataset_csv import create_dataloaders_from_csv\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, NormalizeIntensityd,\n",
    "    Orientationd, CropForegroundd, GaussianSmoothd, ScaleIntensityd,\n",
    "    RandSpatialCropd, RandRotate90d, RandFlipd, RandGaussianNoised,\n",
    "    ToTensord, RandCropByLabelClassesd, RandCropd,RandCropByPosNegLabeld, RandGaussianSmoothd, RandCoarseDropoutd\n",
    ")\n",
    "from monai.transforms import CastToTyped\n",
    "import numpy as np\n",
    "\n",
    "train_csv = \"./datasets/aug_train.csv\"\n",
    "val_csv = \"./datasets/val_647.csv\"\n",
    "# DATA CONFIG\n",
    "img_size =  96 # Match your patch size\n",
    "img_depth = 32\n",
    "n_classes = 7\n",
    "batch_size = 32 # 13.8GB GPU memory required for 128x128 img size\n",
    "loader_batch = 1\n",
    "num_samples = batch_size // loader_batch # 한 이미지에서 뽑을 샘플 수\n",
    "num_repeat = 3\n",
    "val_num_repeat = 20\n",
    "# MODEL CONFIG\n",
    "num_epochs = 4000\n",
    "lamda = 0.5\n",
    "ce_weight = 0.4\n",
    "lr = 0.001\n",
    "feature_size = [32, 64, 128, 256]\n",
    "dropout= 0.25\n",
    "use_ds = True\n",
    "# CLASS_WEIGHTS\n",
    "class_weights = None\n",
    "# class_weights = torch.tensor([0.0001, 1, 0.001, 1.1, 1, 1.1, 1], dtype=torch.float32)  # 클래스별 가중치\n",
    "sigma = 1.5\n",
    "\n",
    "\n",
    "accumulation_steps = 1\n",
    "# INIT\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "best_val_fbeta_score = 0\n",
    "\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    # GaussianSmoothd(\n",
    "    #     keys=[\"image\"],      # 변환을 적용할 키\n",
    "    #     sigma=[sigma, sigma, sigma]  # 각 축(x, y, z)의 시그마 값\n",
    "    #     ),\n",
    "])\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    RandCoarseDropoutd(\n",
    "        keys=[\"image\"],\n",
    "        holes=3,\n",
    "        spatial_size=(4,4,4),\n",
    "        prob=0.5,\n",
    "        fill_value=255,\n",
    "    ),\n",
    "    \n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    # RandGaussianSmoothd(\n",
    "    # keys=[\"image\"],      # 변환을 적용할 키\n",
    "    # sigma_x = (0.5, sigma), # 각 축(x, y, z)의 시그마 값\n",
    "    # sigma_y = (0.5, sigma),\n",
    "    # sigma_z = (0.5, sigma),\n",
    "    # prob=0.5,\n",
    "    # ),\n",
    "])\n",
    "val_random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[img_depth, img_size, img_size],\n",
    "        num_classes=n_classes,\n",
    "        num_samples=num_samples, \n",
    "        ratios=ratios_list,\n",
    "    ),\n",
    "    # RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[1, 2]),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    # RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    # RandGaussianSmoothd(\n",
    "    # keys=[\"image\"],      # 변환을 적용할 키\n",
    "    # sigma_x = (0.0, sigma), # 각 축(x, y, z)의 시그마 값\n",
    "    # sigma_y = (0.0, sigma),\n",
    "    # sigma_z = (0.0, sigma),\n",
    "    # prob=1.0,\n",
    "    # ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184, 630, 630)\n",
      "(184, 630, 630)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = None, None\n",
    "train_loader, val_loader = create_dataloaders_from_csv(\n",
    "    train_csv,\n",
    "    val_csv, \n",
    "    train_non_random_transforms = non_random_transforms, \n",
    "    val_non_random_transforms=non_random_transforms,\n",
    "    train_random_transforms=random_transforms,\n",
    "    val_random_transforms=val_random_transforms,\n",
    "    batch_size = loader_batch,\n",
    "    num_workers=0,train_num_repeat=num_repeat, val_num_repeat=val_num_repeat\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://monai.io/model-zoo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 best model 발견: model_checkpoints\\UNetCBAM_aug_hole_maxf256_32x96x96_e4000_lr0.001_lamda0.5_ce0.4_\\best_model.pt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Workspace\\czll\\src\\models\\unet_cbam.py:93: UserWarning: `len(strides) > len(channels) - 1`, the last 1 values of strides will not be used.\n",
      "  warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n",
      "c:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "C:\\Users\\Seungwoo\\AppData\\Local\\Temp\\ipykernel_12640\\717354296.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "기존 학습된 가중치를 성공적으로 로드했습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet_CBAM(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=n_classes,\n",
    "    channels=feature_size,\n",
    "    strides=[2, 2, 2, 2],\n",
    "    kernel_size=3,\n",
    "    up_kernel_size=3,\n",
    "    dropout=dropout,\n",
    "    # use_supervision=use_ds,\n",
    "    ).to(device)\n",
    "# x = (batch_size, 1, img_depth, img_size, img_size)\n",
    "# print_model_summary(model, x)\n",
    "# profile_model(model, x, './log')\n",
    "\n",
    "criterion = CombinedCETverskyLoss(\n",
    "    lamda=lamda,\n",
    "    ce_weight=ce_weight,\n",
    "    n_classes=n_classes,\n",
    "    \n",
    ").to(device)\n",
    "\n",
    "\n",
    "weight_str = \"weighted\" if class_weights is not None else \"\"\n",
    "\n",
    "# 체크포인트 디렉토리 및 파일 설정\n",
    "checkpoint_base_dir = Path(\"./model_checkpoints\")\n",
    "folder_name = f\"UNetCBAM_aug_hole_maxf{feature_size[-1]}_{img_depth}x{img_size}x{img_size}_e{num_epochs}_lr{lr}_lamda{lamda}_ce{ce_weight}_{weight_str}\"\n",
    "checkpoint_dir = checkpoint_base_dir / folder_name\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "# 체크포인트 디렉토리 생성\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    best_model_path = checkpoint_dir / 'best_model.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"기존 best model 발견: {best_model_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(best_model_path, map_location=device)\n",
    "            # 체크포인트 내부 키 검증\n",
    "            required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch', 'best_val_loss']\n",
    "            if all(k in checkpoint for k in required_keys):\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                best_val_loss = checkpoint['best_val_loss']\n",
    "                print(\"기존 학습된 가중치를 성공적으로 로드했습니다.\")\n",
    "                checkpoint= None\n",
    "            else:\n",
    "                raise ValueError(\"체크포인트 파일에 필요한 key가 없습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"체크포인트 파일을 로드하는 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(val_loader))\n",
    "# images, labels = batch[\"image\"], batch[\"label\"]\n",
    "# print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwoow070840\u001b[0m (\u001b[33mwaooang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Workspace\\czll\\wandb\\run-20250204_175700-wz9m2mpf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/wz9m2mpf' target=\"_blank\">UNetCBAM_aug_hole_maxf256_32x96x96_e4000_lr0.001_lamda0.5_ce0.4_</a></strong> to <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/czii_SwinUnetR' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/czii_SwinUnetR/runs/wz9m2mpf' target=\"_blank\">https://wandb.ai/waooang/czii_SwinUnetR/runs/wz9m2mpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = folder_name\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project='czii_SwinUnetR',  # 프로젝트 이름 설정\n",
    "    name=run_name,         # 실행(run) 이름 설정\n",
    "    config={\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'lambda': lamda,\n",
    "        \"cross_entropy_weight\": ce_weight,\n",
    "        'img_depth': img_depth,\n",
    "        'img_size': img_size,\n",
    "        'sampling_ratio': ratios_list,\n",
    "        'device': device.type,\n",
    "        \"checkpoint_dir\": str(checkpoint_dir),\n",
    "        \"class_weights\": class_weights.tolist() if class_weights is not None else None,\n",
    "        \"feature_size\": feature_size,\n",
    "        \"deep_supervision\": use_ds,\n",
    "        \"dropout\": dropout,        \n",
    "        \"accumulation_steps\": accumulation_steps,\n",
    "        \"num_repeat\": num_repeat,\n",
    "        \n",
    "        # 필요한 하이퍼파라미터 추가\n",
    "    }\n",
    ")\n",
    "# 모델을 wandb에 연결\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric\n",
    "\n",
    "def processing(batch_data, model, criterion,device):\n",
    "    \n",
    "        \n",
    "    images = batch_data['image'].to(device)  # Input 이미지 (B, 1, 96, 96, 96)\n",
    "    labels = batch_data['label'].to(device)  # 라벨 (B, 96, 96, 96)\n",
    "\n",
    "    labels = labels.squeeze(1)  # (B, 1, 96, 96, 96) → (B, 96, 96, 96)\n",
    "    labels = labels.long()  # 라벨을 정수형으로 변환\n",
    "\n",
    "    # 원핫 인코딩 (B, H, W, D) → (B, num_classes, H, W, D)\n",
    "    \n",
    "    labels_onehot = torch.nn.functional.one_hot(labels, num_classes=n_classes)\n",
    "    labels_onehot = labels_onehot.permute(0, 4, 1, 2, 3).float()  # (B, num_classes, H, W, D)\n",
    "\n",
    "    \n",
    "    outputs = model(images)  # outputs: (B, num_classes, H, W, D)\n",
    "\n",
    "    # Loss 계산\n",
    "    loss = criterion(outputs, labels_onehot)\n",
    "        \n",
    "    \n",
    "    # loss = loss_fn(criterion(outputs, labels_onehot),class_weights=class_weights, device=device)\n",
    "    return loss, outputs, labels, outputs[0].argmax(dim=1) if type(outputs) == list else outputs.argmax(dim=1)\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, accumulation_steps=4):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()  # 그래디언트 초기화\n",
    "    with tqdm(train_loader, desc='Training') as pbar:\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "            # 손실 계산\n",
    "            loss, _, _, _ = processing(batch_data, model, criterion, device)\n",
    "\n",
    "            # 그래디언트를 계산하고 누적\n",
    "            loss = loss / accumulation_steps  # 그래디언트 누적을 위한 스케일링\n",
    "            loss.backward()  # 그래디언트 계산 및 누적\n",
    "            \n",
    "            # 그래디언트 업데이트 (accumulation_steps마다 한 번)\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                optimizer.step()  # 파라미터 업데이트\n",
    "                optimizer.zero_grad()  # 누적된 그래디언트 초기화\n",
    "            \n",
    "            # 손실값 누적 (스케일링 복구)\n",
    "            epoch_loss += loss.item() * accumulation_steps  # 실제 손실값 반영\n",
    "            pbar.set_postfix(loss=loss.item() * accumulation_steps)  # 실제 손실값 출력\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    wandb.log({'train_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device, epoch, calculate_dice_interval, ce_weight):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    class_dice_scores = {i: [] for i in range(n_classes)}\n",
    "    class_f_beta_scores = {i: [] for i in range(n_classes)}\n",
    "    class_mIoU_scores = {i: [] for i in range(n_classes)}\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc='Validation') as pbar:\n",
    "            for batch_data in pbar:\n",
    "                loss, _, labels, preds = processing(batch_data, model, criterion, device)\n",
    "                val_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "                # 각 클래스별 Dice 점수 계산\n",
    "                if epoch % calculate_dice_interval == 0:\n",
    "                    for i in range(n_classes):\n",
    "                        pred_i = (preds == i)\n",
    "                        label_i = (labels == i)\n",
    "                        dice_score = (2.0 * torch.sum(pred_i & label_i)) / (torch.sum(pred_i) + torch.sum(label_i) + 1e-8)\n",
    "                        class_dice_scores[i].append(dice_score.item())\n",
    "                        precision = (torch.sum(pred_i & label_i) + 1e-8) / (torch.sum(pred_i) + 1e-8)\n",
    "                        recall = (torch.sum(pred_i & label_i) + 1e-8) / (torch.sum(label_i) + 1e-8)\n",
    "                        f_beta_score = (1 + 4**2) * (precision * recall) / (4**2 * precision + recall + 1e-8)\n",
    "                        class_f_beta_scores[i].append(f_beta_score.item())\n",
    "                        intersection = torch.sum(pred_i & label_i).float()\n",
    "                        union = torch.sum(pred_i | label_i).float()\n",
    "                        iou = (intersection + 1e-8) / (union + 1e-8)\n",
    "                        class_mIoU_scores[i].append(iou.item())\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    # 에포크별 평균 손실 로깅\n",
    "    wandb.log({'val_epoch_loss': avg_loss, 'epoch': epoch + 1})\n",
    "    \n",
    "    # 각 클래스별 평균 Dice 점수 출력\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation Dice Score\")\n",
    "        all_classes_dice_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_dice = np.mean(class_dice_scores[i])\n",
    "            wandb.log({f'class_{i}_dice_score': mean_dice, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_dice:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_dice_scores.append(mean_dice)\n",
    "            \n",
    "        print()\n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation F-beta Score\")\n",
    "        all_classes_fbeta_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_fbeta = np.mean(class_f_beta_scores[i])\n",
    "            wandb.log({f'class_{i}_f_beta_score': mean_fbeta, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_fbeta:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_fbeta_scores.append(mean_fbeta)\n",
    "               \n",
    "        print() \n",
    "    if epoch % calculate_dice_interval == 0:\n",
    "        print(\"Validation mIoU Score\")\n",
    "        all_classes_mIoU_scores = []\n",
    "        for i in range(n_classes):\n",
    "            mean_IoU = np.mean(class_mIoU_scores[i])\n",
    "            wandb.log({f'class_{i}_IoU_score': mean_fbeta, 'epoch': epoch + 1})\n",
    "            print(f\"Class {i}: {mean_IoU:.4f}\", end=\", \")\n",
    "            if i not in [0, 2]:  # 평균에 포함할 클래스만 추가\n",
    "                all_classes_mIoU_scores.append(mean_IoU)\n",
    "                \n",
    "        print()\n",
    "        overall_mean_dice = np.mean(all_classes_dice_scores)\n",
    "        overall_mean_fbeta = np.mean(all_classes_fbeta_scores)\n",
    "        overall_mean_IoU = np.mean(all_classes_mIoU_scores)\n",
    "        wandb.log({'overall_mean_f_beta_score': overall_mean_fbeta, 'overall_mean_dice_score': overall_mean_dice, 'epoch': epoch + 1, 'overall_mean_IoU_score': overall_mean_IoU})\n",
    "        print(f\"\\nOverall Mean Dice Score: {overall_mean_dice:.4f}\\nOverall Mean F-beta Score: {overall_mean_fbeta:.4f}\\nOverall Mean IoU Score: {overall_mean_IoU:.4f}\")\n",
    "\n",
    "    if overall_mean_fbeta is None:\n",
    "        overall_mean_fbeta = 0\n",
    "\n",
    "    final_score = overall_mean_fbeta * (1 - ce_weight) + overall_mean_IoU * ce_weight\n",
    "    return val_loss / len(val_loader), final_score \n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, \n",
    "    device, start_epoch, best_val_loss, best_val_fbeta_score, calculate_dice_interval=1,\n",
    "    accumulation_steps=4, pretrained=False\n",
    "):\n",
    "    \"\"\"\n",
    "    모델을 학습하고 검증하는 함수\n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        train_loader: 학습 데이터 로더\n",
    "        val_loader: 검증 데이터 로더\n",
    "        criterion: 손실 함수\n",
    "        optimizer: 최적화 알고리즘\n",
    "        num_epochs: 총 학습 epoch 수\n",
    "        patience: early stopping 기준\n",
    "        device: GPU/CPU 장치\n",
    "        start_epoch: 시작 epoch\n",
    "        best_val_loss: 이전 최적 validation loss\n",
    "        best_val_fbeta_score: 이전 최적 validation f-beta score\n",
    "        calculate_dice_interval: Dice 점수 계산 주기\n",
    "    \"\"\"\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train One Epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model=model, \n",
    "            train_loader=train_loader, \n",
    "            criterion=criterion, \n",
    "            optimizer=optimizer, \n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            accumulation_steps= accumulation_steps\n",
    "        )\n",
    "        \n",
    "        scheduler.step(train_loss)\n",
    "        # Validate One Epoch\n",
    "        val_loss, overall_mean_fbeta_score = validate_one_epoch(\n",
    "            model=model, \n",
    "            val_loader=val_loader, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            epoch=epoch, \n",
    "            calculate_dice_interval=calculate_dice_interval,\n",
    "            ce_weight=ce_weight\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F-beta: {overall_mean_fbeta_score:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss and overall_mean_fbeta_score > best_val_fbeta_score:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_fbeta_score = overall_mean_fbeta_score\n",
    "            epochs_no_improve = 0\n",
    "            if pretrained:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model_pretrained.pt')\n",
    "            else:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            print(f\"========================================================\")\n",
    "            print(f\"SUPER Best model saved. Loss:{best_val_loss:.4f}, Score:{best_val_fbeta_score:.4f}\")\n",
    "            print(f\"========================================================\")\n",
    "\n",
    "        # Early stopping 조건 체크\n",
    "        if val_loss >= best_val_loss and overall_mean_fbeta_score <= best_val_fbeta_score:\n",
    "            epochs_no_improve += 1\n",
    "        else:\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'last.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_fbeta_score': best_val_fbeta_score\n",
    "            }, checkpoint_path)\n",
    "            break\n",
    "        # if epochs_no_improve % 6 == 0 & epochs_no_improve != 0:\n",
    "        #     # 손실이 개선되지 않았으므로 lambda 감소\n",
    "        #     new_lamda = max(criterion.lamda - 0.01, 0.35)  # 최소값은 0.1로 설정\n",
    "        #     criterion.set_lamda(new_lamda)\n",
    "        #     print(f\"Validation loss did not improve. Reducing lambda to {new_lamda:.4f}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/432 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CombinedCETverskyLoss.forward() got an unexpected keyword argument 'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_val_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_val_fbeta_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_val_fbeta_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalculate_dice_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 161\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device, start_epoch, best_val_loss, best_val_fbeta_score, calculate_dice_interval, accumulation_steps, pretrained)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Train One Epoch\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Validate One Epoch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device, epoch, accumulation_steps)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# 손실 계산\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m         loss, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# 그래디언트를 계산하고 누적\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps  \u001b[38;5;66;03m# 그래디언트 누적을 위한 스케일링\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m, in \u001b[0;36mprocessing\u001b[1;34m(batch_data, model, criterion, device, val)\u001b[0m\n\u001b[0;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)  \u001b[38;5;66;03m# outputs: (B, num_classes, H, W, D)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Loss 계산\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# loss = loss_fn(criterion(outputs, labels_onehot),class_weights=class_weights, device=device)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, outputs, labels, outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\czii\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: CombinedCETverskyLoss.forward() got an unexpected keyword argument 'validation'"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=10,\n",
    "    device=device,\n",
    "    start_epoch=start_epoch,\n",
    "    best_val_loss=best_val_loss,\n",
    "    best_val_fbeta_score=best_val_fbeta_score,\n",
    "    calculate_dice_interval=1,\n",
    "    accumulation_steps = accumulation_steps\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "czii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
