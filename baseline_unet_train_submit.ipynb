{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline UNet training + prediction/submission\n",
    "\n",
    "\n",
    "This is the notebook I cobbled together to wrap my head around this challenge.\n",
    "I don't garuantee that the results are great, only that it works from end-to-end. \n",
    "\n",
    "It trains a basic UNet and makes a submission. \n",
    "\n",
    "It's based on these three notebooks: \n",
    "\n",
    "1. [3D U-Net : Training Only](https://www.kaggle.com/code/ahsuna123/3d-u-net-training-only)\n",
    "2. [3D U-Net PyTorch Lightning distributed training](https://www.kaggle.com/code/zhuowenzhao11/3d-u-net-pytorch-lightning-distributed-training)\n",
    "3. [3d-unet using 2d image encoder](https://www.kaggle.com/code/hengck23/3d-unet-using-2d-image-encoder/notebook)\n",
    "\n",
    "\n",
    "I've pre-computed the input data and stored them as numpy arrays so they don't have to be extracted every time the notebooks is run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing offline deps\n",
    "\n",
    "As this is a code comp, there is no internet. \n",
    "So we have to do some silly things to get dependencies in here. \n",
    "Why is asciitree such a PITA? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:18:47.609002Z",
     "iopub.status.busy": "2024-11-11T08:18:47.60867Z",
     "iopub.status.idle": "2024-11-11T08:18:47.619844Z",
     "shell.execute_reply": "2024-11-11T08:18:47.618622Z",
     "shell.execute_reply.started": "2024-11-11T08:18:47.608968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "deps_path = './kaggle/input/czii-cryoet-dependencies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:20:08.595867Z",
     "iopub.status.busy": "2024-11-11T08:20:08.595437Z",
     "iopub.status.idle": "2024-11-11T08:20:51.076502Z",
     "shell.execute_reply": "2024-11-11T08:20:51.075627Z",
     "shell.execute_reply.started": "2024-11-11T08:20:08.595816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,  \n",
    "    AsDiscrete,  \n",
    "    RandFlipd, \n",
    "    RandRotate90d, \n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some helper functions\n",
    "\n",
    "\n",
    "### Patching helper functions\n",
    "\n",
    "These are mostly used to split large volumes into smaller ones and stitch them back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-11-11T08:20:51.078598Z",
     "iopub.status.busy": "2024-11-11T08:20:51.077793Z",
     "iopub.status.idle": "2024-11-11T08:20:51.099865Z",
     "shell.execute_reply": "2024-11-11T08:20:51.098597Z",
     "shell.execute_reply.started": "2024-11-11T08:20:51.078562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Calculate the starting positions of patches along a single dimension\n",
    "    with minimal overlap to cover the entire dimension.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dimension_size : int\n",
    "        Size of the dimension\n",
    "    patch_size : int\n",
    "        Size of the patch in this dimension\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[int]\n",
    "        List of starting positions for patches\n",
    "    \"\"\"\n",
    "    if dimension_size <= patch_size:\n",
    "        return [0]\n",
    "        \n",
    "    # Calculate number of patches needed\n",
    "    n_patches = np.ceil(dimension_size / patch_size)\n",
    "    \n",
    "    if n_patches == 1:\n",
    "        return [0]\n",
    "    \n",
    "    # Calculate overlap\n",
    "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
    "    \n",
    "    # Generate starting positions\n",
    "    positions = []\n",
    "    for i in range(int(n_patches)):\n",
    "        pos = int(i * (patch_size - total_overlap))\n",
    "        if pos + patch_size > dimension_size:\n",
    "            pos = dimension_size - patch_size\n",
    "        if pos not in positions:  # Avoid duplicates\n",
    "            positions.append(pos)\n",
    "    \n",
    "    return positions\n",
    "\n",
    "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    \"\"\"\n",
    "    Extract 3D patches from multiple arrays with minimal overlap to cover the entire array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arrays : List[np.ndarray]\n",
    "        List of input arrays, each with shape (m, n, l)\n",
    "    patch_size : int\n",
    "        Size of cubic patches (a x a x a)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    patches : List[np.ndarray]\n",
    "        List of all patches from all input arrays\n",
    "    coordinates : List[Tuple[int, int, int]]\n",
    "        List of starting coordinates (x, y, z) for each patch\n",
    "    \"\"\"\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "    \n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "    \n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
    "    \n",
    "    m, n, l = shape\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "    \n",
    "    # Calculate starting positions for each dimension\n",
    "    x_starts = calculate_patch_starts(m, patch_size)\n",
    "    y_starts = calculate_patch_starts(n, patch_size)\n",
    "    z_starts = calculate_patch_starts(l, patch_size)\n",
    "    \n",
    "    # Extract patches from each array\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "    \n",
    "    return patches, coordinates\n",
    "\n",
    "# Note: I should probably averge the overlapping areas, \n",
    "# but here they are just overwritten by the most recent one. \n",
    "\n",
    "def reconstruct_array(patches: List[np.ndarray], \n",
    "                     coordinates: List[Tuple[int, int, int]], \n",
    "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstruct array from patches.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patches : List[np.ndarray]\n",
    "        List of patches to reconstruct from\n",
    "    coordinates : List[Tuple[int, int, int]]\n",
    "        Starting coordinates for each patch\n",
    "    original_shape : Tuple[int, int, int]\n",
    "        Shape of the original array\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Reconstructed array\n",
    "    \"\"\"\n",
    "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
    "    \n",
    "    patch_size = patches[0].shape[0]\n",
    "    \n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        reconstructed[\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] = patch\n",
    "        \n",
    "    \n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission helper functions\n",
    "\n",
    "These help with getting the submission in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-11-11T08:20:51.101969Z",
     "iopub.status.busy": "2024-11-11T08:20:51.101503Z",
     "iopub.status.idle": "2024-11-11T08:20:51.135036Z",
     "shell.execute_reply": "2024-11-11T08:20:51.134014Z",
     "shell.execute_reply.started": "2024-11-11T08:20:51.101912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    \"\"\"\n",
    "    Convert dictionary of coordinates to pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    coord_dict : dict\n",
    "        Dictionary where keys are labels and values are Nx3 coordinate arrays\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns ['x', 'y', 'z', 'label']\n",
    "    \"\"\"\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each label and its coordinates\n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    # Concatenate all coordinates\n",
    "    all_coords = np.vstack(all_coords)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:20:51.137046Z",
     "iopub.status.busy": "2024-11-11T08:20:51.13654Z",
     "iopub.status.idle": "2024-11-11T08:20:51.14532Z",
     "shell.execute_reply": "2024-11-11T08:20:51.144492Z",
     "shell.execute_reply.started": "2024-11-11T08:20:51.137007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = \"./kaggle/input/create-numpy-dataset\"\n",
    "TEST_DATA_DIR = \"./kaggle/input/czii-cryo-et-object-identification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:20:51.148826Z",
     "iopub.status.busy": "2024-11-11T08:20:51.148427Z",
     "iopub.status.idle": "2024-11-11T08:21:07.749553Z",
     "shell.execute_reply": "2024-11-11T08:21:07.748732Z",
     "shell.execute_reply.started": "2024-11-11T08:20:51.148774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in range(7):\n",
    "    image = np.load(f\"{TRAIN_DATA_DIR}/train_image_{i}.npy\")\n",
    "    label = np.load(f\"{TRAIN_DATA_DIR}/train_label_{i}.npy\")\n",
    "\n",
    "    data_list.append({\"image\": image, \"label\": label})\n",
    "    \n",
    "\n",
    "train_files, val_files = data_list[:6], data_list[6:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training dataloader\n",
    "\n",
    "I should probably find a way to create a dataloader that takes more batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:21:07.75154Z",
     "iopub.status.busy": "2024-11-11T08:21:07.750839Z",
     "iopub.status.idle": "2024-11-11T08:21:09.600666Z",
     "shell.execute_reply": "2024-11-11T08:21:09.599777Z",
     "shell.execute_reply.started": "2024-11-11T08:21:07.751495Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.spatial.dictionary.Orientationd object at 0x00000187EFA89E50>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOptionalImportError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[1;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(data)\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\spatial\\dictionary.py:603\u001b[0m, in \u001b[0;36mOrientationd.__call__\u001b[1;34m(self, data, lazy)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d):\n\u001b[1;32m--> 603\u001b[0m     d[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mornt_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\spatial\\array.py:631\u001b[0m, in \u001b[0;36mOrientation.__call__\u001b[1;34m(self, data_array, lazy)\u001b[0m\n\u001b[0;32m    629\u001b[0m     affine_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(sr \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m--> 631\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio_orientation\u001b[49m(affine_)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_closest_canonical:\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\utils\\module.py:420\u001b[0m, in \u001b[0;36moptional_import.<locals>._LazyRaise.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03mRaises:\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    OptionalImportError: When you call this method.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[1;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(data)\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\spatial\\dictionary.py:603\u001b[0m, in \u001b[0;36mOrientationd.__call__\u001b[1;34m(self, data, lazy)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d):\n\u001b[1;32m--> 603\u001b[0m     d[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mornt_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\spatial\\array.py:631\u001b[0m, in \u001b[0;36mOrientation.__call__\u001b[1;34m(self, data_array, lazy)\u001b[0m\n\u001b[0;32m    629\u001b[0m     affine_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(sr \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m--> 631\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio_orientation\u001b[49m(affine_)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_closest_canonical:\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\utils\\module.py:420\u001b[0m, in \u001b[0;36moptional_import.<locals>._LazyRaise.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03mRaises:\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    OptionalImportError: When you call this method.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\utils\\module.py:378\u001b[0m, in \u001b[0;36moptional_import\u001b[1;34m(module, version, version_checker, name, descriptor, version_args, allow_namespace_pkg, as_type)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     pkg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# top level module\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     the_module \u001b[38;5;241m=\u001b[39m import_module(module)\n",
      "\u001b[1;31mOptionalImportError\u001b[0m: import nibabel (No module named 'nibabel').\n\nFor details about installing the optional dependencies, please visit:\n    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Non-random transforms to be cached\u001b[39;00m\n\u001b[0;32m      2\u001b[0m non_random_transforms \u001b[38;5;241m=\u001b[39m Compose([\n\u001b[0;32m      3\u001b[0m     EnsureChannelFirstd(keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], channel_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_channel\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      4\u001b[0m     NormalizeIntensityd(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m     Orientationd(keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], axcodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m ])\n\u001b[1;32m----> 8\u001b[0m raw_train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mCacheDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_random_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m my_num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m     12\u001b[0m train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\data\\dataset.py:813\u001b[0m, in \u001b[0;36mCacheDataset.__init__\u001b[1;34m(self, data, transform, cache_num, cache_rate, num_workers, progress, copy_cache, as_contiguous, hash_as_key, hash_func, runtime_cache)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m ListProxy \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hash_keys: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 813\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\data\\dataset.py:840\u001b[0m, in \u001b[0;36mCacheDataset.set_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    837\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_num))\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# prepare cache content immediately\u001b[39;00m\n\u001b[1;32m--> 840\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache:\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;66;03m# this must be in the main process, not in dataloader's workers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\data\\dataset.py:869\u001b[0m, in \u001b[0;36mCacheDataset._fill_cache\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress \u001b[38;5;129;01mand\u001b[39;00m has_tqdm:\n\u001b[1;32m--> 869\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cache_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mimap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cache_item, indices))\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\multiprocessing\\pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\data\\dataset.py:882\u001b[0m, in \u001b[0;36mCacheDataset._load_cache_item\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    877\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m    879\u001b[0m first_random \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mget_index_of_first(\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28misinstance\u001b[39m(t, RandomizableTrait) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, Transform)\n\u001b[0;32m    881\u001b[0m )\n\u001b[1;32m--> 882\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_random\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_contiguous:\n\u001b[0;32m    885\u001b[0m     item \u001b[38;5;241m=\u001b[39m convert_to_contiguous(item, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    334\u001b[0m     _lazy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[1;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[1;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[0;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[1;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\pook0\\.conda\\envs\\UM\\Lib\\site-packages\\monai\\transforms\\transform.py:171\u001b[0m, in \u001b[0;36mapply_transform\u001b[1;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m         _log_stats(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: applying transform <monai.transforms.spatial.dictionary.Orientationd object at 0x00000187EFA89E50>"
     ]
    }
   ],
   "source": [
    "# Non-random transforms to be cached\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\")\n",
    "])\n",
    "\n",
    "raw_train_ds = CacheDataset(data=train_files, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "\n",
    "my_num_samples = 16\n",
    "train_batch_size = 1\n",
    "\n",
    "# Random transforms to be applied during training\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[96, 96, 96],\n",
    "        num_classes=6,\n",
    "        num_samples=my_num_samples\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),    \n",
    "])\n",
    "\n",
    "train_ds = Dataset(data=raw_train_ds, transform=random_transforms)\n",
    "\n",
    "\n",
    "# DataLoader remains the same\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the validation dataloader\n",
    "\n",
    "Here I deviate a little from the source notebooks. \n",
    "\n",
    "In the source, the validation dataloader also used the random transformations. This is bad practice and will result in noisy validation. \n",
    "\n",
    "Here I split the validation dataset in (slightly) overlapping blocks of `(96, 96 , 96)` so that we can have a consistent validation set that uses all the validation data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:21:09.602675Z",
     "iopub.status.busy": "2024-11-11T08:21:09.602006Z",
     "iopub.status.idle": "2024-11-11T08:21:10.37067Z",
     "shell.execute_reply": "2024-11-11T08:21:10.369674Z",
     "shell.execute_reply.started": "2024-11-11T08:21:09.602631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_images,val_labels = [dcts['image'] for dcts in val_files],[dcts['label'] for dcts in val_files]\n",
    "\n",
    "val_image_patches, _ = extract_3d_patches_minimal_overlap(val_images, 96)\n",
    "val_label_patches, _ = extract_3d_patches_minimal_overlap(val_labels, 96)\n",
    "\n",
    "val_patched_data = [{\"image\": img, \"label\": lbl} for img, lbl in zip(val_image_patches, val_label_patches)]\n",
    "\n",
    "\n",
    "valid_ds = CacheDataset(data=val_patched_data, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "\n",
    "valid_batch_size = 16\n",
    "# DataLoader remains the same\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=valid_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model\n",
    "\n",
    "This model is pretty much directly copied from [3D U-Net PyTorch Lightning distributed training](https://www.kaggle.com/code/zhuowenzhao11/3d-u-net-pytorch-lightning-distributed-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:21:10.372953Z",
     "iopub.status.busy": "2024-11-11T08:21:10.372362Z",
     "iopub.status.idle": "2024-11-11T08:21:11.240463Z",
     "shell.execute_reply": "2024-11-11T08:21:11.239691Z",
     "shell.execute_reply.started": "2024-11-11T08:21:10.372903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 6,\n",
    "        channels: Union[Tuple[int, ...], List[int]] = (48, 64, 80, 80),\n",
    "        strides: Union[Tuple[int, ...], List[int]] = (2, 2, 1),\n",
    "        num_res_units: int = 1,\n",
    "        lr: float=1e-3):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = UNet(\n",
    "            spatial_dims=self.hparams.spatial_dims,\n",
    "            in_channels=self.hparams.in_channels,\n",
    "            out_channels=self.hparams.out_channels,\n",
    "            channels=self.hparams.channels,\n",
    "            strides=self.hparams.strides,\n",
    "            num_res_units=self.hparams.num_res_units,\n",
    "        )\n",
    "        self.loss_fn = TverskyLoss(include_background=True, to_onehot_y=True, softmax=True)  # softmax=True for multiclass\n",
    "        self.metric_fn = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)\n",
    "\n",
    "        self.train_loss = 0\n",
    "        self.val_metric = 0\n",
    "        self.num_train_batch = 0\n",
    "        self.num_val_batch = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['image'], batch['label']\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.train_loss += loss\n",
    "        self.num_train_batch += 1\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        loss_per_epoch = self.train_loss/self.num_train_batch\n",
    "        #print(f\"Epoch {self.current_epoch} - Average Train Loss: {loss_per_epoch:.4f}\")\n",
    "        self.log('train_loss', loss_per_epoch, prog_bar=True)\n",
    "        self.train_loss = 0\n",
    "        self.num_train_batch = 0\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad(): # This ensures that gradients are not stored in memory\n",
    "            x, y = batch['image'], batch['label']\n",
    "            y_hat = self(x)\n",
    "            metric_val_outputs = [AsDiscrete(argmax=True, to_onehot=self.hparams.out_channels)(i) for i in decollate_batch(y_hat)]\n",
    "            metric_val_labels = [AsDiscrete(to_onehot=self.hparams.out_channels)(i) for i in decollate_batch(y)]\n",
    "\n",
    "            # compute metric for current iteration\n",
    "            self.metric_fn(y_pred=metric_val_outputs, y=metric_val_labels)\n",
    "            metrics = self.metric_fn.aggregate(reduction=\"mean_batch\")\n",
    "            val_metric = torch.mean(metrics) # I used mean over all particle species as the metric. This can be explored.\n",
    "            self.val_metric += val_metric \n",
    "            self.num_val_batch += 1\n",
    "        torch.cuda.empty_cache()\n",
    "        return {'val_metric': val_metric}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metric_per_epoch = self.val_metric/self.num_val_batch\n",
    "        #print(f\"Epoch {self.current_epoch} - Average Val Metric: {metric_per_epoch:.4f}\")\n",
    "        self.log('val_metric', metric_per_epoch, prog_bar=True, sync_dist=False) # sync_dist=True for distributed training\n",
    "        self.val_metric = 0\n",
    "        self.num_val_batch = 0\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:21:11.242099Z",
     "iopub.status.busy": "2024-11-11T08:21:11.241705Z",
     "iopub.status.idle": "2024-11-11T08:21:11.272732Z",
     "shell.execute_reply": "2024-11-11T08:21:11.271883Z",
     "shell.execute_reply.started": "2024-11-11T08:21:11.242056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "channels = (48, 64, 80, 80)\n",
    "strides_pattern = (2, 2, 1)       \n",
    "num_res_units = 1\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "\n",
    "model = Model(channels=channels, strides=strides_pattern, num_res_units=num_res_units, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T22:22:02.19348Z",
     "iopub.status.busy": "2024-11-10T22:22:02.193188Z",
     "iopub.status.idle": "2024-11-10T22:22:02.2914Z",
     "shell.execute_reply": "2024-11-10T22:22:02.290531Z",
     "shell.execute_reply.started": "2024-11-10T22:22:02.193449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Check if CUDA is available and then count the GPUs\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "else:\n",
    "    print(\"No GPU available. Running on CPU.\")\n",
    "devices = list(range(num_gpus))\n",
    "print(devices)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    #strategy=\"ddp_notebook\", \n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],# devices\n",
    "    num_nodes=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let there be gradients!\n",
    "\n",
    "Locally this config seems to train for about 1000 steps before the model starts overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T22:22:02.292904Z",
     "iopub.status.busy": "2024-11-10T22:22:02.292592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:25:49.558823Z",
     "iopub.status.busy": "2024-11-11T08:25:49.55842Z",
     "iopub.status.idle": "2024-11-11T08:25:49.566484Z",
     "shell.execute_reply": "2024-11-11T08:25:49.565607Z",
     "shell.execute_reply.started": "2024-11-11T08:25:49.558784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval();\n",
    "model.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:25:50.699039Z",
     "iopub.status.busy": "2024-11-11T08:25:50.698326Z",
     "iopub.status.idle": "2024-11-11T08:25:50.712738Z",
     "shell.execute_reply": "2024-11-11T08:25:50.711837Z",
     "shell.execute_reply.started": "2024-11-11T08:25:50.699001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "copick_config_path = TRAIN_DATA_DIR + \"/copick.config\"\n",
    "\n",
    "with open(copick_config_path) as f:\n",
    "    copick_config = json.load(f)\n",
    "\n",
    "copick_config['static_root'] = '/kaggle/input/czii-cryo-et-object-identification/test/static'\n",
    "\n",
    "copick_test_config_path = 'copick_test.config'\n",
    "\n",
    "with open(copick_test_config_path, 'w') as outfile:\n",
    "    json.dump(copick_config, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:25:51.052884Z",
     "iopub.status.busy": "2024-11-11T08:25:51.051886Z",
     "iopub.status.idle": "2024-11-11T08:25:52.072891Z",
     "shell.execute_reply": "2024-11-11T08:25:52.071945Z",
     "shell.execute_reply.started": "2024-11-11T08:25:51.052841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copick\n",
    "\n",
    "root = copick.from_file(copick_test_config_path)\n",
    "\n",
    "copick_user_name = \"copickUtils\"\n",
    "copick_segmentation_name = \"paintedPicks\"\n",
    "voxel_size = 10\n",
    "tomo_type = \"denoised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:25:52.07555Z",
     "iopub.status.busy": "2024-11-11T08:25:52.074451Z",
     "iopub.status.idle": "2024-11-11T08:25:52.080883Z",
     "shell.execute_reply": "2024-11-11T08:25:52.079896Z",
     "shell.execute_reply.started": "2024-11-11T08:25:52.07551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Non-random transforms to be cached\n",
    "inference_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:25:52.083124Z",
     "iopub.status.busy": "2024-11-11T08:25:52.082594Z",
     "iopub.status.idle": "2024-11-11T08:25:52.097421Z",
     "shell.execute_reply": "2024-11-11T08:25:52.09666Z",
     "shell.execute_reply.started": "2024-11-11T08:25:52.083091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cc3d\n",
    "\n",
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              2: \"beta-galactosidase\", \n",
    "              3: \"ribosome\", \n",
    "              4: \"thyroglobulin\", \n",
    "              5: \"virus-like-particle\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over test set\n",
    "\n",
    "\n",
    "Below we will: \n",
    "1. Read in a run\n",
    "2. Split it into patches of size (96, 96, 96)\n",
    "3. Create a dataset from the patches\n",
    "4. Predict the segmentation mask\n",
    "5. Glue the mask back together\n",
    "6. Find the connected components for each class\n",
    "7. Find the centroids of the connected components\n",
    "8. Add to the dataframe\n",
    "\n",
    "Then do this for all runs. \n",
    "\n",
    "This can probably be optimized quite a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T08:25:52.100464Z",
     "iopub.status.busy": "2024-11-11T08:25:52.099465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BLOB_THRESHOLD = 500\n",
    "CERTAINTY_THRESHOLD = 0.5\n",
    "\n",
    "classes = [1, 2, 3, 4, 5]\n",
    "with torch.no_grad():\n",
    "    location_df = []\n",
    "    for run in root.runs:\n",
    "        print(run)\n",
    "\n",
    "        tomo = run.get_voxel_spacing(10)\n",
    "        tomo = tomo.get_tomogram(tomo_type).numpy()\n",
    "\n",
    "\n",
    "\n",
    "        tomo_patches, coordinates  = extract_3d_patches_minimal_overlap([tomo], 96)\n",
    "\n",
    "        tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n",
    "\n",
    "        tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0)\n",
    "\n",
    "        pred_masks = []\n",
    "\n",
    "        for i in range(len(tomo_ds)):\n",
    "            input_tensor = tomo_ds[i]['image'].unsqueeze(0).to(\"cuda\")\n",
    "            model_output = model(input_tensor)\n",
    "\n",
    "            probs = torch.softmax(model_output[0], dim=0)\n",
    "            thresh_probs = probs > CERTAINTY_THRESHOLD\n",
    "            _, max_classes = thresh_probs.max(dim=0)\n",
    "\n",
    "            pred_masks.append(max_classes.cpu().numpy())\n",
    "            \n",
    "\n",
    "        reconstructed_mask = reconstruct_array(pred_masks, coordinates, tomo.shape)\n",
    "        \n",
    "        location = {}\n",
    "\n",
    "        for c in classes:\n",
    "            cc = cc3d.connected_components(reconstructed_mask == c)\n",
    "            stats = cc3d.statistics(cc)\n",
    "            zyx=stats['centroids'][1:]*10.012444 #https://www.kaggle.com/competitions/czii-cryo-et-object-identification/discussion/544895#3040071\n",
    "            zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]\n",
    "            xyz =np.ascontiguousarray(zyx_large[:,::-1])\n",
    "\n",
    "            location[id_to_name[c]] = xyz\n",
    "\n",
    "\n",
    "        df = dict_to_df(location, run.name)\n",
    "        location_df.append(df)\n",
    "    \n",
    "    location_df = pd.concat(location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "location_df.insert(loc=0, column='id', value=np.arange(len(location_df)))\n",
    "location_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10033515,
     "sourceId": 84969,
     "sourceType": "competition"
    },
    {
     "datasetId": 6052780,
     "sourceId": 9862305,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6058495,
     "sourceId": 9869730,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 206165222,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
