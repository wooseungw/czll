{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84969,"databundleVersionId":10033515,"sourceType":"competition"},{"sourceId":9862305,"sourceType":"datasetVersion","datasetId":6052780},{"sourceId":9869730,"sourceType":"datasetVersion","datasetId":6058495},{"sourceId":206165222,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Baseline UNet training + prediction/submission\n\n\nThis is the notebook I cobbled together to wrap my head around this challenge.\nI don't garuantee that the results are great, only that it works from end-to-end. \n\nIt trains a basic UNet and makes a submission. \n\nIt's based on these three notebooks: \n\n1. [3D U-Net : Training Only](https://www.kaggle.com/code/ahsuna123/3d-u-net-training-only)\n2. [3D U-Net PyTorch Lightning distributed training](https://www.kaggle.com/code/zhuowenzhao11/3d-u-net-pytorch-lightning-distributed-training)\n3. [3d-unet using 2d image encoder](https://www.kaggle.com/code/hengck23/3d-unet-using-2d-image-encoder/notebook)\n\n\nI've pre-computed the input data and stored them as numpy arrays so they don't have to be extracted every time the notebooks is run. ","metadata":{}},{"cell_type":"markdown","source":"## Installing offline deps\n\nAs this is a code comp, there is no internet. \nSo we have to do some silly things to get dependencies in here. \nWhy is asciitree such a PITA? ","metadata":{}},{"cell_type":"code","source":"deps_path = '/kaggle/input/czii-cryoet-dependencies'","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:18:47.60867Z","iopub.execute_input":"2024-11-11T08:18:47.609002Z","iopub.status.idle":"2024-11-11T08:18:47.619844Z","shell.execute_reply.started":"2024-11-11T08:18:47.608968Z","shell.execute_reply":"2024-11-11T08:18:47.618622Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! cp -r /kaggle/input/czii-cryoet-dependencies/asciitree-0.3.3/ asciitree-0.3.3/","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:18:47.621731Z","iopub.execute_input":"2024-11-11T08:18:47.622077Z","iopub.status.idle":"2024-11-11T08:18:48.738544Z","shell.execute_reply.started":"2024-11-11T08:18:47.622029Z","shell.execute_reply":"2024-11-11T08:18:48.737231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip wheel asciitree-0.3.3/asciitree-0.3.3/\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-11T08:18:48.740204Z","iopub.execute_input":"2024-11-11T08:18:48.740602Z","iopub.status.idle":"2024-11-11T08:19:13.208949Z","shell.execute_reply.started":"2024-11-11T08:18:48.740556Z","shell.execute_reply":"2024-11-11T08:19:13.207821Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install asciitree-0.3.3-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:19:13.211535Z","iopub.execute_input":"2024-11-11T08:19:13.21191Z","iopub.status.idle":"2024-11-11T08:19:45.291424Z","shell.execute_reply.started":"2024-11-11T08:19:13.21187Z","shell.execute_reply":"2024-11-11T08:19:45.290458Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install -q --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-11T08:19:45.29272Z","iopub.execute_input":"2024-11-11T08:19:45.293034Z","iopub.status.idle":"2024-11-11T08:20:08.593885Z","shell.execute_reply.started":"2024-11-11T08:19:45.293001Z","shell.execute_reply":"2024-11-11T08:20:08.592965Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List, Tuple, Union\nimport numpy as np\nimport torch\nfrom monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\nfrom monai.transforms import (\n    Compose, \n    EnsureChannelFirstd, \n    Orientationd,  \n    AsDiscrete,  \n    RandFlipd, \n    RandRotate90d, \n    NormalizeIntensityd,\n    RandCropByLabelClassesd,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:20:08.595437Z","iopub.execute_input":"2024-11-11T08:20:08.595867Z","iopub.status.idle":"2024-11-11T08:20:51.076502Z","shell.execute_reply.started":"2024-11-11T08:20:08.595816Z","shell.execute_reply":"2024-11-11T08:20:51.075627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define some helper functions\n\n\n### Patching helper functions\n\nThese are mostly used to split large volumes into smaller ones and stitch them back together. ","metadata":{}},{"cell_type":"code","source":"def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n    \"\"\"\n    Calculate the starting positions of patches along a single dimension\n    with minimal overlap to cover the entire dimension.\n    \n    Parameters:\n    -----------\n    dimension_size : int\n        Size of the dimension\n    patch_size : int\n        Size of the patch in this dimension\n        \n    Returns:\n    --------\n    List[int]\n        List of starting positions for patches\n    \"\"\"\n    if dimension_size <= patch_size:\n        return [0]\n        \n    # Calculate number of patches needed\n    n_patches = np.ceil(dimension_size / patch_size)\n    \n    if n_patches == 1:\n        return [0]\n    \n    # Calculate overlap\n    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n    \n    # Generate starting positions\n    positions = []\n    for i in range(int(n_patches)):\n        pos = int(i * (patch_size - total_overlap))\n        if pos + patch_size > dimension_size:\n            pos = dimension_size - patch_size\n        if pos not in positions:  # Avoid duplicates\n            positions.append(pos)\n    \n    return positions\n\ndef extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n    \"\"\"\n    Extract 3D patches from multiple arrays with minimal overlap to cover the entire array.\n    \n    Parameters:\n    -----------\n    arrays : List[np.ndarray]\n        List of input arrays, each with shape (m, n, l)\n    patch_size : int\n        Size of cubic patches (a x a x a)\n        \n    Returns:\n    --------\n    patches : List[np.ndarray]\n        List of all patches from all input arrays\n    coordinates : List[Tuple[int, int, int]]\n        List of starting coordinates (x, y, z) for each patch\n    \"\"\"\n    if not arrays or not isinstance(arrays, list):\n        raise ValueError(\"Input must be a non-empty list of arrays\")\n    \n    # Verify all arrays have the same shape\n    shape = arrays[0].shape\n    if not all(arr.shape == shape for arr in arrays):\n        raise ValueError(\"All input arrays must have the same shape\")\n    \n    if patch_size > min(shape):\n        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n    \n    m, n, l = shape\n    patches = []\n    coordinates = []\n    \n    # Calculate starting positions for each dimension\n    x_starts = calculate_patch_starts(m, patch_size)\n    y_starts = calculate_patch_starts(n, patch_size)\n    z_starts = calculate_patch_starts(l, patch_size)\n    \n    # Extract patches from each array\n    for arr in arrays:\n        for x in x_starts:\n            for y in y_starts:\n                for z in z_starts:\n                    patch = arr[\n                        x:x + patch_size,\n                        y:y + patch_size,\n                        z:z + patch_size\n                    ]\n                    patches.append(patch)\n                    coordinates.append((x, y, z))\n    \n    return patches, coordinates\n\n# Note: I should probably averge the overlapping areas, \n# but here they are just overwritten by the most recent one. \n\ndef reconstruct_array(patches: List[np.ndarray], \n                     coordinates: List[Tuple[int, int, int]], \n                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n    \"\"\"\n    Reconstruct array from patches.\n    \n    Parameters:\n    -----------\n    patches : List[np.ndarray]\n        List of patches to reconstruct from\n    coordinates : List[Tuple[int, int, int]]\n        Starting coordinates for each patch\n    original_shape : Tuple[int, int, int]\n        Shape of the original array\n        \n    Returns:\n    --------\n    np.ndarray\n        Reconstructed array\n    \"\"\"\n    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n    \n    patch_size = patches[0].shape[0]\n    \n    for patch, (x, y, z) in zip(patches, coordinates):\n        reconstructed[\n            x:x + patch_size,\n            y:y + patch_size,\n            z:z + patch_size\n        ] = patch\n        \n    \n    return reconstructed","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-11-11T08:20:51.077793Z","iopub.execute_input":"2024-11-11T08:20:51.078598Z","iopub.status.idle":"2024-11-11T08:20:51.099865Z","shell.execute_reply.started":"2024-11-11T08:20:51.078562Z","shell.execute_reply":"2024-11-11T08:20:51.098597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission helper functions\n\nThese help with getting the submission in the correct format","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef dict_to_df(coord_dict, experiment_name):\n    \"\"\"\n    Convert dictionary of coordinates to pandas DataFrame.\n    \n    Parameters:\n    -----------\n    coord_dict : dict\n        Dictionary where keys are labels and values are Nx3 coordinate arrays\n        \n    Returns:\n    --------\n    pd.DataFrame\n        DataFrame with columns ['x', 'y', 'z', 'label']\n    \"\"\"\n    # Create lists to store data\n    all_coords = []\n    all_labels = []\n    \n    # Process each label and its coordinates\n    for label, coords in coord_dict.items():\n        all_coords.append(coords)\n        all_labels.extend([label] * len(coords))\n    \n    # Concatenate all coordinates\n    all_coords = np.vstack(all_coords)\n    \n    df = pd.DataFrame({\n        'experiment': experiment_name,\n        'particle_type': all_labels,\n        'x': all_coords[:, 0],\n        'y': all_coords[:, 1],\n        'z': all_coords[:, 2]\n    })\n\n    \n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-11-11T08:20:51.101503Z","iopub.execute_input":"2024-11-11T08:20:51.101969Z","iopub.status.idle":"2024-11-11T08:20:51.135036Z","shell.execute_reply.started":"2024-11-11T08:20:51.101912Z","shell.execute_reply":"2024-11-11T08:20:51.134014Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reading in the data","metadata":{}},{"cell_type":"code","source":"TRAIN_DATA_DIR = \"/kaggle/input/create-numpy-dataset\"\nTEST_DATA_DIR = \"/kaggle/input/czii-cryo-et-object-identification\"","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:20:51.13654Z","iopub.execute_input":"2024-11-11T08:20:51.137046Z","iopub.status.idle":"2024-11-11T08:20:51.14532Z","shell.execute_reply.started":"2024-11-11T08:20:51.137007Z","shell.execute_reply":"2024-11-11T08:20:51.144492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_list = []\nfor i in range(7):\n    image = np.load(f\"{TRAIN_DATA_DIR}/train_image_{i}.npy\")\n    label = np.load(f\"{TRAIN_DATA_DIR}/train_label_{i}.npy\")\n\n    data_list.append({\"image\": image, \"label\": label})\n    \n\ntrain_files, val_files = data_list[:6], data_list[6:7]","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:20:51.148427Z","iopub.execute_input":"2024-11-11T08:20:51.148826Z","iopub.status.idle":"2024-11-11T08:21:07.749553Z","shell.execute_reply.started":"2024-11-11T08:20:51.148774Z","shell.execute_reply":"2024-11-11T08:21:07.748732Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create the training dataloader\n\nI should probably find a way to create a dataloader that takes more batches. ","metadata":{}},{"cell_type":"code","source":"# Non-random transforms to be cached\nnon_random_transforms = Compose([\n    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n    NormalizeIntensityd(keys=\"image\"),\n    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\")\n])\n\nraw_train_ds = CacheDataset(data=train_files, transform=non_random_transforms, cache_rate=1.0)\n\n\nmy_num_samples = 16\ntrain_batch_size = 1\n\n# Random transforms to be applied during training\nrandom_transforms = Compose([\n    RandCropByLabelClassesd(\n        keys=[\"image\", \"label\"],\n        label_key=\"label\",\n        spatial_size=[96, 96, 96],\n        num_classes=6,\n        num_samples=my_num_samples\n    ),\n    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[0, 2]),\n    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),    \n])\n\ntrain_ds = Dataset(data=raw_train_ds, transform=random_transforms)\n\n\n# DataLoader remains the same\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=train_batch_size,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=torch.cuda.is_available()\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:21:07.750839Z","iopub.execute_input":"2024-11-11T08:21:07.75154Z","iopub.status.idle":"2024-11-11T08:21:09.600666Z","shell.execute_reply.started":"2024-11-11T08:21:07.751495Z","shell.execute_reply":"2024-11-11T08:21:09.599777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create the validation dataloader\n\nHere I deviate a little from the source notebooks. \n\nIn the source, the validation dataloader also used the random transformations. This is bad practice and will result in noisy validation. \n\nHere I split the validation dataset in (slightly) overlapping blocks of `(96, 96 , 96)` so that we can have a consistent validation set that uses all the validation data. \n","metadata":{}},{"cell_type":"code","source":"val_images,val_labels = [dcts['image'] for dcts in val_files],[dcts['label'] for dcts in val_files]\n\nval_image_patches, _ = extract_3d_patches_minimal_overlap(val_images, 96)\nval_label_patches, _ = extract_3d_patches_minimal_overlap(val_labels, 96)\n\nval_patched_data = [{\"image\": img, \"label\": lbl} for img, lbl in zip(val_image_patches, val_label_patches)]\n\n\nvalid_ds = CacheDataset(data=val_patched_data, transform=non_random_transforms, cache_rate=1.0)\n\n\nvalid_batch_size = 16\n# DataLoader remains the same\nvalid_loader = DataLoader(\n    valid_ds,\n    batch_size=valid_batch_size,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=torch.cuda.is_available()\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:21:09.602006Z","iopub.execute_input":"2024-11-11T08:21:09.602675Z","iopub.status.idle":"2024-11-11T08:21:10.37067Z","shell.execute_reply.started":"2024-11-11T08:21:09.602631Z","shell.execute_reply":"2024-11-11T08:21:10.369674Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initialize the model\n\nThis model is pretty much directly copied from [3D U-Net PyTorch Lightning distributed training](https://www.kaggle.com/code/zhuowenzhao11/3d-u-net-pytorch-lightning-distributed-training)","metadata":{}},{"cell_type":"code","source":"import lightning.pytorch as pl\n\nfrom monai.networks.nets import UNet\nfrom monai.losses import TverskyLoss\nfrom monai.metrics import DiceMetric\n\nclass Model(pl.LightningModule):\n    def __init__(\n        self, \n        spatial_dims: int = 3,\n        in_channels: int = 1,\n        out_channels: int = 6,\n        channels: Union[Tuple[int, ...], List[int]] = (48, 64, 80, 80),\n        strides: Union[Tuple[int, ...], List[int]] = (2, 2, 1),\n        num_res_units: int = 1,\n        lr: float=1e-3):\n    \n        super().__init__()\n        self.save_hyperparameters()\n        self.model = UNet(\n            spatial_dims=self.hparams.spatial_dims,\n            in_channels=self.hparams.in_channels,\n            out_channels=self.hparams.out_channels,\n            channels=self.hparams.channels,\n            strides=self.hparams.strides,\n            num_res_units=self.hparams.num_res_units,\n        )\n        self.loss_fn = TverskyLoss(include_background=True, to_onehot_y=True, softmax=True)  # softmax=True for multiclass\n        self.metric_fn = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)\n\n        self.train_loss = 0\n        self.val_metric = 0\n        self.num_train_batch = 0\n        self.num_val_batch = 0\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch['image'], batch['label']\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.train_loss += loss\n        self.num_train_batch += 1\n        torch.cuda.empty_cache()\n        return loss\n\n    def on_train_epoch_end(self):\n        loss_per_epoch = self.train_loss/self.num_train_batch\n        #print(f\"Epoch {self.current_epoch} - Average Train Loss: {loss_per_epoch:.4f}\")\n        self.log('train_loss', loss_per_epoch, prog_bar=True)\n        self.train_loss = 0\n        self.num_train_batch = 0\n    \n    def validation_step(self, batch, batch_idx):\n        with torch.no_grad(): # This ensures that gradients are not stored in memory\n            x, y = batch['image'], batch['label']\n            y_hat = self(x)\n            metric_val_outputs = [AsDiscrete(argmax=True, to_onehot=self.hparams.out_channels)(i) for i in decollate_batch(y_hat)]\n            metric_val_labels = [AsDiscrete(to_onehot=self.hparams.out_channels)(i) for i in decollate_batch(y)]\n\n            # compute metric for current iteration\n            self.metric_fn(y_pred=metric_val_outputs, y=metric_val_labels)\n            metrics = self.metric_fn.aggregate(reduction=\"mean_batch\")\n            val_metric = torch.mean(metrics) # I used mean over all particle species as the metric. This can be explored.\n            self.val_metric += val_metric \n            self.num_val_batch += 1\n        torch.cuda.empty_cache()\n        return {'val_metric': val_metric}\n\n    def on_validation_epoch_end(self):\n        metric_per_epoch = self.val_metric/self.num_val_batch\n        #print(f\"Epoch {self.current_epoch} - Average Val Metric: {metric_per_epoch:.4f}\")\n        self.log('val_metric', metric_per_epoch, prog_bar=True, sync_dist=False) # sync_dist=True for distributed training\n        self.val_metric = 0\n        self.num_val_batch = 0\n    \n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:21:10.372362Z","iopub.execute_input":"2024-11-11T08:21:10.372953Z","iopub.status.idle":"2024-11-11T08:21:11.240463Z","shell.execute_reply.started":"2024-11-11T08:21:10.372903Z","shell.execute_reply":"2024-11-11T08:21:11.239691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"channels = (48, 64, 80, 80)\nstrides_pattern = (2, 2, 1)       \nnum_res_units = 1\nlearning_rate = 1e-3\nnum_epochs = 50\n\nmodel = Model(channels=channels, strides=strides_pattern, num_res_units=num_res_units, lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:21:11.241705Z","iopub.execute_input":"2024-11-11T08:21:11.242099Z","iopub.status.idle":"2024-11-11T08:21:11.272732Z","shell.execute_reply.started":"2024-11-11T08:21:11.242056Z","shell.execute_reply":"2024-11-11T08:21:11.271883Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the model\n\n","metadata":{}},{"cell_type":"code","source":"torch.set_float32_matmul_precision('medium')\n\n# Check if CUDA is available and then count the GPUs\nif torch.cuda.is_available():\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of GPUs available: {num_gpus}\")\nelse:\n    print(\"No GPU available. Running on CPU.\")\ndevices = list(range(num_gpus))\nprint(devices)\n\n\ntrainer = pl.Trainer(\n    max_epochs=num_epochs,\n    #strategy=\"ddp_notebook\", \n    accelerator=\"gpu\",\n    devices=[0],# devices\n    num_nodes=1,\n    log_every_n_steps=10,\n    enable_progress_bar=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T22:22:02.193188Z","iopub.execute_input":"2024-11-10T22:22:02.19348Z","iopub.status.idle":"2024-11-10T22:22:02.2914Z","shell.execute_reply.started":"2024-11-10T22:22:02.193449Z","shell.execute_reply":"2024-11-10T22:22:02.290531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let there be gradients!\n\nLocally this config seems to train for about 1000 steps before the model starts overfitting. ","metadata":{}},{"cell_type":"code","source":"\ntrainer.fit(model, train_loader, valid_loader)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T22:22:02.292592Z","iopub.execute_input":"2024-11-10T22:22:02.292904Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predict on the test set\n\n","metadata":{}},{"cell_type":"code","source":"model.eval();\nmodel.to(\"cuda\");","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:25:49.55842Z","iopub.execute_input":"2024-11-11T08:25:49.558823Z","iopub.status.idle":"2024-11-11T08:25:49.566484Z","shell.execute_reply.started":"2024-11-11T08:25:49.558784Z","shell.execute_reply":"2024-11-11T08:25:49.565607Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\ncopick_config_path = TRAIN_DATA_DIR + \"/copick.config\"\n\nwith open(copick_config_path) as f:\n    copick_config = json.load(f)\n\ncopick_config['static_root'] = '/kaggle/input/czii-cryo-et-object-identification/test/static'\n\ncopick_test_config_path = 'copick_test.config'\n\nwith open(copick_test_config_path, 'w') as outfile:\n    json.dump(copick_config, outfile)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:25:50.698326Z","iopub.execute_input":"2024-11-11T08:25:50.699039Z","iopub.status.idle":"2024-11-11T08:25:50.712738Z","shell.execute_reply.started":"2024-11-11T08:25:50.699001Z","shell.execute_reply":"2024-11-11T08:25:50.711837Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copick\n\nroot = copick.from_file(copick_test_config_path)\n\ncopick_user_name = \"copickUtils\"\ncopick_segmentation_name = \"paintedPicks\"\nvoxel_size = 10\ntomo_type = \"denoised\"","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:25:51.051886Z","iopub.execute_input":"2024-11-11T08:25:51.052884Z","iopub.status.idle":"2024-11-11T08:25:52.072891Z","shell.execute_reply.started":"2024-11-11T08:25:51.052841Z","shell.execute_reply":"2024-11-11T08:25:52.071945Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Non-random transforms to be cached\ninference_transforms = Compose([\n    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n    NormalizeIntensityd(keys=\"image\"),\n    Orientationd(keys=[\"image\"], axcodes=\"RAS\")\n])","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:25:52.074451Z","iopub.execute_input":"2024-11-11T08:25:52.07555Z","iopub.status.idle":"2024-11-11T08:25:52.080883Z","shell.execute_reply.started":"2024-11-11T08:25:52.07551Z","shell.execute_reply":"2024-11-11T08:25:52.079896Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cc3d\n\nid_to_name = {1: \"apo-ferritin\", \n              2: \"beta-galactosidase\", \n              3: \"ribosome\", \n              4: \"thyroglobulin\", \n              5: \"virus-like-particle\"}","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:25:52.082594Z","iopub.execute_input":"2024-11-11T08:25:52.083124Z","iopub.status.idle":"2024-11-11T08:25:52.097421Z","shell.execute_reply.started":"2024-11-11T08:25:52.083091Z","shell.execute_reply":"2024-11-11T08:25:52.09666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Iterate over test set\n\n\nBelow we will: \n1. Read in a run\n2. Split it into patches of size (96, 96, 96)\n3. Create a dataset from the patches\n4. Predict the segmentation mask\n5. Glue the mask back together\n6. Find the connected components for each class\n7. Find the centroids of the connected components\n8. Add to the dataframe\n\nThen do this for all runs. \n\nThis can probably be optimized quite a bit. ","metadata":{}},{"cell_type":"code","source":"BLOB_THRESHOLD = 500\nCERTAINTY_THRESHOLD = 0.5\n\nclasses = [1, 2, 3, 4, 5]\nwith torch.no_grad():\n    location_df = []\n    for run in root.runs:\n        print(run)\n\n        tomo = run.get_voxel_spacing(10)\n        tomo = tomo.get_tomogram(tomo_type).numpy()\n\n\n\n        tomo_patches, coordinates  = extract_3d_patches_minimal_overlap([tomo], 96)\n\n        tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n\n        tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0)\n\n        pred_masks = []\n\n        for i in range(len(tomo_ds)):\n            input_tensor = tomo_ds[i]['image'].unsqueeze(0).to(\"cuda\")\n            model_output = model(input_tensor)\n\n            probs = torch.softmax(model_output[0], dim=0)\n            thresh_probs = probs > CERTAINTY_THRESHOLD\n            _, max_classes = thresh_probs.max(dim=0)\n\n            pred_masks.append(max_classes.cpu().numpy())\n            \n\n        reconstructed_mask = reconstruct_array(pred_masks, coordinates, tomo.shape)\n        \n        location = {}\n\n        for c in classes:\n            cc = cc3d.connected_components(reconstructed_mask == c)\n            stats = cc3d.statistics(cc)\n            zyx=stats['centroids'][1:]*10.012444 #https://www.kaggle.com/competitions/czii-cryo-et-object-identification/discussion/544895#3040071\n            zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]\n            xyz =np.ascontiguousarray(zyx_large[:,::-1])\n\n            location[id_to_name[c]] = xyz\n\n\n        df = dict_to_df(location, run.name)\n        location_df.append(df)\n    \n    location_df = pd.concat(location_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T08:25:52.099465Z","iopub.execute_input":"2024-11-11T08:25:52.100464Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"location_df.insert(loc=0, column='id', value=np.arange(len(location_df)))\nlocation_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}